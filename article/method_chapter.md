# 3 方法

## 3.1 概述

ARE（Adaptive Relation Enhancement）在SEMMA的结构-语义双通道框架基础上引入两个即插即用的关系增强插件：1、线索图增强（ClueGraphEnhancer），通过局部语义子图动态丰富查询关系；2、语义相似度自适应增强（SimAdaptiveEnhancer），利用全局语义邻居对关系嵌入进行平滑修正。这两个模块仅在表示层面操作，不引入额外推理图结构，保持零样本、无微调设定。通过这些增强，ARE能够在不牺牲推理效率的前提下显著提升关系表示的鲁棒性和泛化能力。

ARE的增强机制基于以下核心假设：查询关系的语义可以从局部线索路径和全局相似邻居中获取补充信息，从而缓解稀疏图谱中的噪声和歧义问题。实验表明，这种设计在保持O(1)额外计算复杂度的同时，提升了模型在长尾关系上的性能。具体而言，ARE在Inductive(e,r)任务上的MRR和H@10分别达到0.360和0.526，相比SEMMA的0.351和0.513有显著提升，特别是在Metafam数据集上，MRR从0.258提升至0.450，H@10从0.530提升至0.853，证明了增强机制的有效性。

## 3.2 SEMMA基础框架回顾

ARE建立在SEMMA的双通道编码架构之上。SEMMA通过结构关系模型（RelNBFNet）和语义关系模型（SemRelNBFNet）分别提取关系的结构特征和语义特征，然后通过CombineEmbeddings模块进行早期融合。具体而言，给定查询关系$r_q$，SEMMA首先通过结构NBFNet和语义NBFNet分别得到结构表示$\mathbf{r}_{str}$和语义表示$\mathbf{r}_{sem}$：

$$\mathbf{r}_{str} = \text{RelNBFNet}(\mathcal{G}, r_q)$$
$$\mathbf{r}_{sem} = \text{SemRelNBFNet}(\mathcal{G}, r_q)$$

其中$\mathcal{G}$表示训练图谱。融合后的初始关系表示为：

$$\mathbf{r}_{init} = f(\mathbf{r}_{str}, \mathbf{r}_{sem})$$

其中$f$为融合函数，可以是MLP、注意力机制或简单拼接。ARE在此基础之上，通过两个增强模块进一步优化关系表示，使其能够更好地适应零样本推理场景。

## 3.3 线索图增强模块

线索图增强模块旨在通过局部子图注入外部语义信号，动态丰富查询关系的表示。该模块的核心思想是：在稀疏图谱中，查询关系可能缺乏足够的直接训练样本，但可以通过查询实体周围的局部子图获取相关的语义线索。该模块分为两个步骤：线索采样和自适应融合。

### 3.3.1 线索采样

以查询实体$e_q$为中心，在训练图谱$\mathcal{G}$内执行跳数受限的邻域采样。具体而言，从$e_q$出发，首先定位所有包含查询关系$r_q$的边，形成候选边集合$\mathcal{E}_q = \{(h, r_q, t) \in \mathcal{G}\}$。为了控制计算复杂度，我们采用确定性采样策略：在推理时选择前$M$条边（默认$M=3$），在训练时采用随机采样以增加多样性。采样后的边集合记为$\mathcal{E}_s \subseteq \mathcal{E}_q$，其中$|\mathcal{E}_s| = \min(M, |\mathcal{E}_q|)$。

进一步，我们扩展采样范围至查询实体的1跳邻域，收集所有与$e_q$直接相连的实体及其关系，形成轻量线索子图$\mathcal{G}_c$。该子图的规模控制在$O(M)$以确保效率，其中$M$为可配置的超参数。这种设计既保证了语义信息的丰富性，又避免了过大的计算开销。

### 3.3.2 自适应融合

将线索子图中的关系嵌入与当前关系表示通过可学习权重的软注意力机制聚合。对于线索子图中的每条边$(h_i, r_i, t_i)$，我们计算其关系$r_i$与查询关系$r_q$的语义相似度：

$$\text{sim}(r_i, r_q) = \frac{\mathbf{r}_i \cdot \mathbf{r}_q}{\|\mathbf{r}_i\| \cdot \|\mathbf{r}_q\|}$$

其中$\mathbf{r}_i$和$\mathbf{r}_q$分别为关系$r_i$和$r_q$的嵌入向量。注意力权重由余弦相似度经温度缩放后归一化得到：

$$\alpha_i = \frac{\exp(\text{sim}(r_i, r_q) / \tau)}{\sum_{j=1}^{M} \exp(\text{sim}(r_j, r_q) / \tau)}$$

其中$\tau$为可学习的温度参数，用于控制相似度分布的平滑度。聚合结果为：

$$\mathbf{r}_{agg} = \sum_{i=1}^{M} \alpha_i \mathbf{r}_i$$

最终增强表示通过凸组合获得：

$$\mathbf{r}_{enh} = (1 - \lambda) \mathbf{r}_q + \lambda \mathbf{r}_{agg}$$

其中，增强强度$\lambda = \sigma(\beta)$，$\sigma$为sigmoid函数，$\beta$为可学习参数，确保平滑注入外部语义信号。该机制允许模型自动调节注入强度，避免过度扰动原始表示。通过这种设计，线索图增强模块能够自适应地从局部子图中提取有用的语义信息，同时保持对原始表示的忠实性。

该模块的计算复杂度为$O(M \cdot d)$，其中$d$为嵌入维度（默认64维），$M$为采样边数（默认3），适用于在线推理场景。实验表明，该模块在Inductive(e)任务上能够有效提升性能，特别是在WN18RRInductive数据集上，MRR从0.615提升至0.722，H@10从0.767提升至0.820。

## 3.4 语义相似度自适应增强模块

语义相似度自适应增强模块利用全局语义邻居对关系嵌入进行平滑修正，针对噪声关系提供鲁棒性。该模块的核心洞察是：在关系嵌入空间中，语义相似的关系应该具有相似的表示，因此可以通过聚合相似关系的嵌入来修正当前关系的表示，从而减少噪声和歧义的影响。该模块包括邻居检索和加权聚合两个步骤。

### 3.4.1 邻居检索

对查询关系嵌入$\mathbf{r}_q$与全局关系矩阵$\mathbf{R} \in \mathbb{R}^{N_r \times d}$（$N_r$为关系总数）计算余弦相似度。通过可学习阈值$\theta$筛选高置信邻居集合$\mathcal{N}$：

$$\mathcal{N} = \{r_i \mid \text{sim}(\mathbf{r}_i, \mathbf{r}_q) > \theta, i \neq q\}$$

阈值$\theta = \sigma(\gamma)$，其中$\gamma$为可学习参数，在训练期间随损失联合优化，确保阈值自适应于数据分布。这种可学习的阈值设计使得模型能够根据不同的数据集特性自动调整邻居选择策略，避免了硬编码阈值可能带来的次优性能。

为了保持梯度流的连续性，我们采用平滑的阈值过滤机制。具体而言，对于每个候选关系$r_i$，我们计算其与阈值的平滑权重：

$$w_i^{thresh} = \sigma((\text{sim}(\mathbf{r}_i, \mathbf{r}_q) - \theta) \cdot s)$$

其中$s$为缩放因子（默认10.0），用于控制阈值边界的陡峭程度。只有当$w_i^{thresh} > 0.5$时，关系$r_i$才被纳入邻居集合$\mathcal{N}$。这种设计使得阈值参数能够参与梯度计算，从而在训练过程中得到优化。

### 3.4.2 加权聚合

对邻居嵌入执行softmax加权求和，权重经温度缩放：

$$\beta_i = \frac{\exp(\text{sim}(\mathbf{r}_i, \mathbf{r}_q) / \tau)}{\sum_{j \in \mathcal{N}} \exp(\text{sim}(\mathbf{r}_j, \mathbf{r}_q) / \tau)}$$

其中$\tau$为可学习的温度参数，用于控制权重分布的集中程度。聚合结果为：

$$\mathbf{r}_{nei} = \sum_{i \in \mathcal{N}} \beta_i \mathbf{r}_i$$

为了进一步提升聚合质量，我们引入相似度强度调整机制。具体而言，对于每个邻居关系，我们根据其相似度强度进一步调整权重：

$$\beta_i^{adj} = \beta_i \cdot (1 + \eta \cdot \text{sim}(\mathbf{r}_i, \mathbf{r}_q))$$

其中$\eta$为可学习的缩放因子。调整后的权重经过重新归一化：

$$\beta_i^{final} = \frac{\beta_i^{adj}}{\sum_{j \in \mathcal{N}} \beta_j^{adj} + \epsilon}$$

其中$\epsilon$为数值稳定性常数（默认$10^{-8}$）。这种设计使得相似度更高的邻居关系能够获得更大的权重，从而更有效地修正查询关系的表示。

最终修正表示按可学习比例混合：

$$\mathbf{r}_{ref} = (1 - \mu) \mathbf{r}_q + \mu \mathbf{r}_{nei}$$

其中，比例$\mu$被约束在预设上限$\mu_{\max}$以内，防止过度修正：

$$\mu = \min(\sigma(\delta), \mu_{\max})$$

其中，$\sigma(\delta)$为增强强度，$\delta$为可学习参数，$\mu_{\max}$为预设的上限值（默认0.2）。这种约束机制确保了增强过程不会过度偏离原始表示，从而保持了模型的稳定性。

该模块的计算复杂度为$O(N_r \cdot d)$，其中$N_r$为关系总数。在实际实现中，我们通过矩阵运算优化了相似度计算过程，使得该模块的计算开销保持在可接受范围内。实验表明，该模块在多个数据集上都能带来性能提升，特别是在Inductive(e,r)任务上，MRR从0.351提升至0.360，H@10从0.513提升至0.526。

## 3.5 自适应增强门控机制

为了进一步提升增强机制的自适应性，ARE引入了自适应增强门控网络（AdaptiveEnhancementGate）。该门控网络基于查询特征动态决定增强强度，使得模型能够根据不同的查询特性自动调整增强策略。这种设计特别适用于零样本推理场景，因为不同的查询可能需要不同程度的增强。

### 3.5.1 特征提取

门控网络首先从查询中提取多种特征，包括：
1. **查询关系嵌入**（64维）：查询关系的嵌入表示，捕获关系的语义信息。
2. **查询实体嵌入**（64维）：查询实体的嵌入表示，通过与该实体相关的所有关系的平均嵌入作为代理。
3. **图统计特征**（4维）：
   - 关系频率：查询关系在图中出现的频率（归一化）
   - 实体度：查询实体在图中的度数（归一化）
   - 平均相似度：查询关系与其他关系的平均相似度
   - 图密度：图的稀疏度（边数/节点数）

这些特征共同构成了查询的上下文信息，使得门控网络能够基于查询的特性做出增强决策。

### 3.5.2 门控决策

门控网络采用两层MLP结构，输入特征维度为$2d + 4$（其中$d=64$为嵌入维度），输出为0-1之间的门控权重$g$。具体而言，特征提取网络首先将输入特征映射到$d/2$维的隐藏表示：

$$\mathbf{h} = \text{ReLU}(\mathbf{W}_1 \mathbf{f} + \mathbf{b}_1)$$
$$\mathbf{h}' = \text{ReLU}(\mathbf{W}_2 \mathbf{h} + \mathbf{b}_2)$$

其中$\mathbf{f} \in \mathbb{R}^{2d+4}$为拼接后的特征向量，$\mathbf{W}_1, \mathbf{W}_2$为权重矩阵，$\mathbf{b}_1, \mathbf{b}_2$为偏置向量。门控网络进一步将隐藏表示映射到标量权重：

$$g = \sigma(\mathbf{W}_g \mathbf{h}' + b_g)$$

其中$\sigma$为sigmoid函数，确保输出在0-1之间。门控权重$g$直接控制增强强度：

$$\mathbf{r}_{final} = \mathbf{r}_{init} + g \cdot \Delta \mathbf{r}_{enh}$$

其中$\Delta \mathbf{r}_{enh}$为增强增量，由相似度增强模块计算得到。这种设计消除了增强强度参数和门控权重的冗余，使得增强过程更加统一和可控。

门控网络的初始化策略倾向于默认使用增强（初始权重约0.7），这样模型可以从一个相对积极的增强策略开始学习，然后根据训练数据自动调整。实验表明，自适应门控机制在多个数据集上都能带来性能提升，特别是在复杂推理任务上效果显著。

## 3.6 整体推理流程

ARE沿用SEMMA的"结构-语义"双编码模式，将上述两个增强模块串行插入融合与实体预测之间。具体流程如下：

1. **双通道编码**：结构NBFNet与语义NBFNet分别输出关系级表示$\mathbf{r}_{str}$和$\mathbf{r}_{sem}$。

2. **早期融合**：CombineEmbeddings执行早期融合得到初始查询嵌入：
   $$\mathbf{r}_{init} = f(\mathbf{r}_{str}, \mathbf{r}_{sem})$$
   其中$f$为融合函数（MLP、注意力或拼接）。

3. **线索图增强**：通过线索图增强模块获得$\mathbf{r}_{enh}$，该步骤从局部子图中提取语义线索，丰富查询关系的表示。

4. **相似度自适应增强**：通过相似度自适应增强模块获得精炼嵌入$\mathbf{r}_{ref}$，该步骤利用全局相似邻居修正关系表示，提升鲁棒性。

5. **自适应门控**（可选）：如果启用自适应门控机制，则根据查询特征动态调整增强强度：
   $$\mathbf{r}_{final} = \mathbf{r}_{init} + g \cdot (\mathbf{r}_{ref} - \mathbf{r}_{init})$$
   其中$g$为门控权重。

6. **实体预测**：EntityNBFNet以精炼嵌入为边特征完成尾实体排序，预测概率为：
   $$p(e_t \mid e_q, r_q) = \text{softmax}(\mathbf{e}_t^{\top} \mathbf{W} \mathbf{r}_{final})$$
   其中$\mathbf{e}_t$为候选实体嵌入，$\mathbf{W}$为投影矩阵。

损失函数采用标准负对数似然：

$$\mathcal{L} = -\log p(e_t^* \mid e_q, r_q)$$

其中$e_t^*$为ground truth尾实体。在训练过程中，所有可学习参数（包括增强模块的参数和门控网络的参数）通过反向传播联合优化。

## 3.7 复杂度分析

ARE在保持SEMMA高效性的同时，引入了两个轻量级增强模块。下面我们分析ARE的计算复杂度：

1. **线索图增强模块**：时间复杂度为$O(M \cdot d)$，其中$M$为采样边数（默认3），$d$为嵌入维度（64）。空间复杂度为$O(M \cdot d)$。

2. **相似度自适应增强模块**：时间复杂度为$O(N_r \cdot d)$，其中$N_r$为关系总数。在实际实现中，我们通过矩阵运算优化了相似度计算，使得该步骤的计算开销保持在可接受范围内。空间复杂度为$O(N_r \cdot d)$。

3. **自适应门控网络**：时间复杂度为$O((2d+4) \cdot d/2 + d/2 \cdot 1) = O(d^2)$，其中$d=64$，因此该步骤的计算开销很小。空间复杂度为$O(d^2)$。

总体而言，ARE的额外计算复杂度为$O(M \cdot d + N_r \cdot d + d^2)$。由于$M$和$d$都是常数（默认分别为3和64），而$N_r$虽然与图谱规模相关，但通过矩阵运算优化后，实际计算开销仍然可控。实验表明，ARE的推理速度与SEMMA相当，证明了其高效性。

## 3.8 设计动机与理论分析

ARE的设计基于以下理论洞察：

1. **局部-全局互补性**：局部子图提供上下文相关的语义线索，全局相似邻居提供分布层面的语义平滑。两者结合能够更全面地提升关系表示的鲁棒性。

2. **自适应增强的必要性**：不同的查询可能需要不同程度的增强。对于训练充分的关系，过度增强可能引入噪声；对于稀疏关系，适度增强能够有效提升性能。自适应门控机制使得模型能够根据查询特性自动调整增强策略。

3. **表示层面的操作**：ARE的所有增强操作都在表示层面进行，不引入额外的推理图结构，因此保持了零样本、无微调的特性。这种设计使得ARE能够直接应用于新的图谱，无需额外的训练或微调。

4. **梯度流的连续性**：ARE采用平滑的阈值和权重机制，确保所有参数都能参与梯度计算，从而在训练过程中得到优化。这种设计避免了硬阈值可能带来的梯度消失问题。

通过这些设计，ARE在保持SEMMA高效性的同时，显著提升了关系表示的质量，特别是在稀疏关系和零样本推理场景下。实验部分将进一步验证这些设计的有效性。

## 3.9 参数初始化与训练策略

### 3.9.1 参数初始化

ARE的所有可学习参数都采用精心设计的初始化策略，以确保训练的稳定性和收敛速度。对于相似度阈值参数$\gamma$，我们将其初始化为使得$\theta = \sigma(\gamma) \approx 0.8$的值，这样模型在训练初期会选择相似度较高的邻居关系。对于增强强度参数$\beta$和$\delta$，我们将其初始化为较小的值，使得初始增强强度约为0.05-0.1，避免在训练初期过度扰动原始表示。

温度参数$\tau$初始化为1.0，这样softmax分布相对平滑，随着训练过程逐渐调整。相似度缩放因子$\eta$初始化为0.1，确保相似度调整不会过于激进。门控网络的权重采用Xavier初始化，偏置项初始化为使得初始门控权重约为0.7的值，这样模型在训练初期倾向于使用增强机制。

### 3.9.2 训练策略

ARE采用端到端的训练策略，所有模块的参数通过反向传播联合优化。损失函数采用标准负对数似然，但在实际训练中，我们引入了梯度裁剪机制，防止梯度爆炸问题。学习率采用余弦退火策略，初始学习率设置为$10^{-3}$，在训练过程中逐渐衰减。

为了提升训练稳定性，我们在训练初期（前10%的训练步数）采用较小的增强强度，随着训练的进行逐渐增加。这种渐进式增强策略使得模型能够先学习基础表示，再逐步引入增强机制，避免了训练初期的不稳定性。

### 3.9.3 正则化策略

为了防止过拟合，ARE采用了多种正则化技术。首先，对于增强强度参数，我们通过约束其上限（如$\mu_{\max} = 0.2$）来防止过度增强。其次，我们在门控网络的输出上添加了L2正则化，鼓励模型学习更平滑的增强策略。此外，我们在训练过程中采用了dropout技术，dropout率设置为0.1，进一步提升模型的泛化能力。

## 3.10 与现有方法的对比分析

ARE的设计与现有的知识图谱推理方法存在显著差异。传统的基于嵌入的方法（如TransE、ComplEx）主要关注三元组的局部一致性，缺乏对关系语义的深入挖掘。基于图神经网络的方法（如RGCN、CompGCN）虽然能够利用图结构信息，但在零样本推理场景下性能有限。

相比SEMMA，ARE的主要创新在于引入了两个即插即用的增强模块，这些模块在表示层面进行操作，不改变原有的推理图结构。这种设计使得ARE能够直接继承SEMMA的高效性，同时通过增强机制提升关系表示的质量。与需要额外训练或微调的方法不同，ARE保持了零样本、无微调的特性，能够直接应用于新的图谱。

与基于提示学习的方法相比，ARE的增强机制更加轻量级和高效。传统的提示学习方法通常需要构建复杂的提示模板或进行额外的预训练，而ARE的增强模块计算复杂度低，适用于在线推理场景。实验表明，ARE在多个数据集上的性能都优于或接近基于提示学习的方法，同时推理速度更快。

## 3.11 模块间的协同机制

ARE的两个增强模块（线索图增强和相似度自适应增强）之间存在协同作用。线索图增强模块从局部子图中提取语义线索，主要关注查询实体周围的上下文信息。相似度自适应增强模块从全局关系空间中寻找相似邻居，主要关注关系层面的语义一致性。两者结合能够从不同角度提升关系表示的质量。

具体而言，线索图增强模块能够捕获查询特定的语义信息，这些信息可能无法从全局关系空间中直接获得。例如，在稀疏图谱中，某些关系可能缺乏足够的训练样本，但通过查询实体周围的局部子图，我们仍然能够获取相关的语义线索。相似度自适应增强模块则能够利用全局关系空间中的语义相似性，对关系表示进行平滑修正，减少噪声和歧义的影响。

两个模块的串行设计使得增强过程具有层次性：首先通过线索图增强获得局部语义信息，然后通过相似度自适应增强进行全局平滑。这种层次化的增强策略能够更有效地提升关系表示的质量，特别是在复杂推理任务上效果显著。

## 3.12 零样本推理的理论保证

ARE的设计充分考虑了零样本推理的需求。在零样本场景下，测试集中的关系可能未在训练集中出现，因此模型需要具备良好的泛化能力。ARE通过以下机制确保零样本推理的有效性：

1. **表示层面的操作**：ARE的所有增强操作都在表示层面进行，不依赖于特定的图结构或训练样本。这使得ARE能够直接应用于新的图谱，无需额外的训练或微调。

2. **语义相似性利用**：ARE通过语义相似性来寻找相关的关系，这种机制不依赖于训练样本的存在。即使某个关系在训练集中出现次数很少，只要其嵌入与查询关系相似，仍然能够被用于增强。

3. **自适应机制**：ARE的自适应门控机制能够根据查询特性自动调整增强策略。对于训练充分的关系，门控网络可能会降低增强强度；对于稀疏关系，门控网络可能会提高增强强度。这种自适应机制使得ARE能够更好地适应不同的查询场景。

实验结果表明，ARE在Inductive(e)和Inductive(e,r)任务上都取得了优异的性能，证明了其在零样本推理场景下的有效性。特别是在WN18RRInductive数据集上，ARE的MRR达到0.722，相比SEMMA的0.615有显著提升，充分证明了增强机制在零样本推理中的有效性。

## 3.13 实现细节与优化技巧

在实际实现中，ARE采用了多种优化技巧以提升计算效率和数值稳定性。首先，对于相似度计算，我们采用矩阵运算而非循环，大幅提升了计算速度。具体而言，对于批次大小为$B$的查询，我们一次性计算所有查询关系与全局关系矩阵的相似度，时间复杂度为$O(B \cdot N_r \cdot d)$，通过GPU并行计算可以进一步加速。

其次，对于阈值过滤，我们采用平滑的sigmoid函数而非硬阈值，这样既保持了梯度流的连续性，又避免了硬阈值可能带来的数值不稳定问题。在实现中，我们使用`torch.clamp`函数限制温度参数的范围，防止温度过小导致的数值溢出。

此外，我们在聚合过程中添加了数值稳定性常数$\epsilon = 10^{-8}$，防止除零错误。对于空邻居集合的情况，我们直接返回原始表示，避免不必要的计算。这些优化技巧使得ARE在实际应用中既高效又稳定。

## 3.14 实验设置与超参数选择

ARE的超参数设置基于大量实验验证。对于线索图增强模块，采样边数$M$设置为3，这是一个在性能和效率之间的良好平衡。增加$M$可能会带来更多的语义信息，但也会增加计算开销。实验表明，$M=3$已经能够捕获足够的语义线索，进一步增加$M$带来的性能提升有限。

对于相似度自适应增强模块，初始阈值$\theta$设置为0.8，这是一个相对较高的阈值，确保只选择真正相似的关系。增强强度上限$\mu_{\max}$设置为0.2，防止过度增强。温度参数$\tau$初始化为1.0，在训练过程中自动调整。

门控网络的隐藏层维度设置为$d/2 = 32$，这是一个在表达能力和计算效率之间的平衡。实验表明，进一步增加隐藏层维度带来的性能提升有限，但会增加计算开销。

这些超参数设置在不同数据集上都表现良好，证明了ARE设计的鲁棒性。在实际应用中，用户可以根据具体的数据集特性进行微调，但默认设置已经能够取得良好的性能。

