"""
åˆ†ææ¯ä¸ªæµ‹è¯•æ ·æœ¬çš„ç›¸ä¼¼å…³ç³»å‚è€ƒæƒ…å†µ
ç»Ÿè®¡æœ‰å¤šå°‘å¯ä»¥å‚è€ƒçš„ã€æœ‰å¤šå°‘æ˜¯æœ‰æ•ˆçš„ã€å¤šå°‘å¼•å…¥äº†å™ªéŸ³
"""

import os
import sys
import yaml
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from collections import defaultdict
from types import SimpleNamespace
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from ultra import datasets, util, parse
from ultra.models import Ultra
from ultra.enhanced_models import EnhancedUltra
from torch_geometric.data import Data
import torch.nn.functional as F

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def load_flags():
    """åŠ è½½flags.yamlé…ç½®"""
    flags_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "flags.yaml")
    with open(flags_path, 'r', encoding='utf-8') as f:
        flags = yaml.safe_load(f)
    return flags

def load_dataset(dataset_name, dataset_type="transductive"):
    """åŠ è½½æ•°æ®é›†"""
    flags = load_flags()
    
    # å¤„ç†æ•°æ®é›†åç§°æ˜ å°„
    dataset_name_mapping = {
        'YAGO310-ht': 'YAGO310',
        'ConceptNet 100k-ht': 'ConceptNet100k',
        'WDsinger-ht': 'WDsinger',
        'AristoV4-ht': 'AristoV4',
        'FB15K237Inductive:v1': 'FB15k237Inductive',  # æ³¨æ„ï¼šå°å†™k
        'FB15K237Inductive:v2': 'FB15k237Inductive',
        'FB15K237Inductive:v3': 'FB15k237Inductive',
        'FB15K237Inductive:v4': 'FB15k237Inductive',
        'WN18RRInductive:v3': 'WN18RRInductive',
        'NELLInductive:v1': 'NELLInductive',
        'NELLInductive:v3': 'NELLInductive',
        'NELLInductive:v4': 'NELLInductive',
        'WKIngram:25': 'WKIngram',
        'NLIngram:25': 'NLIngram',
        'NLIngram:75': 'NLIngram',
        'Metafam': 'Metafam',
        'WikiTopicsMT1:health': 'WikiTopicsMT1',
        'WikiTopicsMT3:infra': 'WikiTopicsMT3',
    }
    
    # è·å–å®é™…çš„æ•°æ®é›†ç±»å
    actual_dataset_name = dataset_name_mapping.get(dataset_name, dataset_name)
    
    # å¤„ç†ç‰ˆæœ¬å·
    version = None
    if ':' in dataset_name:
        parts = dataset_name.split(':')
        if len(parts) == 2:
            # å¦‚æœå·²ç»åœ¨mappingä¸­ï¼Œä¸è¦è¦†ç›–
            if dataset_name not in dataset_name_mapping:
                # å¯¹äºFB15K237Inductiveç³»åˆ—ï¼Œç»Ÿä¸€ä½¿ç”¨å°å†™kçš„ç±»å
                base_name = parts[0]
                if base_name == 'FB15K237Inductive':
                    actual_dataset_name = 'FB15k237Inductive'
                elif base_name in ['WKIngram', 'NLIngram']:
                    # WKIngramå’ŒNLIngraméœ€è¦ç‰ˆæœ¬å·
                    actual_dataset_name = base_name
                elif base_name.startswith('WikiTopicsMT'):
                    # WikiTopicsMTç³»åˆ—éœ€è¦ç‰ˆæœ¬å·
                    actual_dataset_name = base_name
                else:
                    actual_dataset_name = base_name
            version = parts[1]
    
    # è·å–æ•°æ®é›†è·¯å¾„
    kg_datasets_path = flags.get('kg_datasets_path', '/T20030104/ynj/semma/kg-datasets')
    
    # æ„å»ºé…ç½®
    # rootå‚æ•°åº”è¯¥æ˜¯kg_datasets_pathï¼ˆæ•°æ®é›†ä¼šè‡ªåŠ¨æ ¹æ®nameåœ¨kg_datasets_pathä¸‹æŸ¥æ‰¾ï¼‰
    # æ ¹æ®é…ç½®æ–‡ä»¶ï¼Œrootç›´æ¥è®¾ç½®ä¸ºkg_datasets_path
    cfg = {
        'dataset': {
            'class': actual_dataset_name,
            'root': kg_datasets_path,  # ä½¿ç”¨kg_datasets_pathä½œä¸ºroot
        },
        'task': {
            'name': 'InductiveInference' if dataset_type != 'transductive' else 'LinkPrediction'
        }
    }
    
    # å¦‚æœæœ‰ç‰ˆæœ¬å·ï¼Œæ·»åŠ åˆ°é…ç½®ä¸­
    if version:
        cfg['dataset']['version'] = version
    
    try:
        # å°†å­—å…¸è½¬æ¢ä¸ºå¯¹è±¡ï¼Œä»¥ä¾¿ build_dataset å¯ä»¥è®¿é—® cfg.dataset
        cfg_obj = SimpleNamespace()
        cfg_obj.dataset = cfg['dataset']
        cfg_obj.task = cfg.get('task', {})
        
        dataset = util.build_dataset(cfg_obj)
        # æ•°æ®é›†é€šè¿‡ç´¢å¼•è®¿é—®ï¼šdataset[0] = train, dataset[1] = valid, dataset[2] = test
        train_data = dataset[0]
        valid_data = dataset[1]
        test_data = dataset[2]
        return dataset, train_data, valid_data, test_data
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥ {dataset_name} (å®é™…ç±»å: {actual_dataset_name}): {e}")
        import traceback
        traceback.print_exception(*sys.exc_info())
        return None, None, None, None

def load_model(checkpoint_path, dataset, device):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    flags = load_flags()
    
    # AREå°±æ˜¯EnhanceUltraæ¨¡å‹ï¼Œæ‰€ä»¥ä½¿ç”¨EnhancedUltra
    # ä½†ä¹Ÿè¦æ£€æŸ¥flagsä¸­çš„è®¾ç½®
    model_type = flags.get('run', 'semma')
    
    # å¦‚æœflagsä¸­ä¸æ˜¯EnhancedUltraï¼Œä½†æˆ‘ä»¬è¦åˆ†æAREï¼Œåˆ™ä½¿ç”¨EnhancedUltra
    # å› ä¸ºç”¨æˆ·æ˜ç¡®è¯´AREæŒ‡çš„æ˜¯EnhanceUltraæ¨¡å‹
    use_enhanced = True  # åˆ†æAREï¼Œæ‰€ä»¥æ€»æ˜¯ä½¿ç”¨EnhancedUltra
    
    # ä½¿ç”¨é…ç½®æ„å»ºæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
    # è¿™é‡Œæˆ‘ä»¬å°è¯•ç›´æ¥åŠ è½½checkpointï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    model = None
    
    # å°è¯•åŠ è½½checkpoint
    if checkpoint_path and os.path.exists(checkpoint_path):
        try:
            state = torch.load(checkpoint_path, map_location='cpu')
            
            # å°è¯•ä»stateä¸­æ¨æ–­æ¨¡å‹ç»“æ„
            if 'model' in state:
                state_dict = state['model']
            else:
                state_dict = state
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯EnhancedUltraï¼ˆAREå°±æ˜¯EnhanceUltraï¼‰
            if 'similarity_enhancer' in state_dict or use_enhanced or model_type == 'EnhancedUltra':
                from ultra.enhanced_models import EnhancedUltra
                # éœ€è¦ä»state_dictæ¨æ–­é…ç½®ï¼Œè¿™é‡Œä½¿ç”¨é»˜è®¤é…ç½®
                model = EnhancedUltra(
                    rel_model_cfg={'class': 'RelNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64], 'num_relations': dataset[0].num_relations},
                    entity_model_cfg={'class': 'EntityNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64]},
                    sem_model_cfg={'class': 'SemRelNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64], 'num_relations': dataset[0].num_relations}
                )
            else:
                from ultra.models import Ultra
                model = Ultra(
                    rel_model_cfg={'class': 'RelNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64], 'num_relations': dataset[0].num_relations},
                    entity_model_cfg={'class': 'EntityNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64]},
                    sem_model_cfg={'class': 'SemRelNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64], 'num_relations': dataset[0].num_relations}
                )
            
            # åŠ è½½æƒé‡
            if 'model' in state:
                model.load_state_dict(state['model'], strict=False)
            else:
                model.load_state_dict(state, strict=False)
            print(f"âœ… åŠ è½½æ¨¡å‹checkpoint: {checkpoint_path}")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½checkpointå¤±è´¥: {e}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ›å»ºé»˜è®¤æ¨¡å‹ï¼ˆä½¿ç”¨EnhancedUltraï¼Œå› ä¸ºAREå°±æ˜¯EnhanceUltraï¼‰
            if use_enhanced or model_type == 'EnhancedUltra':
                from ultra.enhanced_models import EnhancedUltra
                model = EnhancedUltra(
                    rel_model_cfg={'class': 'RelNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64], 'num_relations': dataset[0].num_relations},
                    entity_model_cfg={'class': 'EntityNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64]},
                    sem_model_cfg={'class': 'SemRelNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64], 'num_relations': dataset[0].num_relations}
                )
            else:
                from ultra.models import Ultra
                model = Ultra(
                    rel_model_cfg={'class': 'RelNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64], 'num_relations': dataset[0].num_relations},
                    entity_model_cfg={'class': 'EntityNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64]},
                    sem_model_cfg={'class': 'SemRelNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64], 'num_relations': dataset[0].num_relations}
                )
    else:
        print(f"âš ï¸  Checkpointä¸å­˜åœ¨: {checkpoint_path}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        # åˆ›å»ºé»˜è®¤æ¨¡å‹ï¼ˆä½¿ç”¨EnhancedUltraï¼Œå› ä¸ºAREå°±æ˜¯EnhanceUltraï¼‰
        if use_enhanced or model_type == 'EnhancedUltra':
            from ultra.enhanced_models import EnhancedUltra
            model = EnhancedUltra(
                rel_model_cfg={'class': 'RelNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64], 'num_relations': dataset[0].num_relations},
                entity_model_cfg={'class': 'EntityNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64]},
                sem_model_cfg={'class': 'SemRelNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64], 'num_relations': dataset[0].num_relations}
            )
        else:
            from ultra.models import Ultra
            model = Ultra(
                rel_model_cfg={'class': 'RelNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64], 'num_relations': dataset[0].num_relations},
                entity_model_cfg={'class': 'EntityNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64]},
                sem_model_cfg={'class': 'SemRelNBFNet', 'input_dim': 64, 'hidden_dims': [64, 64], 'num_relations': dataset[0].num_relations}
            )
    
    model = model.to(device)
    model.eval()
    return model

def find_similar_relations(model, data, query_rel_idx, threshold=0.8, device='cuda'):
    """
    æ‰¾åˆ°ä¸æŸ¥è¯¢å…³ç³»ç›¸ä¼¼çš„å…³ç³»
    
    Returns:
        similar_rels: list of (rel_idx, similarity) tuples
    """
    model.eval()
    with torch.no_grad():
        try:
            # ç¡®ä¿dataåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if hasattr(data, 'to'):
                data = data.to(device)
            elif hasattr(data, 'keys'):
                # å¦‚æœdataæ˜¯Dataå¯¹è±¡ï¼Œéœ€è¦ç§»åŠ¨æ‰€æœ‰tensorå±æ€§
                for key in data.keys:
                    if isinstance(getattr(data, key), torch.Tensor):
                        setattr(data, key, getattr(data, key).to(device))
            # å¦‚æœdataå·²ç»æ˜¯torch_geometric.data.Dataå¯¹è±¡ï¼Œå®ƒåº”è¯¥å·²ç»æœ‰toæ–¹æ³•
            
            # è·å–å…³ç³»è¡¨ç¤º
            query_rels = torch.tensor([query_rel_idx], device=device)
            
            # è·å–å…³ç³»è¡¨ç¤º - å¯¹äºUltraå’ŒEnhancedUltraæ¨¡å‹
            if hasattr(model, 'relation_model'):
                relation_reprs = model.relation_model(data, query=query_rels)
            elif hasattr(model, 'model') and hasattr(model.model, 'relation_model'):
                # å¯¹äºUltraQueryç­‰åŒ…è£…æ¨¡å‹
                relation_reprs = model.model.relation_model(data, query=query_rels)
            else:
                return []
            
            if relation_reprs is None or relation_reprs.shape[0] == 0:
                return []
            
            # relation_reprs shape: [batch_size, num_relations, embedding_dim]
            # è·å–æŸ¥è¯¢å…³ç³»çš„è¡¨ç¤º
            query_repr = relation_reprs[0, query_rel_idx, :]  # [embedding_dim]
            all_reprs = relation_reprs[0, :, :]  # [num_relations, embedding_dim]
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            query_norm = F.normalize(query_repr, p=2, dim=0)
            all_norms = F.normalize(all_reprs, p=2, dim=1)
            similarities = torch.matmul(query_norm.unsqueeze(0), all_norms.t()).squeeze(0)
            
            # æ’é™¤æŸ¥è¯¢å…³ç³»æœ¬èº«
            similarities[query_rel_idx] = -1.0
            
            # æ‰¾åˆ°ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„å…³ç³»
            above_threshold = similarities > threshold
            valid_indices = torch.where(above_threshold)[0]
            
            similar_rels = [(idx.item(), similarities[idx].item()) for idx in valid_indices]
            similar_rels.sort(key=lambda x: x[1], reverse=True)
            
            return similar_rels
        except Exception as e:
            print(f"âš ï¸  æŸ¥æ‰¾ç›¸ä¼¼å…³ç³»æ—¶å‡ºé”™: {e}")
            return []

def check_reference_effectiveness(similar_rels, train_triples, test_triple, entity_vocab, relation_vocab):
    """
    æ£€æŸ¥ç›¸ä¼¼å…³ç³»çš„æœ‰æ•ˆæ€§ï¼ˆæ”¹è¿›ç‰ˆï¼šä½¿ç”¨æœ‰æ•ˆæ€§åˆ†æ•°è¿›è¡Œé‡åŒ–ï¼‰
    
    Args:
        similar_rels: list of (rel_idx, similarity) tuples
        train_triples: list of (h, r, t) tuples from training data
        test_triple: (h, r, t) tuple from test data
        entity_vocab: entity vocabulary
        relation_vocab: relation vocabulary
    
    Returns:
        effective_refs: list of (rel_idx, similarity, effectiveness_score, reason) tuples
        noise_refs: list of (rel_idx, similarity) tuples
    """
    test_h, test_r, test_t = test_triple
    
    # æ„å»ºè®­ç»ƒæ•°æ®çš„å…³ç³»-å®ä½“å¯¹é›†åˆï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼‰
    # å¯¹äºtailé¢„æµ‹ï¼š(h, r) -> {t1, t2, ...}
    # å¯¹äºheadé¢„æµ‹ï¼š(r, t) -> {h1, h2, ...}
    train_tail_contexts = defaultdict(set)  # (h, r) -> {tails}
    train_head_contexts = defaultdict(set)  # (r, t) -> {heads}
    
    for h, r, t in train_triples:
        train_tail_contexts[(h, r)].add(t)
        train_head_contexts[(r, t)].add(h)
    
    effective_refs = []
    noise_refs = []
    
    # æœ‰æ•ˆæ€§é˜ˆå€¼ï¼šåªæœ‰åˆ†æ•° >= 0.3 æ‰ç®—æœ‰æ•ˆ
    effectiveness_threshold = 0.3
    
    # æ£€æŸ¥æ¯ä¸ªç›¸ä¼¼å…³ç³»
    for rel_idx, similarity in similar_rels:
        tail_score = 0.0
        head_score = 0.0
        tail_reason = ""
        head_reason = ""
        
        # ========== Tailé¢„æµ‹æœ‰æ•ˆæ€§è¯„åˆ† ==========
        if (test_h, rel_idx) in train_tail_contexts:
            candidate_tails = train_tail_contexts[(test_h, rel_idx)]
            candidate_size = len(candidate_tails)
            
            # æƒ…å†µ1ï¼šç›´æ¥åŒ¹é…ï¼ˆæœ€é«˜åˆ†ï¼‰
            if test_t in candidate_tails:
                # ç›´æ¥åŒ¹é…ï¼šåˆ†æ•° = 1.0 * ç›¸ä¼¼åº¦æƒé‡
                # å€™é€‰é›†åˆè¶Šå°ï¼ŒåŒ¹é…ä»·å€¼è¶Šé«˜ï¼ˆä¿¡æ¯æ›´ç²¾ç¡®ï¼‰
                size_factor = 1.0 / (1.0 + np.log10(max(candidate_size, 1)))
                tail_score = 1.0 * similarity * size_factor
                tail_reason = f"direct_match(size={candidate_size})"
            else:
                # æƒ…å†µ2ï¼šæä¾›å€™é€‰é›†åˆä½†test_tä¸åœ¨å…¶ä¸­
                # åˆ†æ•° = åŸºç¡€åˆ† * ç›¸ä¼¼åº¦æƒé‡ * å€™é€‰é›†åˆè´¨é‡
                if candidate_size > 0:
                    # å€™é€‰é›†åˆè¶Šå°ï¼Œä»·å€¼è¶Šé«˜ï¼ˆæ›´ç²¾ç¡®çš„çº¦æŸï¼‰
                    # ä½†å› ä¸ºæ²¡æœ‰ç›´æ¥åŒ¹é…ï¼Œåˆ†æ•°è¾ƒä½
                    size_factor = 1.0 / (1.0 + np.log10(candidate_size))
                    # åŸºç¡€åˆ†ï¼š0.4ï¼ˆæä¾›ä¸Šä¸‹æ–‡ä½†æœªç›´æ¥åŒ¹é…ï¼‰
                    base_score = 0.4
                    tail_score = base_score * similarity * size_factor
                    tail_reason = f"context_only(size={candidate_size})"
        
        # ========== Headé¢„æµ‹æœ‰æ•ˆæ€§è¯„åˆ† ==========
        if (rel_idx, test_t) in train_head_contexts:
            candidate_heads = train_head_contexts[(rel_idx, test_t)]
            candidate_size = len(candidate_heads)
            
            # æƒ…å†µ1ï¼šç›´æ¥åŒ¹é…ï¼ˆæœ€é«˜åˆ†ï¼‰
            if test_h in candidate_heads:
                size_factor = 1.0 / (1.0 + np.log10(max(candidate_size, 1)))
                head_score = 1.0 * similarity * size_factor
                head_reason = f"direct_match(size={candidate_size})"
            else:
                # æƒ…å†µ2ï¼šæä¾›å€™é€‰é›†åˆä½†test_hä¸åœ¨å…¶ä¸­
                if candidate_size > 0:
                    size_factor = 1.0 / (1.0 + np.log10(candidate_size))
                    base_score = 0.4
                    head_score = base_score * similarity * size_factor
                    head_reason = f"context_only(size={candidate_size})"
        
        # é€‰æ‹©tailå’Œheadä¸­åˆ†æ•°æ›´é«˜çš„ä½œä¸ºæœ€ç»ˆåˆ†æ•°
        max_score = max(tail_score, head_score)
        best_reason = tail_reason if tail_score >= head_score else head_reason
        direction = 'tail' if tail_score >= head_score else 'head'
        
        # åˆ¤æ–­æ˜¯å¦æœ‰æ•ˆ
        if max_score >= effectiveness_threshold:
            effective_refs.append((rel_idx, similarity, max_score, direction, best_reason))
        else:
            noise_refs.append((rel_idx, similarity, max_score))  # ä¹Ÿè®°å½•åˆ†æ•°ï¼Œä¾¿äºåˆ†æ
    
    return effective_refs, noise_refs

def find_checkpoint_path(dataset_name, base_checkpoint='ckpts/optuna_1.pth'):
    """
    æŸ¥æ‰¾æ•°æ®é›†çš„checkpointè·¯å¾„
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        base_checkpoint: åŸºç¡€checkpointè·¯å¾„
    
    Returns:
        checkpoint_path: checkpointè·¯å¾„
    """
    flags = load_flags()
    base_path = flags.get('base_path', '/T20030104/ynj/semma')
    
    # é¦–å…ˆå°è¯•ä½¿ç”¨åŸºç¡€checkpoint
    base_ckpt_path = os.path.join(base_path, base_checkpoint)
    if os.path.exists(base_ckpt_path):
        return base_ckpt_path
    
    # å°è¯•åœ¨optuna_1_outputä¸­æŸ¥æ‰¾æ•°æ®é›†ç‰¹å®šçš„checkpoint
    output_dir = os.path.join(base_path, 'optuna_1_output', 'Ultra')
    
    # æ•°æ®é›†åç§°æ˜ å°„ï¼ˆä¸load_datasetä¸­çš„æ˜ å°„ä¿æŒä¸€è‡´ï¼‰
    dataset_name_mapping = {
        'YAGO310-ht': 'YAGO310',
        'ConceptNet 100k-ht': 'ConceptNet100k',
        'WDsinger-ht': 'WDsinger',
        'AristoV4-ht': 'AristoV4',
        'FB15K237Inductive:v1': 'FB15k237Inductive',
        'FB15K237Inductive:v2': 'FB15k237Inductive',
        'FB15K237Inductive:v3': 'FB15k237Inductive',
        'FB15K237Inductive:v4': 'FB15k237Inductive',
        'WN18RRInductive:v3': 'WN18RRInductive',
        'NELLInductive:v1': 'NELLInductive',
        'NELLInductive:v3': 'NELLInductive',
        'NELLInductive:v4': 'NELLInductive',
        'WKIngram:25': 'WKIngram',
        'NLIngram:25': 'NLIngram',
        'NLIngram:75': 'NLIngram',
        'Metafam': 'Metafam',
        'WikiTopicsMT1:health': 'WikiTopicsMT1',
        'WikiTopicsMT3:infra': 'WikiTopicsMT3',
    }
    
    # å¦‚æœä¸åœ¨æ˜ å°„ä¸­ï¼Œå°è¯•å¤„ç†ç‰ˆæœ¬å·
    if dataset_name not in dataset_name_mapping and ':' in dataset_name:
        parts = dataset_name.split(':')
        if len(parts) == 2:
            base_name = parts[0]
            if base_name == 'FB15K237Inductive':
                mapped_name = 'FB15k237Inductive'
            elif base_name in ['WKIngram', 'NLIngram']:
                mapped_name = base_name
            elif base_name.startswith('WikiTopicsMT'):
                mapped_name = base_name
            else:
                mapped_name = dataset_name_mapping.get(dataset_name, dataset_name)
        else:
            mapped_name = dataset_name_mapping.get(dataset_name, dataset_name)
    else:
        mapped_name = dataset_name_mapping.get(dataset_name, dataset_name)
    
    # æŸ¥æ‰¾æ•°æ®é›†æ–‡ä»¶å¤¹
    dataset_output_dir = os.path.join(output_dir, mapped_name)
    if os.path.exists(dataset_output_dir):
        # æŸ¥æ‰¾æœ€æ–°çš„checkpointæ–‡ä»¶
        import glob
        ckpt_files = glob.glob(os.path.join(dataset_output_dir, '**', '*.pth'), recursive=True)
        if ckpt_files:
            # è¿”å›æœ€æ–°çš„checkpoint
            ckpt_files.sort(key=os.path.getmtime, reverse=True)
            return ckpt_files[0]
    
    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›åŸºç¡€checkpointè·¯å¾„ï¼ˆå³ä½¿ä¸å­˜åœ¨ï¼‰
    return base_ckpt_path

def analyze_dataset_samples(dataset_name, dataset_type, checkpoint_path=None, num_samples=None, device='cuda'):
    """
    åˆ†ææ•°æ®é›†æ ·æœ¬çº§åˆ«çš„ç›¸ä¼¼å…³ç³»å‚è€ƒæƒ…å†µ
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        dataset_type: æ•°æ®é›†ç±»å‹
        checkpoint_path: æ¨¡å‹checkpointè·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œä¼šè‡ªåŠ¨æŸ¥æ‰¾ï¼‰
        num_samples: åˆ†æçš„æ ·æœ¬æ•°é‡ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ†ææ‰€æœ‰æµ‹è¯•æ ·æœ¬ï¼‰
        device: è®¾å¤‡
    """
    print(f"\n{'='*70}")
    print(f"ğŸ“Š åˆ†ææ•°æ®é›†: {dataset_name} ({dataset_type})")
    print(f"{'='*70}")
    
    # åŠ è½½æ•°æ®é›†
    dataset, train_data, valid_data, test_data = load_dataset(dataset_name, dataset_type)
    if dataset is None:
        return None
    
    # æŸ¥æ‰¾checkpointè·¯å¾„
    if checkpoint_path is None:
        checkpoint_path = find_checkpoint_path(dataset_name)
        print(f"ğŸ“ ä½¿ç”¨checkpoint: {checkpoint_path}")
    
    # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨EnhancedUltraï¼Œå› ä¸ºAREå°±æ˜¯EnhanceUltraï¼‰
    model = load_model(checkpoint_path, dataset, device)
    
    # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    test_data = test_data.to(device)
    train_data = train_data.to(device)
    
    # è·å–æµ‹è¯•ä¸‰å…ƒç»„
    test_triplets = torch.cat([test_data.target_edge_index, test_data.target_edge_type.unsqueeze(0)]).t()
    total_test_samples = len(test_triplets)
    
    # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šäº†num_samplesï¼‰
    if num_samples is not None and total_test_samples > num_samples:
        indices = torch.randperm(total_test_samples)[:num_samples]
        test_triplets = test_triplets[indices]
        print(f"âš ï¸  é™åˆ¶åˆ†ææ ·æœ¬æ•°é‡ä¸º: {num_samples} (æ€»æµ‹è¯•æ ·æœ¬æ•°: {total_test_samples})")
    else:
        print(f"ğŸ“Š åˆ†ææ‰€æœ‰æµ‹è¯•æ ·æœ¬: {total_test_samples} ä¸ª")
    
    # è·å–è®­ç»ƒä¸‰å…ƒç»„ï¼ˆç”¨äºæœ‰æ•ˆæ€§æ£€æŸ¥ï¼‰
    train_triplets = torch.cat([train_data.edge_index, train_data.edge_type.unsqueeze(0)]).t()
    train_triples_list = [(int(h.item()), int(r.item()), int(t.item())) for h, r, t in train_triplets]
    
    # æ„å»ºå®ä½“å’Œå…³ç³»è¯æ±‡è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨ç´¢å¼•ï¼‰
    num_entities = dataset[0].num_nodes
    num_relations = dataset[0].num_relations
    entity_vocab = {i: i for i in range(num_entities)}
    relation_vocab = {i: i for i in range(num_relations)}
    
    # ç»Ÿè®¡æ•°æ®
    stats = {
        'total_samples': 0,
        'samples_with_references': 0,
        'total_references': 0,
        'effective_references': 0,
        'noise_references': 0,
        'samples_with_effective_refs': 0,
        'samples_with_only_noise': 0,
        'total_effectiveness_score': 0.0,  # æ€»æœ‰æ•ˆæ€§åˆ†æ•°
        'direct_match_count': 0,  # ç›´æ¥åŒ¹é…çš„æ•°é‡
        'context_only_count': 0,  # ä»…æä¾›ä¸Šä¸‹æ–‡çš„æ•°é‡
    }
    
    sample_results = []
    
    # è·å–ç›¸ä¼¼åº¦é˜ˆå€¼
    flags = load_flags()
    threshold = flags.get('similarity_threshold_init', 0.8)
    
    print(f"\nğŸ” åˆ†æ {len(test_triplets)} ä¸ªæµ‹è¯•æ ·æœ¬...")
    
    for i, triplet in enumerate(tqdm(test_triplets, desc="åˆ†ææ ·æœ¬")):
        h, t, r = int(triplet[0]), int(triplet[1]), int(triplet[2])
        
        stats['total_samples'] += 1
        
        # æ‰¾åˆ°ç›¸ä¼¼å…³ç³»ï¼ˆtest_dataå·²ç»åœ¨deviceä¸Šäº†ï¼‰
        similar_rels = find_similar_relations(model, test_data, r, threshold=threshold, device=device)
        
        if len(similar_rels) == 0:
            sample_results.append({
                'sample_idx': i,
                'test_triple': (h, r, t),
                'num_references': 0,
                'num_effective': 0,
                'num_noise': 0,
                'has_references': False,
                'has_effective_refs': False,
                'has_only_noise': False,
            })
            continue
        
        stats['samples_with_references'] += 1
        stats['total_references'] += len(similar_rels)
        
        # æ£€æŸ¥æœ‰æ•ˆæ€§
        effective_refs, noise_refs = check_reference_effectiveness(
            similar_rels, train_triples_list, (h, r, t), entity_vocab, relation_vocab
        )
        
        stats['effective_references'] += len(effective_refs)
        stats['noise_references'] += len(noise_refs)
        
        # ç»Ÿè®¡æœ‰æ•ˆæ€§åˆ†æ•°å’ŒåŒ¹é…ç±»å‹
        for ref in effective_refs:
            # refæ ¼å¼: (rel_idx, similarity, max_score, direction, reason)
            if len(ref) >= 5:
                score = ref[2]
                reason = ref[4]
                stats['total_effectiveness_score'] += score
                if 'direct_match' in reason:
                    stats['direct_match_count'] += 1
                elif 'context_only' in reason:
                    stats['context_only_count'] += 1
        
        # ä¹Ÿç»Ÿè®¡å™ªéŸ³çš„åˆ†æ•°ï¼ˆè™½ç„¶å®ƒä»¬ä½äºé˜ˆå€¼ï¼‰
        for ref in noise_refs:
            # refæ ¼å¼: (rel_idx, similarity, max_score)
            if len(ref) >= 3:
                score = ref[2]
                stats['total_effectiveness_score'] += score  # ä¹Ÿè®¡å…¥æ€»åˆ†ï¼Œç”¨äºè®¡ç®—å¹³å‡å€¼
        
        if len(effective_refs) > 0:
            stats['samples_with_effective_refs'] += 1
        
        if len(effective_refs) == 0 and len(noise_refs) > 0:
            stats['samples_with_only_noise'] += 1
        
        sample_results.append({
            'sample_idx': i,
            'test_triple': (h, r, t),
            'num_references': len(similar_rels),
            'num_effective': len(effective_refs),
            'num_noise': len(noise_refs),
            'has_references': True,
            'has_effective_refs': len(effective_refs) > 0,
            'has_only_noise': len(effective_refs) == 0 and len(noise_refs) > 0,
            'effective_refs': effective_refs,
            'noise_refs': noise_refs,
        })
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    if stats['total_samples'] > 0:
        stats['reference_rate'] = stats['samples_with_references'] / stats['total_samples']
        stats['effective_rate'] = stats['samples_with_effective_refs'] / stats['total_samples']
        stats['noise_rate'] = stats['samples_with_only_noise'] / stats['total_samples']
    
    if stats['total_references'] > 0:
        stats['reference_effectiveness'] = stats['effective_references'] / stats['total_references']
        stats['reference_noise_ratio'] = stats['noise_references'] / stats['total_references']
        # å¹³å‡æœ‰æ•ˆæ€§åˆ†æ•°
        stats['avg_effectiveness_score'] = stats['total_effectiveness_score'] / stats['total_references']
    else:
        stats['avg_effectiveness_score'] = 0.0
    
    if stats['effective_references'] > 0:
        # ä»sample_resultsä¸­æå–æ‰€æœ‰æœ‰æ•ˆå‚è€ƒçš„åˆ†æ•°
        all_effective_scores = []
        for sample_result in sample_results:
            if 'effective_refs' in sample_result:
                for ref in sample_result['effective_refs']:
                    if len(ref) >= 3:
                        all_effective_scores.append(ref[2])
        
        if len(all_effective_scores) > 0:
            stats['avg_effective_score'] = sum(all_effective_scores) / len(all_effective_scores)
        else:
            stats['avg_effective_score'] = 0.0
        
        # ç›´æ¥åŒ¹é…æ¯”ä¾‹
        stats['direct_match_ratio'] = stats['direct_match_count'] / stats['effective_references']
        stats['context_only_ratio'] = stats['context_only_count'] / stats['effective_references']
    else:
        stats['avg_effective_score'] = 0.0
        stats['direct_match_ratio'] = 0.0
        stats['context_only_ratio'] = 0.0
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print(f"\nğŸ“ˆ ç»Ÿè®¡ç»“æœ:")
    print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
    print(f"  æœ‰å‚è€ƒçš„æ ·æœ¬æ•°: {stats['samples_with_references']} ({stats.get('reference_rate', 0)*100:.2f}%)")
    print(f"  æ€»å‚è€ƒæ•°: {stats['total_references']}")
    print(f"  æœ‰æ•ˆå‚è€ƒæ•°: {stats['effective_references']} ({stats.get('reference_effectiveness', 0)*100:.2f}%)")
    print(f"  å™ªéŸ³å‚è€ƒæ•°: {stats['noise_references']} ({stats.get('reference_noise_ratio', 0)*100:.2f}%)")
    print(f"  æœ‰æœ‰æ•ˆå‚è€ƒçš„æ ·æœ¬æ•°: {stats['samples_with_effective_refs']} ({stats.get('effective_rate', 0)*100:.2f}%)")
    print(f"  åªæœ‰å™ªéŸ³çš„æ ·æœ¬æ•°: {stats['samples_with_only_noise']} ({stats.get('noise_rate', 0)*100:.2f}%)")
    print(f"\nğŸ“Š æœ‰æ•ˆæ€§é‡åŒ–æŒ‡æ ‡:")
    print(f"  å¹³å‡æœ‰æ•ˆæ€§åˆ†æ•°: {stats.get('avg_effectiveness_score', 0):.4f} (æ‰€æœ‰å‚è€ƒ)")
    print(f"  æœ‰æ•ˆå‚è€ƒå¹³å‡åˆ†æ•°: {stats.get('avg_effective_score', 0):.4f} (ä»…æœ‰æ•ˆå‚è€ƒ)")
    print(f"  ç›´æ¥åŒ¹é…æ•°: {stats['direct_match_count']} ({stats.get('direct_match_ratio', 0)*100:.2f}% of effective)")
    print(f"  ä»…ä¸Šä¸‹æ–‡æ•°: {stats['context_only_count']} ({stats.get('context_only_ratio', 0)*100:.2f}% of effective)")
    
    return {
        'dataset_name': dataset_name,
        'dataset_type': dataset_type,
        'stats': stats,
        'sample_results': sample_results,
    }

def visualize_results(all_results, output_dir='analyze/figures'):
    """å¯è§†åŒ–åˆ†æç»“æœ"""
    os.makedirs(output_dir, exist_ok=True)
    
    # å‡†å¤‡æ•°æ®
    datasets = []
    reference_rates = []
    effectiveness_rates = []
    noise_rates = []
    
    for result in all_results:
        if result is None:
            continue
        datasets.append(result['dataset_name'])
        stats = result['stats']
        reference_rates.append(stats.get('reference_rate', 0) * 100)
        effectiveness_rates.append(stats.get('reference_effectiveness', 0) * 100)
        noise_rates.append(stats.get('reference_noise_ratio', 0) * 100)
    
    if len(datasets) == 0:
        print("âš ï¸  æ²¡æœ‰æ•°æ®å¯å¯è§†åŒ–")
        return
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. å‚è€ƒç‡å¯¹æ¯”
    ax1 = axes[0, 0]
    bars1 = ax1.bar(range(len(datasets)), reference_rates, color='skyblue')
    ax1.set_xlabel('Dataset', fontsize=12)
    ax1.set_ylabel('Reference Rate (%)', fontsize=12)
    ax1.set_title('Percentage of Samples with References', fontsize=14, fontweight='bold')
    ax1.set_xticks(range(len(datasets)))
    ax1.set_xticklabels(datasets, rotation=45, ha='right')
    ax1.grid(axis='y', alpha=0.3)
    for i, v in enumerate(reference_rates):
        ax1.text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # 2. æœ‰æ•ˆæ€§ç‡å¯¹æ¯”
    ax2 = axes[0, 1]
    bars2 = ax2.bar(range(len(datasets)), effectiveness_rates, color='lightgreen')
    ax2.set_xlabel('Dataset', fontsize=12)
    ax2.set_ylabel('Effectiveness Rate (%)', fontsize=12)
    ax2.set_title('Percentage of Effective References', fontsize=14, fontweight='bold')
    ax2.set_xticks(range(len(datasets)))
    ax2.set_xticklabels(datasets, rotation=45, ha='right')
    ax2.grid(axis='y', alpha=0.3)
    for i, v in enumerate(effectiveness_rates):
        ax2.text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # 3. å™ªéŸ³ç‡å¯¹æ¯”
    ax3 = axes[1, 0]
    bars3 = ax3.bar(range(len(datasets)), noise_rates, color='lightcoral')
    ax3.set_xlabel('Dataset', fontsize=12)
    ax3.set_ylabel('Noise Rate (%)', fontsize=12)
    ax3.set_title('Percentage of Noise References', fontsize=14, fontweight='bold')
    ax3.set_xticks(range(len(datasets)))
    ax3.set_xticklabels(datasets, rotation=45, ha='right')
    ax3.grid(axis='y', alpha=0.3)
    for i, v in enumerate(noise_rates):
        ax3.text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # 4. ç»¼åˆå¯¹æ¯”ï¼ˆå †å æŸ±çŠ¶å›¾ï¼‰
    ax4 = axes[1, 1]
    x = np.arange(len(datasets))
    width = 0.6
    effective_bars = ax4.bar(x, effectiveness_rates, width, label='Effective', color='lightgreen')
    noise_bars = ax4.bar(x, noise_rates, width, bottom=effectiveness_rates, label='Noise', color='lightcoral')
    ax4.set_xlabel('Dataset', fontsize=12)
    ax4.set_ylabel('Rate (%)', fontsize=12)
    ax4.set_title('Effective vs Noise References', fontsize=14, fontweight='bold')
    ax4.set_xticks(x)
    ax4.set_xticklabels(datasets, rotation=45, ha='right')
    ax4.legend()
    ax4.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, '28_sample_level_reference_analysis.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ… å›¾è¡¨å·²ä¿å­˜: {output_path}")
    plt.close()

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('analyze/output_log.txt'),
            logging.StreamHandler()
        ]
    )

    # è¯»å–æ˜¾è‘—æå‡å’Œä¸‹é™çš„æ•°æ®é›†
    csv_path = 'analyze/common_features_analysis.csv'
    if not os.path.exists(csv_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return
    
    df = pd.read_csv(csv_path)
    
    # ç­›é€‰æ˜¾è‘—æå‡å’Œä¸‹é™çš„æ•°æ®é›†ï¼Œä¸”åªé€‰æ‹©Inductive(e,r)ç±»å‹
    improved = df[(df['performance_category'] == 'significantly_improved') & (df['dataset_type'] == 'Inductive(e,r)')]
    degraded = df[(df['performance_category'] == 'significantly_degraded') & (df['dataset_type'] == 'Inductive(e,r)')]
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(improved)} ä¸ªæ˜¾è‘—æå‡çš„Inductive(e,r)æ•°æ®é›†")
    print(f"ğŸ“Š æ‰¾åˆ° {len(degraded)} ä¸ªæ˜¾è‘—ä¸‹é™çš„Inductive(e,r)æ•°æ®é›†")
    
    # åˆ†ææ‰€æœ‰æ˜¾è‘—æå‡å’Œä¸‹é™çš„Inductive(e,r)æ•°æ®é›†
    key_datasets = []
    
    # æ˜¾è‘—æå‡çš„æ•°æ®é›†ï¼ˆåªåŒ…å«Inductive(e,r)ï¼‰
    for _, row in improved.iterrows():
        dataset_name = row['dataset']
        dataset_type = row['dataset_type']
        key_datasets.append((dataset_name, dataset_type, 'improved'))
    
    # æ˜¾è‘—ä¸‹é™çš„æ•°æ®é›†ï¼ˆåªåŒ…å«Inductive(e,r)ï¼‰
    for _, row in degraded.iterrows():
        dataset_name = row['dataset']
        dataset_type = row['dataset_type']
        key_datasets.append((dataset_name, dataset_type, 'degraded'))
    
    # é¢å¤–æ·»åŠ ç”¨æˆ·æŒ‡å®šçš„æ•°æ®é›†ï¼ˆå³ä½¿å®ƒä»¬è¢«æ ‡è®°ä¸ºstableï¼Œä½†å˜åŒ–è¾ƒå¤§ï¼‰
    additional_datasets = {
        'WikiTopicsMT2:sci': 'degraded',  # ä¸‹é™è¾ƒå¤š
        'NLIngram:0': 'degraded',  # ä¸‹é™è¾ƒå¤š
        'WikiTopicsMT1:tax': 'improved',  # æå‡è¾ƒå¤š
        'WikiTopicsMT3:art': 'improved',  # æå‡è¾ƒå¤š
    }
    
    for dataset_name, category in additional_datasets.items():
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨ä¸”æ˜¯Inductive(e,r)ç±»å‹
        dataset_row = df[(df['dataset'] == dataset_name) & (df['dataset_type'] == 'Inductive(e,r)')]
        if len(dataset_row) > 0:
            dataset_type = dataset_row.iloc[0]['dataset_type']
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ·»åŠ è¿‡ï¼ˆé¿å…é‡å¤ï¼‰
            if (dataset_name, dataset_type, category) not in key_datasets:
                key_datasets.append((dataset_name, dataset_type, category))
                print(f"â• é¢å¤–æ·»åŠ æ•°æ®é›†: {dataset_name} ({category})")
    
    print(f"\nğŸ” å°†åˆ†ææ‰€æœ‰ {len(key_datasets)} ä¸ªæ•°æ®é›†")
    
    # åˆ†ææ¯ä¸ªæ•°æ®é›†
    all_results = []
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    for dataset_name, dataset_type, category in key_datasets:
        try:
            result = analyze_dataset_samples(
                dataset_name=dataset_name,
                dataset_type=dataset_type,
                checkpoint_path=None,  # ä½¿ç”¨é»˜è®¤è·¯å¾„
                num_samples=None,  # åˆ†ææ‰€æœ‰æµ‹è¯•æ ·æœ¬
                device=device
            )
            if result:
                result['category'] = category
                all_results.append(result)
        except Exception as e:
            print(f"âŒ åˆ†ææ•°æ®é›† {dataset_name} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exception(*sys.exc_info())
            continue
    
    # å¯è§†åŒ–ç»“æœ
    if len(all_results) > 0:
        visualize_results(all_results)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        output_csv = 'analyze/sample_level_reference_analysis.csv'
        rows = []
        for result in all_results:
            if result is None:
                continue
            stats = result['stats']
            rows.append({
                'dataset': result['dataset_name'],
                'category': result.get('category', 'unknown'),
                'total_samples': stats['total_samples'],
                'samples_with_references': stats['samples_with_references'],
                'reference_rate': stats.get('reference_rate', 0),
                'total_references': stats['total_references'],
                'effective_references': stats['effective_references'],
                'reference_effectiveness': stats.get('reference_effectiveness', 0),
                'noise_references': stats['noise_references'],
                'reference_noise_ratio': stats.get('reference_noise_ratio', 0),
                'samples_with_effective_refs': stats['samples_with_effective_refs'],
                'effective_rate': stats.get('effective_rate', 0),
                'samples_with_only_noise': stats['samples_with_only_noise'],
                'noise_rate': stats.get('noise_rate', 0),
                # æ–°å¢é‡åŒ–æŒ‡æ ‡
                'avg_effectiveness_score': stats.get('avg_effectiveness_score', 0),
                'avg_effective_score': stats.get('avg_effective_score', 0),
                'direct_match_count': stats.get('direct_match_count', 0),
                'direct_match_ratio': stats.get('direct_match_ratio', 0),
                'context_only_count': stats.get('context_only_count', 0),
                'context_only_ratio': stats.get('context_only_ratio', 0),
            })
        
        df_results = pd.DataFrame(rows)
        df_results.to_csv(output_csv, index=False)
        print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_csv}")
    else:
        print("\nâŒ æ²¡æœ‰è·å¾—ä»»ä½•åˆ†æç»“æœ")

if __name__ == '__main__':
    main()

