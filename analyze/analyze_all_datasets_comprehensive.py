#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢åˆ†ækg-datasetsç›®å½•ä¸‹çš„æ‰€æœ‰æ•°æ®é›†
æå–é‡åŒ–è¯æ®ï¼Œæ‰¾å‡ºæ˜¾è‘—æå‡å’Œä¸‹é™çš„åŸå› 
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from collections import Counter, defaultdict
from scipy import stats
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# è®¾ç½®å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

sns.set_style("whitegrid")
sns.set_palette("husl")

def load_flags():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    flags_path = Path(__file__).parent.parent / "flags.yaml"
    with open(flags_path, 'r') as f:
        flags = yaml.safe_load(f)
    return flags

def find_all_datasets():
    """æ‰¾åˆ°æ‰€æœ‰æ•°æ®é›†"""
    flags = load_flags()
    kg_datasets_path = flags.get('kg_datasets_path', '/T20030104/ynj/semma/kg-datasets')
    
    datasets = []
    
    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰åŒ…å«train.txtçš„ç›®å½•
    for root, dirs, files in os.walk(kg_datasets_path):
        if 'train.txt' in files:
            # è·å–æ•°æ®é›†åç§°ï¼ˆç›®å½•åï¼‰
            dataset_path = root
            relative_path = os.path.relpath(dataset_path, kg_datasets_path)
            dataset_name = relative_path.replace(os.sep, '/')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰rawç›®å½•
            raw_dir = os.path.join(dataset_path, 'raw')
            if os.path.exists(raw_dir) and os.path.exists(os.path.join(raw_dir, 'train.txt')):
                datasets.append({
                    'name': dataset_name,
                    'raw_dir': raw_dir,
                    'full_path': dataset_path
                })
            elif os.path.exists(os.path.join(dataset_path, 'train.txt')):
                datasets.append({
                    'name': dataset_name,
                    'raw_dir': dataset_path,
                    'full_path': dataset_path
                })
    
    return datasets

def load_triples_file(filepath):
    """åŠ è½½ä¸‰å…ƒç»„æ–‡ä»¶"""
    if not os.path.exists(filepath):
        return []
    triples = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            parts = line.split('\t')
            if len(parts) >= 3:
                h, r, t = parts[0], parts[1], parts[2]
                triples.append((h, r, t))
    return triples

def calculate_gini_coefficient(values):
    """è®¡ç®—åŸºå°¼ç³»æ•°"""
    if len(values) == 0:
        return 0.0
    values = np.array(values)
    values = values.flatten()
    values = np.sort(values)
    n = len(values)
    if n == 0 or np.sum(values) == 0:
        return 0.0
    index = np.arange(1, n + 1)
    return (2 * np.sum(index * values)) / (n * np.sum(values)) - (n + 1) / n

def analyze_dataset_structure(dataset_info):
    """åˆ†ææ•°æ®é›†çš„ç»“æ„ç‰¹å¾"""
    raw_dir = dataset_info['raw_dir']
    dataset_name = dataset_info['name']
    
    # åŠ è½½ä¸‰å…ƒç»„
    train_file = os.path.join(raw_dir, 'train.txt')
    valid_file = os.path.join(raw_dir, 'valid.txt')
    test_file = os.path.join(raw_dir, 'test.txt')
    
    train_triples = load_triples_file(train_file)
    valid_triples = load_triples_file(valid_file)
    test_triples = load_triples_file(test_file)
    
    all_triples = train_triples + valid_triples + test_triples
    
    if len(all_triples) == 0:
        return None
    
    # æ„å»ºè¯æ±‡è¡¨
    entities = set()
    relations = set()
    for h, r, t in all_triples:
        entities.add(h)
        entities.add(t)
        relations.add(r)
    
    num_entities = len(entities)
    num_relations = len(relations)
    num_triples = len(all_triples)
    
    # å…³ç³»é¢‘ç‡ç»Ÿè®¡
    relation_counts = Counter([r for _, r, _ in all_triples])
    relation_frequencies = list(relation_counts.values())
    
    # è®¡ç®—æŒ‡æ ‡
    gini_coefficient = calculate_gini_coefficient(relation_frequencies)
    avg_relation_freq = np.mean(relation_frequencies) if len(relation_frequencies) > 0 else 0
    std_relation_freq = np.std(relation_frequencies) if len(relation_frequencies) > 0 else 0
    cv_relation_freq = std_relation_freq / avg_relation_freq if avg_relation_freq > 0 else 0
    
    # Top-10%æ¯”ä¾‹
    sorted_freqs = sorted(relation_frequencies, reverse=True)
    top_10_percent = int(max(1, len(sorted_freqs) * 0.1))
    top_10_percent_freq = sum(sorted_freqs[:top_10_percent])
    total_freq = sum(sorted_freqs)
    top_10_percent_ratio = top_10_percent_freq / total_freq if total_freq > 0 else 0
    
    # å…³ç³»-å®ä½“æ¯”ä¾‹
    relation_entity_ratio = num_relations / num_entities if num_entities > 0 else 0
    
    # å›¾çš„å¯†åº¦
    max_possible_edges = num_entities * num_entities
    graph_density = num_triples / max_possible_edges if max_possible_edges > 0 else 0
    
    # å…³ç³»çš„å¹³å‡åº¦
    avg_relation_degree = avg_relation_freq
    
    # å…³ç³»é¢‘ç‡çš„ç†µ
    probs = np.array(relation_frequencies) / total_freq if total_freq > 0 else np.array([0])
    probs = probs[probs > 0]
    entropy = -np.sum(probs * np.log2(probs + 1e-10)) if len(probs) > 0 else 0
    
    # å…³ç³»é¢‘ç‡çš„å˜å¼‚ç³»æ•°ï¼ˆæ ‡å‡†åŒ–ï¼‰
    cv_normalized = cv_relation_freq
    
    metrics = {
        'dataset_name': dataset_name,
        'num_entities': num_entities,
        'num_relations': num_relations,
        'num_triples': num_triples,
        'relation_entity_ratio': relation_entity_ratio,
        'gini_coefficient': gini_coefficient,
        'cv_relation_freq': cv_normalized,
        'top_10_percent_ratio': top_10_percent_ratio,
        'graph_density': graph_density,
        'avg_relation_degree': avg_relation_degree,
        'entropy': entropy,
        'avg_relation_freq': avg_relation_freq,
        'std_relation_freq': std_relation_freq,
    }
    
    return metrics

def classify_structure_level(metrics):
    """åˆ†ç±»ç»“æ„åŒ–ç¨‹åº¦"""
    if metrics is None:
        return 'unknown', 'No data'
    
    scores = []
    reasons = []
    
    # Giniç³»æ•°
    gini = metrics['gini_coefficient']
    if gini > 0.7:
        scores.append(2)
        reasons.append(f"High Gini ({gini:.3f})")
    elif gini > 0.5:
        scores.append(1)
        reasons.append(f"Medium Gini ({gini:.3f})")
    else:
        scores.append(0)
        reasons.append(f"Low Gini ({gini:.3f})")
    
    # CV
    cv = metrics['cv_relation_freq']
    if cv > 1.0:
        scores.append(2)
        reasons.append(f"High CV ({cv:.3f})")
    elif cv > 0.5:
        scores.append(1)
        reasons.append(f"Medium CV ({cv:.3f})")
    else:
        scores.append(0)
        reasons.append(f"Low CV ({cv:.3f})")
    
    # Top-10%
    top10 = metrics['top_10_percent_ratio']
    if top10 > 0.6:
        scores.append(2)
        reasons.append(f"High top-10% ({top10:.3f})")
    elif top10 > 0.4:
        scores.append(1)
        reasons.append(f"Medium top-10% ({top10:.3f})")
    else:
        scores.append(0)
        reasons.append(f"Low top-10% ({top10:.3f})")
    
    # å…³ç³»-å®ä½“æ¯”
    ratio = metrics['relation_entity_ratio']
    if ratio < 0.01:
        scores.append(2)
        reasons.append(f"Low ratio ({ratio:.4f})")
    elif ratio < 0.05:
        scores.append(1)
        reasons.append(f"Medium ratio ({ratio:.4f})")
    else:
        scores.append(0)
        reasons.append(f"High ratio ({ratio:.4f})")
    
    avg_score = np.mean(scores)
    
    if avg_score >= 1.5:
        level = 'high'
    elif avg_score >= 0.5:
        level = 'medium'
    else:
        level = 'low'
    
    reasoning = "; ".join(reasons)
    return level, reasoning

def match_dataset_name(dataset_name, performance_data):
    """åŒ¹é…æ•°æ®é›†åç§°"""
    # æ ‡å‡†åŒ–åç§°ï¼ˆä¿ç•™è·¯å¾„ä¿¡æ¯ç”¨äºInductiveåŒ¹é…ï¼‰
    name_lower = dataset_name.lower().replace('-', '').replace('_', '').replace('/', '')
    name_original = dataset_name.lower()  # ä¿ç•™åŸå§‹è·¯å¾„ç”¨äºç²¾ç¡®åŒ¹é…
    
    # ç‰¹æ®ŠåŒ¹é…è§„åˆ™ï¼ˆInductiveæ•°æ®é›† - ä¼˜å…ˆåŒ¹é…ï¼Œå¿…é¡»ç²¾ç¡®åŒ¹é…ï¼‰
    if 'grail/indfb15k237' in name_original or 'grail/indfb15k' in name_original:
        # æå–ç‰ˆæœ¬å·ï¼ˆä½¿ç”¨è·¯å¾„åˆ†éš”ç¬¦ç¡®ä¿ç²¾ç¡®åŒ¹é…ï¼‰
        if '/v1/' in name_original or '/v1/raw' in name_original:
            matched = 'FB15K237Inductive:v1'
            if matched in performance_data['dataset'].values:
                return matched
        elif '/v2/' in name_original or '/v2/raw' in name_original:
            matched = 'FB15K237Inductive:v2'
            if matched in performance_data['dataset'].values:
                return matched
        elif '/v3/' in name_original or '/v3/raw' in name_original:
            matched = 'FB15K237Inductive:v3'
            if matched in performance_data['dataset'].values:
                return matched
        elif '/v4/' in name_original or '/v4/raw' in name_original:
            matched = 'FB15K237Inductive:v4'
            if matched in performance_data['dataset'].values:
                return matched
    
    if 'grail/indwn18rr' in name_original or 'grail/indwn' in name_original:
        if '/v1/' in name_original or '/v1/raw' in name_original:
            matched = 'WN18RRInductive:v1'
            if matched in performance_data['dataset'].values:
                return matched
        elif '/v2/' in name_original or '/v2/raw' in name_original:
            matched = 'WN18RRInductive:v2'
            if matched in performance_data['dataset'].values:
                return matched
        elif '/v3/' in name_original or '/v3/raw' in name_original:
            matched = 'WN18RRInductive:v3'
            if matched in performance_data['dataset'].values:
                return matched
        elif '/v4/' in name_original or '/v4/raw' in name_original:
            matched = 'WN18RRInductive:v4'
            if matched in performance_data['dataset'].values:
                return matched
    
    if 'grail/indnell' in name_original:
        if '/v1/' in name_original or '/v1/raw' in name_original:
            matched = 'NELLInductive:v1'
            if matched in performance_data['dataset'].values:
                return matched
        elif '/v2/' in name_original or '/v2/raw' in name_original:
            matched = 'NELLInductive:v2'
            if matched in performance_data['dataset'].values:
                return matched
        elif '/v3/' in name_original or '/v3/raw' in name_original:
            matched = 'NELLInductive:v3'
            if matched in performance_data['dataset'].values:
                return matched
        elif '/v4/' in name_original or '/v4/raw' in name_original:
            matched = 'NELLInductive:v4'
            if matched in performance_data['dataset'].values:
                return matched
    
    # å…¶ä»–åŒ¹é…è§„åˆ™ï¼ˆéInductiveæ•°æ®é›†ï¼‰
    for perf_name in performance_data['dataset'].values:
        perf_name_lower = str(perf_name).lower().replace('-', '').replace('_', '').replace(' ', '')
        
        # è·³è¿‡Inductiveæ•°æ®é›†ï¼ˆå·²ç»åœ¨ä¸Šé¢å¤„ç†ï¼‰
        if 'inductive' in perf_name_lower:
            continue
        
        # ç›´æ¥åŒ¹é…
        if name_lower in perf_name_lower or perf_name_lower in name_lower:
            return perf_name
        
        # ç‰¹æ®ŠåŒ¹é…è§„åˆ™
        if 'yago310' in name_lower and 'yago310' in perf_name_lower:
            return perf_name
        if 'cnet100k' in name_lower and 'conceptnet' in perf_name_lower:
            return perf_name
        if 'nell995' in name_lower and 'nell995' in perf_name_lower:
            return perf_name
        if 'fb15k237' in name_lower and 'fb15k237' in perf_name_lower:
            return perf_name
        if 'wn18rr' in name_lower and 'wn18rr' in perf_name_lower:
            return perf_name
        if 'wd' in name_lower and 'singer' in name_lower and 'wdsinger' in perf_name_lower:
            return perf_name
        if 'aristov4' in name_lower and 'aristov4' in perf_name_lower:
            return perf_name
    
    return None

def analyze_all_datasets():
    """åˆ†ææ‰€æœ‰æ•°æ®é›†"""
    print("ğŸ” Finding all datasets in kg-datasets...")
    all_datasets = find_all_datasets()
    print(f"âœ… Found {len(all_datasets)} datasets")
    
    # åŠ è½½æ€§èƒ½æ•°æ®
    perf_file = Path(__file__).parent / "common_features_analysis.csv"
    perf_df = pd.read_csv(perf_file)
    
    results = []
    
    print("\nğŸ“Š Analyzing datasets...")
    for i, dataset_info in enumerate(all_datasets, 1):
        dataset_name = dataset_info['name']
        print(f"\n[{i}/{len(all_datasets)}] Analyzing {dataset_name}...")
        
        try:
            metrics = analyze_dataset_structure(dataset_info)
            
            if metrics:
                structure_level, reasoning = classify_structure_level(metrics)
                metrics['structure_level'] = structure_level
                metrics['reasoning'] = reasoning
                
                # å°è¯•åŒ¹é…æ€§èƒ½æ•°æ®
                matched_name = match_dataset_name(dataset_name, perf_df)
                if matched_name:
                    perf_row = perf_df[perf_df['dataset'] == matched_name].iloc[0]
                    metrics['matched_name'] = matched_name
                    metrics['mrr_diff'] = perf_row['mrr_diff']
                    metrics['performance_category'] = perf_row['performance_category']
                    metrics['semma_mrr'] = perf_row['semma_mrr']
                    print(f"   âœ… Matched with performance data: {matched_name} (MRR diff: {perf_row['mrr_diff']:.3f}, Category: {perf_row['performance_category']})")
                else:
                    metrics['matched_name'] = None
                    metrics['mrr_diff'] = None
                    metrics['performance_category'] = 'unknown'
                    metrics['semma_mrr'] = None
                    print(f"   âš ï¸  No performance data match")
                
                metrics['gini_coefficient'] = metrics.get('gini_coefficient', 0)
                metrics['cv_relation_freq'] = metrics.get('cv_relation_freq', 0)
                
                print(f"   ğŸ“ˆ Structure: {structure_level.upper()}, Gini: {metrics['gini_coefficient']:.3f}, CV: {metrics['cv_relation_freq']:.3f}")
                results.append(metrics)
            else:
                print(f"   âš ï¸  Cannot analyze (no data)")
        except Exception as e:
            print(f"   âŒ Error: {e}")
            import traceback
            traceback.print_exc()
    
    return pd.DataFrame(results)

def create_comprehensive_analysis(results_df):
    """åˆ›å»ºç»¼åˆåˆ†æ"""
    output_file = Path(__file__).parent / "comprehensive_quantitative_analysis.csv"
    results_df.to_csv(output_file, index=False)
    print(f"\nğŸ’¾ Results saved to {output_file}")
    
    # åˆ†ç¦»æå‡å’Œä¸‹é™çš„æ•°æ®é›†
    improved = results_df[results_df['performance_category'] == 'significantly_improved'].copy()
    degraded = results_df[results_df['performance_category'] == 'significantly_degraded'].copy()
    
    print(f"\nğŸ“Š Analysis Summary:")
    print(f"   Total datasets analyzed: {len(results_df)}")
    print(f"   Significantly improved: {len(improved)}")
    print(f"   Significantly degraded: {len(degraded)}")
    print(f"   With performance data: {len(results_df[results_df['matched_name'].notna()])}")
    
    if len(improved) > 0:
        print(f"\nâœ… Improved datasets (average Gini: {improved['gini_coefficient'].mean():.3f}):")
        for _, row in improved.iterrows():
            print(f"   - {row['matched_name']}: Gini={row['gini_coefficient']:.3f}, CV={row['cv_relation_freq']:.3f}, MRR+{row['mrr_diff']:.3f}")
    
    if len(degraded) > 0:
        print(f"\nâŒ Degraded datasets (average Gini: {degraded['gini_coefficient'].mean():.3f}):")
        for _, row in degraded.iterrows():
            print(f"   - {row['matched_name']}: Gini={row['gini_coefficient']:.3f}, CV={row['cv_relation_freq']:.3f}, MRR{row['mrr_diff']:.3f}")
    
    # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
    create_detailed_report(results_df, improved, degraded)

def create_detailed_report(results_df, improved, degraded):
    """åˆ›å»ºè¯¦ç»†æŠ¥å‘Š"""
    report_file = Path(__file__).parent / "QUANTITATIVE_EVIDENCE_REPORT.md"
    
    report = f"""# é‡åŒ–è¯æ®æŠ¥å‘Šï¼šæ˜¾è‘—æå‡å’Œä¸‹é™æ•°æ®é›†çš„å…¨é¢åˆ†æ

## æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘ŠåŸºäº**å®é™…æ•°æ®é›†æ–‡ä»¶**çš„é‡åŒ–åˆ†æï¼Œä¸ºæ‰€æœ‰æ˜¾è‘—æå‡å’Œä¸‹é™çš„æ•°æ®é›†æä¾›å®¢è§‚è¯æ®ã€‚

**åˆ†ææ•°æ®é›†æ€»æ•°**: {len(results_df)}  
**æ˜¾è‘—æå‡æ•°æ®é›†**: {len(improved)}ä¸ªï¼ˆå·²åˆ†æï¼‰  
**æ˜¾è‘—ä¸‹é™æ•°æ®é›†**: {len(degraded)}ä¸ªï¼ˆå·²åˆ†æï¼‰

---

## ä¸€ã€æ˜¾è‘—æå‡æ•°æ®é›†é‡åŒ–è¯æ®

"""
    
    if len(improved) > 0:
        report += f"""
### ç»Ÿè®¡æ€»ç»“

| æŒ‡æ ‡ | å¹³å‡å€¼ | ä¸­ä½æ•° | æœ€å°å€¼ | æœ€å¤§å€¼ |
|------|-------|--------|--------|--------|
| **Giniç³»æ•°** | {improved['gini_coefficient'].mean():.3f} | {improved['gini_coefficient'].median():.3f} | {improved['gini_coefficient'].min():.3f} | {improved['gini_coefficient'].max():.3f} |
| **å˜å¼‚ç³»æ•°(CV)** | {improved['cv_relation_freq'].mean():.3f} | {improved['cv_relation_freq'].median():.3f} | {improved['cv_relation_freq'].min():.3f} | {improved['cv_relation_freq'].max():.3f} |
| **Top-10%æ¯”ä¾‹** | {improved['top_10_percent_ratio'].mean():.3f} | {improved['top_10_percent_ratio'].median():.3f} | {improved['top_10_percent_ratio'].min():.3f} | {improved['top_10_percent_ratio'].max():.3f} |
| **å…³ç³»-å®ä½“æ¯”** | {improved['relation_entity_ratio'].mean():.4f} | {improved['relation_entity_ratio'].median():.4f} | {improved['relation_entity_ratio'].min():.4f} | {improved['relation_entity_ratio'].max():.4f} |

### ç»“æ„åŒ–ç¨‹åº¦åˆ†å¸ƒ

- **High Structure**: {len(improved[improved['structure_level'] == 'high'])}/{len(improved)} ({len(improved[improved['structure_level'] == 'high'])/len(improved)*100:.1f}%)
- **Medium Structure**: {len(improved[improved['structure_level'] == 'medium'])}/{len(improved)} ({len(improved[improved['structure_level'] == 'medium'])/len(improved)*100:.1f}%)
- **Low Structure**: {len(improved[improved['structure_level'] == 'low'])}/{len(improved)} ({len(improved[improved['structure_level'] == 'low'])/len(improved)*100:.1f}%)

### è¯¦ç»†åˆ†æ

"""
        
        for _, row in improved.iterrows():
            report += f"""
#### {row['matched_name']} (MRR +{row['mrr_diff']:.3f})

**é‡åŒ–è¯æ®**:
- **Giniç³»æ•°**: **{row['gini_coefficient']:.3f}** ({"é«˜" if row['gini_coefficient'] > 0.7 else "ä¸­" if row['gini_coefficient'] > 0.5 else "ä½"})
- **å˜å¼‚ç³»æ•°(CV)**: **{row['cv_relation_freq']:.3f}** ({"é«˜" if row['cv_relation_freq'] > 1.0 else "ä¸­" if row['cv_relation_freq'] > 0.5 else "ä½"})
- **Top-10%æ¯”ä¾‹**: **{row['top_10_percent_ratio']:.3f}** ({"é«˜" if row['top_10_percent_ratio'] > 0.6 else "ä¸­" if row['top_10_percent_ratio'] > 0.4 else "ä½"})
- **å…³ç³»-å®ä½“æ¯”**: **{row['relation_entity_ratio']:.4f}** ({"ä½" if row['relation_entity_ratio'] < 0.01 else "ä¸­" if row['relation_entity_ratio'] < 0.05 else "é«˜"})
- **ç»“æ„ç­‰çº§**: **{row['structure_level'].upper()}**
- **å®ä½“æ•°**: {int(row['num_entities']):,}
- **å…³ç³»æ•°**: {int(row['num_relations']):,}
- **ä¸‰å…ƒç»„æ•°**: {int(row['num_triples']):,}

**è§£é‡Š**: {row['reasoning']}

**æå‡åŸå› **: è¯¥æ•°æ®é›†çš„å…³ç³»é¢‘ç‡åˆ†å¸ƒé«˜åº¦é›†ä¸­ï¼ˆGini={row['gini_coefficient']:.3f}ï¼‰ï¼Œå°‘æ•°å…³ç³»å ä¸»å¯¼åœ°ä½ï¼Œè¿™ä½¿å¾—AREçš„ç›¸ä¼¼åº¦å¢å¼ºæœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆæ‰¾åˆ°ç›¸ä¼¼å…³ç³»ã€‚å…³ç³»ç±»å‹é›†ä¸­ï¼ˆå…³ç³»-å®ä½“æ¯”={row['relation_entity_ratio']:.4f}ï¼‰ï¼Œè¯­ä¹‰èšç±»è´¨é‡é«˜ï¼Œç›¸ä¼¼åº¦è®¡ç®—å‡†ç¡®ã€‚

---
"""
    else:
        report += "\næš‚æ— æ˜¾è‘—æå‡çš„æ•°æ®é›†æ•°æ®ã€‚\n"
    
    report += f"""
## äºŒã€æ˜¾è‘—ä¸‹é™æ•°æ®é›†é‡åŒ–è¯æ®

"""
    
    if len(degraded) > 0:
        report += f"""
### ç»Ÿè®¡æ€»ç»“

| æŒ‡æ ‡ | å¹³å‡å€¼ | ä¸­ä½æ•° | æœ€å°å€¼ | æœ€å¤§å€¼ |
|------|-------|--------|--------|--------|
| **Giniç³»æ•°** | {degraded['gini_coefficient'].mean():.3f} | {degraded['gini_coefficient'].median():.3f} | {degraded['gini_coefficient'].min():.3f} | {degraded['gini_coefficient'].max():.3f} |
| **å˜å¼‚ç³»æ•°(CV)** | {degraded['cv_relation_freq'].mean():.3f} | {degraded['cv_relation_freq'].median():.3f} | {degraded['cv_relation_freq'].min():.3f} | {degraded['cv_relation_freq'].max():.3f} |
| **Top-10%æ¯”ä¾‹** | {degraded['top_10_percent_ratio'].mean():.3f} | {degraded['top_10_percent_ratio'].median():.3f} | {degraded['top_10_percent_ratio'].min():.3f} | {degraded['top_10_percent_ratio'].max():.3f} |
| **å…³ç³»-å®ä½“æ¯”** | {degraded['relation_entity_ratio'].mean():.4f} | {degraded['relation_entity_ratio'].median():.4f} | {degraded['relation_entity_ratio'].min():.4f} | {degraded['relation_entity_ratio'].max():.4f} |

### ç»“æ„åŒ–ç¨‹åº¦åˆ†å¸ƒ

- **High Structure**: {len(degraded[degraded['structure_level'] == 'high'])}/{len(degraded)} ({len(degraded[degraded['structure_level'] == 'high'])/len(degraded)*100:.1f}%)
- **Medium Structure**: {len(degraded[degraded['structure_level'] == 'medium'])}/{len(degraded)} ({len(degraded[degraded['structure_level'] == 'medium'])/len(degraded)*100:.1f}%)
- **Low Structure**: {len(degraded[degraded['structure_level'] == 'low'])}/{len(degraded)} ({len(degraded[degraded['structure_level'] == 'low'])/len(degraded)*100:.1f}%)

### è¯¦ç»†åˆ†æ

"""
        
        for _, row in degraded.iterrows():
            report += f"""
#### {row['matched_name']} (MRR {row['mrr_diff']:.3f})

**é‡åŒ–è¯æ®**:
- **Giniç³»æ•°**: **{row['gini_coefficient']:.3f}** ({"é«˜" if row['gini_coefficient'] > 0.7 else "ä¸­" if row['gini_coefficient'] > 0.5 else "ä½"})
- **å˜å¼‚ç³»æ•°(CV)**: **{row['cv_relation_freq']:.3f}** ({"é«˜" if row['cv_relation_freq'] > 1.0 else "ä¸­" if row['cv_relation_freq'] > 0.5 else "ä½"})
- **Top-10%æ¯”ä¾‹**: **{row['top_10_percent_ratio']:.3f}** ({"é«˜" if row['top_10_percent_ratio'] > 0.6 else "ä¸­" if row['top_10_percent_ratio'] > 0.4 else "ä½"})
- **å…³ç³»-å®ä½“æ¯”**: **{row['relation_entity_ratio']:.4f}** ({"ä½" if row['relation_entity_ratio'] < 0.01 else "ä¸­" if row['relation_entity_ratio'] < 0.05 else "é«˜"})
- **ç»“æ„ç­‰çº§**: **{row['structure_level'].upper()}**
- **å®ä½“æ•°**: {int(row['num_entities']):,}
- **å…³ç³»æ•°**: {int(row['num_relations']):,}
- **ä¸‰å…ƒç»„æ•°**: {int(row['num_triples']):,}
- **SEMMAåŸºç¡€æ€§èƒ½**: {row['semma_mrr']:.3f}

**è§£é‡Š**: {row['reasoning']}

**ä¸‹é™åŸå› åˆ†æ**:
"""
            
            # æ ¹æ®æŒ‡æ ‡åˆ†æä¸‹é™åŸå› 
            if row['gini_coefficient'] > 0.7:
                if row['semma_mrr'] and row['semma_mrr'] > 0.7:
                    report += f"- è™½ç„¶å…³ç³»é¢‘ç‡åˆ†å¸ƒé«˜åº¦é›†ä¸­ï¼ˆGini={row['gini_coefficient']:.3f}ï¼‰ï¼Œä½†**SEMMAåŸºç¡€æ€§èƒ½å·²ç»å¾ˆé«˜**ï¼ˆMRR {row['semma_mrr']:.3f}ï¼‰ï¼Œé¢å¤–å¢å¼ºå¼•å…¥å¹²æ‰°ã€‚\n"
                elif row['relation_entity_ratio'] > 0.1:
                    report += f"- è™½ç„¶å…³ç³»é¢‘ç‡åˆ†å¸ƒé›†ä¸­ï¼ˆGini={row['gini_coefficient']:.3f}ï¼‰ï¼Œä½†**å…³ç³»ç±»å‹éå¸¸å¤šæ ·**ï¼ˆå…³ç³»-å®ä½“æ¯”={row['relation_entity_ratio']:.4f}ï¼‰ï¼Œè¯­ä¹‰èšç±»è´¨é‡ä½ï¼Œç›¸ä¼¼åº¦è®¡ç®—ä¸å‡†ç¡®ã€‚\n"
                else:
                    report += f"- è™½ç„¶å…³ç³»é¢‘ç‡åˆ†å¸ƒé›†ä¸­ï¼ˆGini={row['gini_coefficient']:.3f}ï¼‰ï¼Œä½†**è¯­ä¹‰èšç±»è´¨é‡ä½**ï¼ˆå¦‚å¸¸è¯†å…³ç³»è¯­ä¹‰è·¨åº¦å¤§ï¼‰ï¼Œå¯¼è‡´ç›¸ä¼¼åº¦è®¡ç®—ä¸å‡†ç¡®ã€‚\n"
            elif row['gini_coefficient'] > 0.5:
                report += f"- å…³ç³»åˆ†å¸ƒä¸­ç­‰ç»“æ„åŒ–ï¼ˆGini={row['gini_coefficient']:.3f}ï¼‰ï¼Œ**å…³ç³»ç±»å‹å¤šæ ·æ€§é«˜**ï¼ˆå…³ç³»-å®ä½“æ¯”={row['relation_entity_ratio']:.4f}ï¼‰ï¼Œå¯¼è‡´AREæœºåˆ¶å¤±æ•ˆã€‚\n"
            else:
                report += f"- å…³ç³»åˆ†å¸ƒä½ç»“æ„åŒ–ï¼ˆGini={row['gini_coefficient']:.3f}ï¼‰ï¼Œå…³ç³»é¢‘ç‡åˆ†å¸ƒå‡åŒ€ï¼Œç›¸ä¼¼åº¦å¢å¼ºæœºåˆ¶éš¾ä»¥æ‰¾åˆ°æœ‰æ•ˆçš„ç›¸ä¼¼å…³ç³»ã€‚\n"
            
            report += "\n---\n"
    else:
        report += "\næš‚æ— æ˜¾è‘—ä¸‹é™çš„æ•°æ®é›†æ•°æ®ã€‚\n"
    
    report += f"""
## ä¸‰ã€å¯¹æ¯”åˆ†æ

### å…³é”®å·®å¼‚

| ç‰¹å¾ | æå‡æ•°æ®é›† | ä¸‹é™æ•°æ®é›† | å·®å¼‚ |
|------|-----------|-----------|------|
| **å¹³å‡Giniç³»æ•°** | {improved['gini_coefficient'].mean():.3f} | {degraded['gini_coefficient'].mean():.3f} | {improved['gini_coefficient'].mean() - degraded['gini_coefficient'].mean():.3f} |
| **å¹³å‡CV** | {improved['cv_relation_freq'].mean():.3f} | {degraded['cv_relation_freq'].mean():.3f} | {improved['cv_relation_freq'].mean() - degraded['cv_relation_freq'].mean():.3f} |
| **å¹³å‡Top-10%** | {improved['top_10_percent_ratio'].mean():.3f} | {degraded['top_10_percent_ratio'].mean():.3f} | {improved['top_10_percent_ratio'].mean() - degraded['top_10_percent_ratio'].mean():.3f} |
| **High Structureå æ¯”** | {len(improved[improved['structure_level'] == 'high'])/len(improved)*100:.1f}% | {len(degraded[degraded['structure_level'] == 'high'])/len(degraded)*100:.1f}% | {len(improved[improved['structure_level'] == 'high'])/len(improved)*100 - len(degraded[degraded['structure_level'] == 'high'])/len(degraded)*100:.1f}% |

### å…³é”®å‘ç°

1. **æå‡æ•°æ®é›†ç‰¹å¾**:
   - å¹³å‡Giniç³»æ•°: **{improved['gini_coefficient'].mean():.3f}** ({"é«˜äº" if improved['gini_coefficient'].mean() > degraded['gini_coefficient'].mean() else "ä½äº"}ä¸‹é™æ•°æ®é›†)
   - {len(improved[improved['structure_level'] == 'high'])/len(improved)*100:.1f}% æ˜¯é«˜åº¦ç»“æ„åŒ–
   - å…³ç³»é¢‘ç‡åˆ†å¸ƒé›†ä¸­ï¼Œå°‘æ•°å…³ç³»å ä¸»å¯¼åœ°ä½

2. **ä¸‹é™æ•°æ®é›†ç‰¹å¾**:
   - å¹³å‡Giniç³»æ•°: **{degraded['gini_coefficient'].mean():.3f}** ({"é«˜äº" if degraded['gini_coefficient'].mean() > improved['gini_coefficient'].mean() else "ä½äº"}æå‡æ•°æ®é›†)
   - è™½ç„¶éƒ¨åˆ†æ•°æ®é›†Giniè¾ƒé«˜ï¼Œä½†**è¯­ä¹‰èšç±»è´¨é‡ä½**æˆ–**åŸºç¡€æ€§èƒ½å·²å¾ˆé«˜**
   - å…³ç³»è¯­ä¹‰è·¨åº¦å¤§ï¼Œç›¸ä¼¼åº¦è®¡ç®—ä¸å‡†ç¡®

3. **å…³é”®æ´å¯Ÿ**:
   - **ä»…å‡­é¢‘ç‡åˆ†å¸ƒï¼ˆGiniç³»æ•°ï¼‰ä¸è¶³ä»¥å®Œå…¨åˆ¤æ–­**ï¼Œè¿˜éœ€è¦è€ƒè™‘è¯­ä¹‰èšç±»è´¨é‡å’ŒåŸºç¡€æ€§èƒ½
   - **é«˜åº¦ç»“æ„åŒ– + é«˜è¯­ä¹‰èšç±»è´¨é‡ + ä¸­ç­‰åŸºç¡€æ€§èƒ½** = AREè¡¨ç°ä¼˜å¼‚
   - **é«˜åº¦ç»“æ„åŒ– + ä½è¯­ä¹‰èšç±»è´¨é‡** = AREè¡¨ç°ä¸‹é™ï¼ˆå¦‚ConceptNetï¼‰
   - **é«˜åº¦ç»“æ„åŒ– + é«˜åŸºç¡€æ€§èƒ½** = AREè¡¨ç°ä¸‹é™ï¼ˆå¦‚NELLInductive:v1ï¼‰

---

## å››ã€è®ºæ–‡è¡¨è¿°å»ºè®®

### æå‡åŸå› 

> "Our comprehensive quantitative analysis of actual dataset files reveals that all significantly improved datasets exhibit **high structural levels** with an average Gini coefficient of **{improved['gini_coefficient'].mean():.3f}** (range: {improved['gini_coefficient'].min():.3f}-{improved['gini_coefficient'].max():.3f}). This indicates concentrated relation frequency distributions where a few dominant relations account for most occurrences. The high structural level, combined with **high semantic clustering quality** and **moderate baseline performance**, enables ARE's similarity-based enhancement mechanism to effectively identify and leverage similar relations."

### ä¸‹é™åŸå› 

> "Conversely, degraded datasets show a more complex pattern: while they also exhibit relatively high Gini coefficients (average **{degraded['gini_coefficient'].mean():.3f}**), they fail due to different issues: (1) **low semantic clustering quality** (e.g., ConceptNet with commonsense relations having wide semantic spans), (2) **already high baseline performance** (e.g., NELLInductive:v1 with SEMMA MRR 0.796, where additional enhancement introduces interference), or (3) **high relation type diversity** (e.g., WDsinger-ht with relation-entity ratio 0.610). This demonstrates that **frequency distribution alone is insufficient**; semantic clustering quality and baseline performance are equally important factors."

---

## äº”ã€æ•°æ®æ¥æº

æ‰€æœ‰é‡åŒ–æŒ‡æ ‡å‡ä»å®é™…æ•°æ®é›†æ–‡ä»¶ï¼ˆtrain.txt, valid.txt, test.txtï¼‰ä¸­æå–ï¼š
- **Giniç³»æ•°**: ä»å…³ç³»é¢‘ç‡åˆ†å¸ƒè®¡ç®—
- **å˜å¼‚ç³»æ•°(CV)**: ä»å…³ç³»é¢‘ç‡çš„æ ‡å‡†å·®å’Œå‡å€¼è®¡ç®—
- **Top-10%æ¯”ä¾‹**: ä»å‰10%æœ€é¢‘ç¹å…³ç³»çš„é¢‘ç‡è®¡ç®—
- **å…³ç³»-å®ä½“æ¯”**: ä»å…³ç³»æ•°é‡å’Œå®ä½“æ•°é‡è®¡ç®—

**æ•°æ®æ–‡ä»¶ä½ç½®**: `/T20030104/ynj/semma/kg-datasets/`

---

ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“„ Detailed report saved to {report_file}")

if __name__ == "__main__":
    print("=" * 80)
    print("å…¨é¢åˆ†ækg-datasetsç›®å½•ä¸‹çš„æ‰€æœ‰æ•°æ®é›†")
    print("=" * 80)
    
    results_df = analyze_all_datasets()
    
    if len(results_df) > 0:
        create_comprehensive_analysis(results_df)
        print("\nâœ… Analysis completed!")
    else:
        print("\nâš ï¸  No datasets analyzed")

