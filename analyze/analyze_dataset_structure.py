#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†ææ•°æ®é›†çš„å…³ç³»ç»“æ„åŒ–ç¨‹åº¦
ä»å®é™…æ•°æ®æ–‡ä»¶ä¸­æå–ç»Ÿè®¡ç‰¹å¾ï¼Œé‡åŒ–åˆ¤æ–­å…³ç³»æ˜¯å¦é«˜åº¦ç»“æ„åŒ–
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from collections import Counter, defaultdict
from scipy import stats
from scipy.spatial.distance import cosine
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# è®¾ç½®å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

sns.set_style("whitegrid")
sns.set_palette("husl")

def load_flags():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    flags_path = Path(__file__).parent.parent / "flags.yaml"
    with open(flags_path, 'r') as f:
        flags = yaml.safe_load(f)
    return flags

def get_dataset_path_mapping():
    """è·å–æ•°æ®é›†åç§°åˆ°è·¯å¾„çš„æ˜ å°„"""
    mapping = {
        'Metafam': 'metafam',
        'YAGO310-ht': 'yago310',
        'YAGO310': 'yago310',
        'ConceptNet 100k-ht': 'cnet100k',
        'ConceptNet100k': 'cnet100k',
        'FB15K237': 'FB15k-237',
        'FB15K-237': 'FB15k-237',
        'WN18RR': 'wn18rr',
        'NELL995-ht': 'nell995',
        'NELL995': 'nell995',
        'CoDExSmall-ht': 'codex-s',
        'CoDExLarge-ht': 'codex-l',
        'CoDExMedium': 'codex-m',
        'DBpedia 100k-ht': 'dbp100k',
        'NELL23k-ht': 'NELL23K',
        'WDsinger-ht': 'WD-singer',
        'WD-singer': 'WD-singer',
        'AristoV4-ht': 'aristov4',
        'Hetionet-ht': 'hetionet',
    }
    return mapping

def find_dataset_raw_dir(kg_datasets_path, dataset_name):
    """æŸ¥æ‰¾æ•°æ®é›†çš„rawç›®å½•"""
    mapping = get_dataset_path_mapping()
    
    # é¦–å…ˆå°è¯•æ˜ å°„
    mapped_name = mapping.get(dataset_name, dataset_name)
    
    # å°è¯•ä¸åŒçš„è·¯å¾„æ ¼å¼
    possible_names = [
        mapped_name,
        dataset_name,
        dataset_name.lower(),
        dataset_name.lower().replace('-', ''),
        dataset_name.lower().replace(' ', ''),
        mapped_name.lower(),
    ]
    
    possible_paths = []
    for name in possible_names:
        # ç›´æ¥è·¯å¾„
        possible_paths.append(os.path.join(kg_datasets_path, name, "raw"))
        # åœ¨grailå­ç›®å½•ä¸­ï¼ˆInductiveæ•°æ®é›†ï¼‰
        possible_paths.append(os.path.join(kg_datasets_path, "grail", f"Ind{name}", "v1", "raw"))
        possible_paths.append(os.path.join(kg_datasets_path, "grail", name, "v1", "raw"))
    
    # é€’å½’æœç´¢
    for root, dirs, files in os.walk(kg_datasets_path):
        if 'raw' in dirs and 'train.txt' in os.listdir(os.path.join(root, 'raw')):
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ•°æ®é›†åç§°
            dir_name = os.path.basename(root)
            if any(name.lower() in dir_name.lower() or dir_name.lower() in name.lower() 
                   for name in possible_names if name):
                possible_paths.append(os.path.join(root, 'raw'))
    
    # å»é‡å¹¶æ£€æŸ¥å­˜åœ¨æ€§
    for path in possible_paths:
        if os.path.exists(path) and os.path.exists(os.path.join(path, "train.txt")):
            return path
    
    return None

def load_dataset_triples(dataset_name, dataset_type="transductive"):
    """
    åŠ è½½æ•°æ®é›†çš„ä¸‰å…ƒç»„æ–‡ä»¶
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        dataset_type: æ•°æ®é›†ç±»å‹ (transductive, inductive, etc.)
    
    Returns:
        train_triples, valid_triples, test_triples: ä¸‰å…ƒç»„åˆ—è¡¨ [(h, r, t), ...]
        entity_vocab, relation_vocab: è¯æ±‡è¡¨
    """
    flags = load_flags()
    kg_datasets_path = flags.get('kg_datasets_path', '/T20030104/ynj/semma/kg-datasets')
    
    raw_dir = find_dataset_raw_dir(kg_datasets_path, dataset_name)
    
    if raw_dir is None:
        print(f"âš ï¸  Warning: Cannot find raw directory for {dataset_name}")
        return None, None, None, None, None
    
    # åŠ è½½ä¸‰å…ƒç»„æ–‡ä»¶
    def load_triples_file(filepath):
        if not os.path.exists(filepath):
            return []
        triples = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                parts = line.split('\t')
                if len(parts) >= 3:
                    h, r, t = parts[0], parts[1], parts[2]
                    triples.append((h, r, t))
        return triples
    
    train_file = os.path.join(raw_dir, "train.txt")
    valid_file = os.path.join(raw_dir, "valid.txt")
    test_file = os.path.join(raw_dir, "test.txt")
    
    train_triples = load_triples_file(train_file)
    valid_triples = load_triples_file(valid_file)
    test_triples = load_triples_file(test_file)
    
    # æ„å»ºè¯æ±‡è¡¨
    all_triples = train_triples + valid_triples + test_triples
    entities = set()
    relations = set()
    
    for h, r, t in all_triples:
        entities.add(h)
        entities.add(t)
        relations.add(r)
    
    entity_vocab = {e: i for i, e in enumerate(sorted(entities))}
    relation_vocab = {r: i for i, r in enumerate(sorted(relations))}
    
    return train_triples, valid_triples, test_triples, entity_vocab, relation_vocab

def calculate_gini_coefficient(values):
    """è®¡ç®—åŸºå°¼ç³»æ•°ï¼ˆè¡¡é‡åˆ†å¸ƒçš„ä¸å‡åŒ€ç¨‹åº¦ï¼‰"""
    if len(values) == 0:
        return 0.0
    values = np.array(values)
    values = values.flatten()
    values = np.sort(values)
    n = len(values)
    index = np.arange(1, n + 1)
    return (2 * np.sum(index * values)) / (n * np.sum(values)) - (n + 1) / n

def calculate_entropy(values):
    """è®¡ç®—ç†µï¼ˆè¡¡é‡åˆ†å¸ƒçš„å‡åŒ€ç¨‹åº¦ï¼‰"""
    if len(values) == 0:
        return 0.0
    values = np.array(values)
    values = values[values > 0]  # åªè€ƒè™‘éé›¶å€¼
    if len(values) == 0:
        return 0.0
    probs = values / np.sum(values)
    return -np.sum(probs * np.log2(probs + 1e-10))

def analyze_relation_structure(dataset_name, triples_list, entity_vocab, relation_vocab):
    """
    åˆ†æå…³ç³»çš„ç»“æ„åŒ–ç¨‹åº¦
    
    Returns:
        metrics: åŒ…å«å„ç§æŒ‡æ ‡çš„å­—å…¸
    """
    all_triples = []
    for triples in triples_list:
        all_triples.extend(triples)
    
    if len(all_triples) == 0:
        return None
    
    # 1. å…³ç³»é¢‘ç‡ç»Ÿè®¡
    relation_counts = Counter([r for _, r, _ in all_triples])
    relation_frequencies = list(relation_counts.values())
    
    # 2. å…³ç³»é¢‘ç‡åˆ†å¸ƒæŒ‡æ ‡
    gini_coefficient = calculate_gini_coefficient(relation_frequencies)
    entropy = calculate_entropy(relation_frequencies)
    
    # 3. å…³ç³»-å®ä½“æ¯”ä¾‹
    num_relations = len(relation_vocab)
    num_entities = len(entity_vocab)
    relation_entity_ratio = num_relations / num_entities if num_entities > 0 else 0
    
    # 4. å…³ç³»çš„å¹³å‡é¢‘ç‡
    avg_relation_freq = np.mean(relation_frequencies) if len(relation_frequencies) > 0 else 0
    std_relation_freq = np.std(relation_frequencies) if len(relation_frequencies) > 0 else 0
    cv_relation_freq = std_relation_freq / avg_relation_freq if avg_relation_freq > 0 else 0  # å˜å¼‚ç³»æ•°
    
    # 5. å…³ç³»çš„é•¿å°¾åˆ†å¸ƒç¨‹åº¦ï¼ˆå‰10%çš„å…³ç³»å æ€»é¢‘ç‡çš„æ¯”ä¾‹ï¼‰
    sorted_freqs = sorted(relation_frequencies, reverse=True)
    top_10_percent = int(max(1, len(sorted_freqs) * 0.1))
    top_10_percent_freq = sum(sorted_freqs[:top_10_percent])
    total_freq = sum(sorted_freqs)
    top_10_percent_ratio = top_10_percent_freq / total_freq if total_freq > 0 else 0
    
    # 6. å…³ç³»çš„å…±ç°æ¨¡å¼ï¼ˆä¸¤ä¸ªå…³ç³»åŒæ—¶å‡ºç°åœ¨åŒä¸€ä¸ªå®ä½“å¯¹ä¸Šçš„é¢‘ç‡ï¼‰
    # ç®€åŒ–ï¼šè®¡ç®—å…³ç³»çš„å¹³å‡é‚»å±…å…³ç³»æ•°
    entity_relations = defaultdict(set)
    for h, r, t in all_triples:
        entity_relations[h].add(r)
        entity_relations[t].add(r)
    
    relation_cooccurrence = defaultdict(set)
    for entity, rels in entity_relations.items():
        for r1 in rels:
            for r2 in rels:
                if r1 != r2:
                    relation_cooccurrence[r1].add(r2)
    
    avg_cooccurrence = np.mean([len(rels) for rels in relation_cooccurrence.values()]) if len(relation_cooccurrence) > 0 else 0
    
    # 7. å›¾çš„å¯†åº¦
    num_edges = len(all_triples)
    max_possible_edges = num_entities * num_entities
    graph_density = num_edges / max_possible_edges if max_possible_edges > 0 else 0
    
    # 8. å…³ç³»çš„å¹³å‡åº¦ï¼ˆæ¯ä¸ªå…³ç³»å¹³å‡è¿æ¥å¤šå°‘å®ä½“å¯¹ï¼‰
    relation_degrees = defaultdict(int)
    for _, r, _ in all_triples:
        relation_degrees[r] += 1
    avg_relation_degree = np.mean(list(relation_degrees.values())) if len(relation_degrees) > 0 else 0
    
    metrics = {
        'dataset_name': dataset_name,
        'num_entities': num_entities,
        'num_relations': num_relations,
        'num_triples': len(all_triples),
        'relation_entity_ratio': relation_entity_ratio,
        'gini_coefficient': gini_coefficient,  # è¶Šé«˜è¶Šä¸å‡åŒ€ï¼ˆå¯èƒ½æ›´ç»“æ„åŒ–ï¼‰
        'entropy': entropy,  # è¶Šé«˜è¶Šå‡åŒ€ï¼ˆå¯èƒ½æ›´ä¸ç»“æ„åŒ–ï¼‰
        'avg_relation_freq': avg_relation_freq,
        'cv_relation_freq': cv_relation_freq,  # å˜å¼‚ç³»æ•°ï¼Œè¶Šé«˜è¶Šä¸å‡åŒ€
        'top_10_percent_ratio': top_10_percent_ratio,  # é•¿å°¾åˆ†å¸ƒç¨‹åº¦
        'avg_cooccurrence': avg_cooccurrence,
        'graph_density': graph_density,
        'avg_relation_degree': avg_relation_degree,
    }
    
    return metrics

def classify_structure_level(metrics):
    """
    æ ¹æ®æŒ‡æ ‡åˆ†ç±»ç»“æ„åŒ–ç¨‹åº¦
    
    Returns:
        structure_level: 'high', 'medium', 'low'
        reasoning: åˆ†ç±»ç†ç”±
    """
    if metrics is None:
        return 'unknown', 'No data available'
    
    # ç»¼åˆå¤šä¸ªæŒ‡æ ‡åˆ¤æ–­
    scores = []
    reasons = []
    
    # 1. Giniç³»æ•°ï¼ˆè¶Šé«˜è¶Šç»“æ„åŒ–ï¼Œå› ä¸ºå…³ç³»åˆ†å¸ƒä¸å‡åŒ€ï¼Œè¯´æ˜æœ‰ä¸»å¯¼å…³ç³»ï¼‰
    gini = metrics['gini_coefficient']
    if gini > 0.7:
        scores.append(2)  # high
        reasons.append(f"High Gini coefficient ({gini:.3f}) indicates concentrated relation distribution")
    elif gini > 0.5:
        scores.append(1)  # medium
        reasons.append(f"Medium Gini coefficient ({gini:.3f})")
    else:
        scores.append(0)  # low
        reasons.append(f"Low Gini coefficient ({gini:.3f}) indicates uniform relation distribution")
    
    # 2. å˜å¼‚ç³»æ•°ï¼ˆè¶Šé«˜è¶Šç»“æ„åŒ–ï¼‰
    cv = metrics['cv_relation_freq']
    if cv > 1.0:
        scores.append(2)
        reasons.append(f"High coefficient of variation ({cv:.3f})")
    elif cv > 0.5:
        scores.append(1)
        reasons.append(f"Medium coefficient of variation ({cv:.3f})")
    else:
        scores.append(0)
        reasons.append(f"Low coefficient of variation ({cv:.3f})")
    
    # 3. é•¿å°¾åˆ†å¸ƒï¼ˆè¶Šé«˜è¶Šç»“æ„åŒ–ï¼‰
    top10 = metrics['top_10_percent_ratio']
    if top10 > 0.6:
        scores.append(2)
        reasons.append(f"High top-10% ratio ({top10:.3f}) indicates long-tail distribution")
    elif top10 > 0.4:
        scores.append(1)
        reasons.append(f"Medium top-10% ratio ({top10:.3f})")
    else:
        scores.append(0)
        reasons.append(f"Low top-10% ratio ({top10:.3f})")
    
    # 4. å…³ç³»-å®ä½“æ¯”ä¾‹ï¼ˆè¶Šä½å¯èƒ½è¶Šç»“æ„åŒ–ï¼Œå› ä¸ºå…³ç³»ç±»å‹é›†ä¸­ï¼‰
    ratio = metrics['relation_entity_ratio']
    if ratio < 0.01:
        scores.append(2)
        reasons.append(f"Low relation-entity ratio ({ratio:.4f}) indicates concentrated relation types")
    elif ratio < 0.05:
        scores.append(1)
        reasons.append(f"Medium relation-entity ratio ({ratio:.4f})")
    else:
        scores.append(0)
        reasons.append(f"High relation-entity ratio ({ratio:.4f})")
    
    avg_score = np.mean(scores)
    
    if avg_score >= 1.5:
        level = 'high'
    elif avg_score >= 0.5:
        level = 'medium'
    else:
        level = 'low'
    
    reasoning = "; ".join(reasons)
    return level, reasoning

def analyze_multiple_datasets(dataset_names):
    """åˆ†æå¤šä¸ªæ•°æ®é›†"""
    results = []
    
    for dataset_name in dataset_names:
        print(f"\nğŸ“Š Analyzing {dataset_name}...")
        
        try:
            triples_list = load_dataset_triples(dataset_name)
            if triples_list[0] is None:
                print(f"   âš ï¸  Skipping {dataset_name} (cannot load data)")
                continue
            
            train_triples, valid_triples, test_triples, entity_vocab, relation_vocab = triples_list
            
            metrics = analyze_relation_structure(
                dataset_name, 
                [train_triples, valid_triples, test_triples],
                entity_vocab,
                relation_vocab
            )
            
            if metrics:
                structure_level, reasoning = classify_structure_level(metrics)
                metrics['structure_level'] = structure_level
                metrics['reasoning'] = reasoning
                results.append(metrics)
                print(f"   âœ… Structure level: {structure_level}")
                print(f"   ğŸ“ˆ Gini: {metrics['gini_coefficient']:.3f}, CV: {metrics['cv_relation_freq']:.3f}")
        except Exception as e:
            print(f"   âŒ Error analyzing {dataset_name}: {e}")
            import traceback
            traceback.print_exc()
    
    return results

def create_visualization(results_df):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    output_dir = Path(__file__).parent / "figures"
    output_dir.mkdir(exist_ok=True)
    
    if len(results_df) == 0:
        print("âš ï¸  No data to visualize")
        return
    
    # 1. ç»“æ„åŒ–ç¨‹åº¦åˆ†ç±»åˆ†å¸ƒ
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1.1 Giniç³»æ•° vs å˜å¼‚ç³»æ•°
    ax1 = axes[0, 0]
    for level in ['high', 'medium', 'low']:
        data = results_df[results_df['structure_level'] == level]
        if len(data) > 0:
            ax1.scatter(data['gini_coefficient'], data['cv_relation_freq'], 
                       label=f'{level.title()} Structure', s=100, alpha=0.7)
            for _, row in data.iterrows():
                ax1.annotate(row['dataset_name'], 
                           (row['gini_coefficient'], row['cv_relation_freq']),
                           fontsize=8, alpha=0.7)
    ax1.set_xlabel('Gini Coefficient', fontsize=11, fontweight='bold')
    ax1.set_ylabel('Coefficient of Variation', fontsize=11, fontweight='bold')
    ax1.set_title('Relation Structure: Gini vs CV', fontsize=12, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 1.2 é•¿å°¾åˆ†å¸ƒ vs å…³ç³»-å®ä½“æ¯”ä¾‹
    ax2 = axes[0, 1]
    for level in ['high', 'medium', 'low']:
        data = results_df[results_df['structure_level'] == level]
        if len(data) > 0:
            ax2.scatter(data['top_10_percent_ratio'], data['relation_entity_ratio'], 
                       label=f'{level.title()} Structure', s=100, alpha=0.7)
            for _, row in data.iterrows():
                ax2.annotate(row['dataset_name'], 
                           (row['top_10_percent_ratio'], row['relation_entity_ratio']),
                           fontsize=8, alpha=0.7)
    ax2.set_xlabel('Top-10% Frequency Ratio', fontsize=11, fontweight='bold')
    ax2.set_ylabel('Relation-Entity Ratio', fontsize=11, fontweight='bold')
    ax2.set_title('Relation Structure: Long-tail vs Ratio', fontsize=12, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 1.3 ç»“æ„åŒ–ç¨‹åº¦åˆ†å¸ƒ
    ax3 = axes[1, 0]
    structure_counts = results_df['structure_level'].value_counts()
    colors = {'high': '#2ecc71', 'medium': '#f39c12', 'low': '#e74c3c'}
    bars = ax3.bar(structure_counts.index, structure_counts.values, 
                   color=[colors.get(x, '#95a5a6') for x in structure_counts.index],
                   alpha=0.7, edgecolor='black')
    ax3.set_xlabel('Structure Level', fontsize=11, fontweight='bold')
    ax3.set_ylabel('Number of Datasets', fontsize=11, fontweight='bold')
    ax3.set_title('Distribution of Structure Levels', fontsize=12, fontweight='bold')
    ax3.grid(True, alpha=0.3, axis='y')
    for bar in bars:
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}', ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    # 1.4 å…³é”®æŒ‡æ ‡å¯¹æ¯”
    ax4 = axes[1, 1]
    structure_levels = ['high', 'medium', 'low']
    metrics_to_compare = ['gini_coefficient', 'cv_relation_freq', 'top_10_percent_ratio']
    x = np.arange(len(structure_levels))
    width = 0.25
    
    for i, metric in enumerate(metrics_to_compare):
        values = [results_df[results_df['structure_level'] == level][metric].mean() 
                 for level in structure_levels]
        ax4.bar(x + i*width, values, width, label=metric.replace('_', ' ').title(), alpha=0.7)
    
    ax4.set_xlabel('Structure Level', fontsize=11, fontweight='bold')
    ax4.set_ylabel('Average Metric Value', fontsize=11, fontweight='bold')
    ax4.set_title('Key Metrics by Structure Level', fontsize=12, fontweight='bold')
    ax4.set_xticks(x + width)
    ax4.set_xticklabels(structure_levels)
    ax4.legend()
    ax4.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(output_dir / '25_dataset_structure_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… Visualization saved to {output_dir / '25_dataset_structure_analysis.png'}")

if __name__ == "__main__":
    # ä»ä¹‹å‰çš„åˆ†æä¸­è·å–å…³é”®æ•°æ®é›†åç§°ï¼ˆä½¿ç”¨data.mdä¸­çš„å®é™…åç§°ï¼‰
    key_datasets = [
        'Metafam',
        'YAGO310-ht',
        'ConceptNet 100k-ht',
        'FB15K237',
        'WN18RR',
        'NELL995-ht',
        'CoDExSmall-ht',
        'CoDExLarge-ht',
        'NELL23k-ht',
        'WDsinger-ht',
        'AristoV4-ht',
    ]
    
    print("ğŸ” Analyzing dataset structure levels...")
    print(f"ğŸ“‹ Analyzing {len(key_datasets)} datasets...")
    
    results = analyze_multiple_datasets(key_datasets)
    
    if len(results) > 0:
        results_df = pd.DataFrame(results)
        
        # ä¿å­˜ç»“æœ
        output_file = Path(__file__).parent / "dataset_structure_analysis.csv"
        results_df.to_csv(output_file, index=False)
        print(f"\nğŸ’¾ Results saved to {output_file}")
        
        # æ‰“å°ç»“æœ
        print("\nğŸ“Š Dataset Structure Analysis Results:")
        print("=" * 80)
        for _, row in results_df.iterrows():
            print(f"\n{row['dataset_name']}:")
            print(f"  Structure Level: {row['structure_level'].upper()}")
            print(f"  Gini Coefficient: {row['gini_coefficient']:.3f}")
            print(f"  CV: {row['cv_relation_freq']:.3f}")
            print(f"  Top-10% Ratio: {row['top_10_percent_ratio']:.3f}")
            print(f"  Relation-Entity Ratio: {row['relation_entity_ratio']:.4f}")
            print(f"  Reasoning: {row['reasoning']}")
        
        # åˆ›å»ºå¯è§†åŒ–
        print("\nğŸ“ˆ Creating visualizations...")
        create_visualization(results_df)
        
        print("\nğŸ‰ Analysis completed!")
    else:
        print("\nâš ï¸  No results to save. Please check dataset paths.")

