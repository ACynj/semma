#!/usr/bin/env python
"""
è¡¥å……Trial 2ç¼ºå¤±çš„ä¸¤ä¸ªæ•°æ®é›†è¯„ä¼°ï¼ˆFB15k237å’ŒWN18RRï¼‰
è¯„ä¼°å®Œæˆåæ›´æ–°å®Œæ•´ç»“æœåˆ°æ•°æ®åº“
"""

import os
import sys
import subprocess
import re
import json
import optuna
import sqlite3
from pathlib import Path
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def parse_metrics_from_output(output):
    """ä»è¾“å‡ºä¸­è§£æMRRå’ŒHits@10"""
    metrics = {}
    
    # æŸ¥æ‰¾MRR
    mrr_patterns = [
        r'mrr(?:-tail)?[:\s]+(\d+\.\d+)',
        r'mrr[:\s]+(\d+\.\d+)',
    ]
    
    for pattern in mrr_patterns:
        matches = re.findall(pattern, output, re.IGNORECASE)
        if matches:
            try:
                metrics['mrr'] = float(matches[-1])
                break
            except:
                pass
    
    # æŸ¥æ‰¾Hits@10
    hits10_patterns = [
        r'hits@10(?:-tail)?[:\s]+(\d+\.\d+)',
        r'hits@10[:\s]+(\d+\.\d+)',
    ]
    
    for pattern in hits10_patterns:
        matches = re.findall(pattern, output, re.IGNORECASE)
        if matches:
            try:
                metrics['hits@10'] = float(matches[-1])
                break
            except:
                pass
    
    return metrics if 'mrr' in metrics and 'hits@10' in metrics else None

def evaluate_dataset(checkpoint_path, dataset_name, dataset_type="transductive"):
    """ä½¿ç”¨checkpointåœ¨æ•°æ®é›†ä¸Šè¯„ä¼°"""
    project_root = "/T20030104/ynj/semma"
    
    if dataset_type == "transductive":
        config_path = "config/transductive/inference-fb.yaml"
        cmd = [
            "python", "script/run.py",
            "-c", config_path,
            "--dataset", dataset_name,
            "--ckpt", checkpoint_path,
            "--gpus", "[0]",
            "--epochs", "0",
            "--bpe", "null"
        ]
    else:  # inductive
        config_path = "config/inductive/inference.yaml"
        cmd = [
            "python", "script/run.py",
            "-c", config_path,
            "--dataset", dataset_name,
            "--version", "v1",
            "--ckpt", checkpoint_path,
            "--gpus", "[0]",
            "--epochs", "0",
            "--bpe", "null"
        ]
    
    try:
        print(f"  è¯„ä¼° {dataset_name}...")
        result = subprocess.run(
            cmd,
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode == 0:
            metrics = parse_metrics_from_output(result.stdout + result.stderr)
            if metrics:
                print(f"    âœ“ MRR: {metrics['mrr']:.4f}, Hits@10: {metrics['hits@10']:.4f}")
                return metrics
            else:
                print(f"    âš  æ— æ³•è§£ææŒ‡æ ‡")
                print(f"    è¾“å‡º: {result.stdout[-500:]}")
        else:
            print(f"    âœ— è¯„ä¼°å¤±è´¥")
            print(f"    é”™è¯¯: {result.stderr[-500:]}")
            
    except subprocess.TimeoutExpired:
        print(f"    âœ— è¯„ä¼°è¶…æ—¶")
    except Exception as e:
        print(f"    âœ— è¯„ä¼°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    
    return None

def complete_trial2_evaluation():
    """è¡¥å……Trial 2ç¼ºå¤±çš„æ•°æ®é›†è¯„ä¼°"""
    checkpoint_path = "/T20030104/ynj/semma/output/Ultra/JointDataset/2025-11-07-22-27-49/model_epoch_9.pth"
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ Checkpointä¸å­˜åœ¨: {checkpoint_path}")
        return False
    
    print("="*70)
    print("ğŸ”§ è¡¥å……Trial 2ç¼ºå¤±çš„æ•°æ®é›†è¯„ä¼°")
    print("="*70)
    print(f"Checkpoint: {checkpoint_path}")
    
    # è¯»å–ç°æœ‰çš„Trial 2ç»“æœ
    result_file = "/T20030104/ynj/semma/optuna_tune/trials/trial_2/result.json"
    if os.path.exists(result_file):
        with open(result_file, 'r') as f:
            existing_result = json.load(f)
        existing_eval_results = existing_result.get('eval_results', {})
        print(f"\nğŸ“‹ ç°æœ‰è¯„ä¼°ç»“æœ ({len(existing_eval_results)}ä¸ªæ•°æ®é›†):")
        for dataset, metrics in existing_eval_results.items():
            if isinstance(metrics, dict) and 'mrr' in metrics:
                print(f"  âœ“ {dataset}: MRR={metrics['mrr']:.4f}, Hits@10={metrics['hits@10']:.4f}")
    else:
        existing_eval_results = {}
        print("\nâš  æ²¡æœ‰æ‰¾åˆ°ç°æœ‰çš„è¯„ä¼°ç»“æœ")
    
    # éœ€è¦è¯„ä¼°çš„æ•°æ®é›†
    missing_datasets = [
        ("FB15k237", "transductive"),
        ("WN18RR", "transductive"),
    ]
    
    # æ£€æŸ¥å“ªäº›æ•°æ®é›†ç¼ºå¤±
    datasets_to_evaluate = []
    for dataset_name, dataset_type in missing_datasets:
        if dataset_name not in existing_eval_results or not isinstance(existing_eval_results[dataset_name], dict) or 'mrr' not in existing_eval_results[dataset_name]:
            datasets_to_evaluate.append((dataset_name, dataset_type))
            print(f"  âš  {dataset_name}: ç¼ºå¤±")
        else:
            print(f"  âœ“ {dataset_name}: å·²æœ‰ç»“æœ")
    
    if not datasets_to_evaluate:
        print("\nâœ… æ‰€æœ‰æ•°æ®é›†éƒ½å·²è¯„ä¼°å®Œæˆï¼")
        return True
    
    # è¿è¡Œè¯„ä¼°
    print(f"\nğŸ“Š å¼€å§‹è¯„ä¼°ç¼ºå¤±çš„æ•°æ®é›† ({len(datasets_to_evaluate)}ä¸ª)...")
    new_results = {}
    
    for dataset_name, dataset_type in datasets_to_evaluate:
        metrics = evaluate_dataset(checkpoint_path, dataset_name, dataset_type)
        if metrics:
            new_results[dataset_name] = metrics
    
    if not new_results:
        print("\nâŒ æ²¡æœ‰è·å¾—ä»»ä½•æ–°çš„è¯„ä¼°ç»“æœ")
        return False
    
    # åˆå¹¶ç»“æœ
    all_eval_results = {**existing_eval_results, **new_results}
    
    # ç§»é™¤ç»Ÿè®¡ä¿¡æ¯ï¼Œåªä¿ç•™æ•°æ®é›†ç»“æœ
    dataset_results = {k: v for k, v in all_eval_results.items() 
                      if isinstance(v, dict) and 'mrr' in v and 'hits@10' in v}
    
    if len(dataset_results) < 6:
        print(f"\nâš  è­¦å‘Š: åªæœ‰{len(dataset_results)}ä¸ªæ•°æ®é›†çš„ç»“æœï¼ŒæœŸæœ›6ä¸ª")
        print(f"  æ•°æ®é›†: {list(dataset_results.keys())}")
    
    # è®¡ç®—ç»¼åˆåˆ†æ•°
    total_mrr = sum(m['mrr'] for m in dataset_results.values())
    total_hits10 = sum(m['hits@10'] for m in dataset_results.values())
    count = len(dataset_results)
    
    avg_mrr = total_mrr / count
    avg_hits10 = total_hits10 / count
    score = 0.6 * avg_mrr + 0.4 * avg_hits10
    
    print(f"\nğŸ“ˆ å®Œæ•´è¯„ä¼°ç»“æœ:")
    print(f"  è¯„ä¼°æ•°æ®é›†æ•°: {len(dataset_results)}/6")
    for dataset, metrics in sorted(dataset_results.items()):
        print(f"  {dataset}: MRR={metrics['mrr']:.4f}, Hits@10={metrics['hits@10']:.4f}")
    print(f"  å¹³å‡MRR: {avg_mrr:.4f}")
    print(f"  å¹³å‡Hits@10: {avg_hits10:.4f}")
    print(f"  ç»¼åˆåˆ†æ•°: {score:.4f}")
    
    # æ›´æ–°Optunaæ•°æ®åº“
    study_db = "/T20030104/ynj/semma/optuna_tune/trials/study.db"
    print(f"\nğŸ’¾ æ›´æ–°Optunaæ•°æ®åº“...")
    
    try:
        study = optuna.load_study(
            study_name="enhancement_params_tuning",
            storage=f"sqlite:///{study_db}"
        )
        
        trial_2 = study.trials[2]
        params = trial_2.params
        
        conn = sqlite3.connect(study_db)
        cursor = conn.cursor()
        
        new_value = -score
        
        cursor.execute("SELECT trial_id FROM trials WHERE number = 2")
        trial_id = cursor.fetchone()[0]
        
        # æ›´æ–°trialå€¼
        cursor.execute("""
            UPDATE trial_values 
            SET value = ?, value_type = 'FINITE'
            WHERE trial_id = ? AND objective = 0
        """, (new_value, trial_id))
        
        if cursor.rowcount == 0:
            cursor.execute("""
                INSERT INTO trial_values (trial_id, objective, value, value_type)
                VALUES (?, 0, ?, 'FINITE')
            """, (trial_id, new_value))
        
        conn.commit()
        conn.close()
        
        print(f"  âœ“ æˆåŠŸæ›´æ–°trial 2:")
        print(f"    å€¼: {new_value:.4f} (å¯¹åº”åˆ†æ•°: {score:.4f})")
        print(f"    å‚æ•°: {params}")
        
    except Exception as e:
        print(f"âŒ æ›´æ–°æ•°æ®åº“å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # ä¿å­˜å®Œæ•´ç»“æœæ–‡ä»¶
    print(f"\nğŸ“ ä¿å­˜å®Œæ•´trialç»“æœæ–‡ä»¶...")
    trial_dir = "/T20030104/ynj/semma/optuna_tune/trials/trial_2"
    os.makedirs(trial_dir, exist_ok=True)
    
    result = {
        'trial_number': 2,
        'score': score,
        'params': params,
        'eval_results': {
            **dataset_results,
            'avg_mrr': avg_mrr,
            'avg_hits10': avg_hits10,
            'score': score
        },
        'timestamp': datetime.now().isoformat(),
        'manually_added': True,
        'note': 'ä½¿ç”¨epoch 9 checkpointè¯„ä¼°ï¼ˆepoch 10æœªå®Œæˆï¼‰ï¼Œå·²è¡¥å……å®Œæ•´6ä¸ªæ•°æ®é›†'
    }
    
    result_file = os.path.join(trial_dir, 'result.json')
    with open(result_file, 'w') as f:
        json.dump(result, f, indent=2)
    
    print(f"  âœ“ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    # éªŒè¯æ›´æ–°
    print(f"\nğŸ” éªŒè¯æ›´æ–°...")
    study = optuna.load_study(
        study_name="enhancement_params_tuning",
        storage=f"sqlite:///{study_db}"
    )
    
    if len(study.trials) > 2:
        trial_2_updated = study.trials[2]
        print(f"  Trial 2æ–°çŠ¶æ€:")
        print(f"    çŠ¶æ€: {trial_2_updated.state}")
        print(f"    å€¼: {trial_2_updated.value:.4f} (å¯¹åº”åˆ†æ•°: {-trial_2_updated.value:.4f})")
        print(f"    å‚æ•°: {trial_2_updated.params}")
    
    if study.best_trial:
        print(f"\nğŸ† å½“å‰æœ€ä½³trial:")
        print(f"  Trial ID: {study.best_trial.number}")
        print(f"  æœ€ä½³å€¼: {-study.best_trial.value:.4f}")
        print(f"  å‚æ•°: {study.best_trial.params}")
    
    print("\n" + "="*70)
    print("âœ… Trial 2å®Œæ•´è¯„ä¼°å®Œæˆï¼")
    print("="*70)
    
    return True

if __name__ == "__main__":
    success = complete_trial2_evaluation()
    sys.exit(0 if success else 1)

