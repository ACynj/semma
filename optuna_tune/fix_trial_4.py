#!/usr/bin/env python
"""
æ‰‹åŠ¨ä¿®å¤ç¬¬äº”æ¬¡å®éªŒï¼ˆTrial 4ï¼‰çš„ç»“æœ
ä»è¯„ä¼°æ—¥å¿—ä¸­æå–æŒ‡æ ‡ï¼Œè®¡ç®—åˆ†æ•°ï¼Œå¹¶æ›´æ–°Optunaæ•°æ®åº“
ä¸ä¼šä¸­æ–­å½“å‰æ­£åœ¨è¿è¡Œçš„ç¨‹åº
"""

import os
import sys
import re
import json
import optuna
import sqlite3
from pathlib import Path
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def parse_metrics_from_log(log_file):
    """ä»æ—¥å¿—æ–‡ä»¶ä¸­è§£æMRRå’ŒHits@10"""
    if not os.path.exists(log_file):
        return None
    
    with open(log_file, 'r') as f:
        content = f.read()
    
    # æŸ¥æ‰¾testé›†çš„MRRå’ŒHits@10ï¼ˆé€šå¸¸æ˜¯æœ€åä¸€ä¸ªï¼‰
    mrr_pattern = r'mrr(?:-tail)?[:\s]+(\d+\.\d+)'
    hits10_pattern = r'hits@10(?:-tail)?[:\s]+(\d+\.\d+)'
    
    mrr_matches = re.findall(mrr_pattern, content, re.IGNORECASE)
    hits10_matches = re.findall(hits10_pattern, content, re.IGNORECASE)
    
    if mrr_matches and hits10_matches:
        # å–æœ€åä¸€ä¸ªåŒ¹é…ï¼ˆé€šå¸¸æ˜¯testé›†çš„ç»“æœï¼‰
        mrr = float(mrr_matches[-1])
        hits10 = float(hits10_matches[-1])
        return {'mrr': mrr, 'hits@10': hits10}
    
    return None

def find_evaluation_logs_for_trial4():
    """æŸ¥æ‰¾Trial 4çš„è¯„ä¼°æ—¥å¿—æ–‡ä»¶ï¼ˆç¬¬äº”æ¬¡å®éªŒï¼‰"""
    eval_dir = "/T20030104/ynj/semma/v3_vip_output/Ultra"
    if not os.path.exists(eval_dir):
        return {}
    
    # Trial 4çš„é¢„è®­ç»ƒå®Œæˆæ—¶é—´ï¼ˆç”¨äºç¡®è®¤è¯„ä¼°æ—¥å¿—ï¼‰
    pretrain_checkpoint = "/T20030104/ynj/semma/output/Ultra/JointDataset/2025-11-08-17-48-54/model_epoch_10.pth"
    if os.path.exists(pretrain_checkpoint):
        pretrain_end_time = os.path.getmtime(pretrain_checkpoint)
    else:
        pretrain_end_time = 0
    
    # ä»£è¡¨æ€§æ•°æ®é›†åˆ—è¡¨ï¼ˆä¸è°ƒå‚è„šæœ¬ä¸­ä¸€è‡´ï¼‰
    representative_datasets = [
        ("FB15k237", None, "transductive"),
        ("WN18RR", None, "transductive"),
        ("CoDExSmall", None, "transductive"),
        ("FB15k237Inductive", "v1", "inductive"),
        ("WN18RRInductive", "v1", "inductive"),
        ("NELLInductive", "v1", "inductive"),
    ]
    
    results = {}
    
    for dataset_name, version, dataset_type in representative_datasets:
        dataset_dir = os.path.join(eval_dir, dataset_name)
        if not os.path.exists(dataset_dir):
            print(f"  âš  {dataset_name}: ç›®å½•ä¸å­˜åœ¨")
            continue
        
        # æŸ¥æ‰¾åœ¨é¢„è®­ç»ƒå®Œæˆåçš„è¯„ä¼°æ—¥å¿—ï¼ˆTrial 4çš„è¯„ä¼°åº”è¯¥åœ¨03:08ä¹‹åï¼‰
        log_files = sorted(Path(dataset_dir).rglob("log.txt"), key=lambda p: p.stat().st_mtime, reverse=True)
        if not log_files:
            print(f"  âš  {dataset_name}: æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
            continue
        
        # æŸ¥æ‰¾åœ¨é¢„è®­ç»ƒå®Œæˆæ—¶é—´ä¹‹åçš„æ—¥å¿—ï¼ˆTrial 4çš„è¯„ä¼°ï¼‰
        latest_log = None
        for log_file in log_files:
            log_time = os.path.getmtime(log_file)
            # åœ¨é¢„è®­ç»ƒå®Œæˆå2å°æ—¶å†…
            if log_time > pretrain_end_time - 3600 and log_time < pretrain_end_time + 7200:
                latest_log = log_file
                break
        
        if latest_log is None:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨æœ€æ–°çš„ï¼ˆå¯èƒ½æ˜¯Trial 4çš„ï¼‰
            latest_log = log_files[0]
        
        metrics = parse_metrics_from_log(str(latest_log))
        if metrics:
            results[dataset_name] = metrics
            log_time_str = datetime.fromtimestamp(os.path.getmtime(latest_log)).strftime('%Y-%m-%d %H:%M:%S')
            print(f"  âœ“ {dataset_name}: MRR={metrics['mrr']:.4f}, Hits@10={metrics['hits@10']:.4f} (æ—¥å¿—æ—¶é—´: {log_time_str})")
        else:
            print(f"  âœ— {dataset_name}: æ— æ³•è§£ææŒ‡æ ‡")
    
    return results

def calculate_score(eval_results):
    """è®¡ç®—ç»¼åˆåˆ†æ•°"""
    if not eval_results:
        return 0.0, 0.0, 0.0
    
    total_mrr = 0.0
    total_hits10 = 0.0
    count = 0
    
    for dataset_name, metrics in eval_results.items():
        if isinstance(metrics, dict) and 'mrr' in metrics and 'hits@10' in metrics:
            total_mrr += metrics['mrr']
            total_hits10 += metrics['hits@10']
            count += 1
    
    if count == 0:
        return 0.0, 0.0, 0.0
    
    avg_mrr = total_mrr / count
    avg_hits10 = total_hits10 / count
    
    # åŠ æƒå¹³å‡ï¼šMRRæƒé‡0.6ï¼ŒHits@10æƒé‡0.4
    score = 0.6 * avg_mrr + 0.4 * avg_hits10
    
    return score, avg_mrr, avg_hits10

def fix_trial_4(result_json_path=None):
    """ä¿®å¤Trial 4çš„ç»“æœï¼ˆç¬¬äº”æ¬¡å®éªŒï¼‰
    
    Args:
        result_json_path: å¯é€‰ï¼ŒåŒ…å«trial 4ç»“æœçš„JSONæ–‡ä»¶è·¯å¾„
                          æ ¼å¼: {"params": {...}, "eval_results": {...}}
    """
    study_db = "/T20030104/ynj/semma/optuna_tune/trials/study.db"
    
    if not os.path.exists(study_db):
        print(f"âŒ Optunaæ•°æ®åº“ä¸å­˜åœ¨: {study_db}")
        return False
    
    print("="*70)
    print("ğŸ”§ ä¿®å¤Trial 4çš„ç»“æœï¼ˆç¬¬äº”æ¬¡å®éªŒï¼‰")
    print("="*70)
    
    # 1. åŠ è½½study
    try:
        study = optuna.load_study(
            study_name="enhancement_params_tuning",
            storage=f"sqlite:///{study_db}"
        )
    except Exception as e:
        print(f"âŒ åŠ è½½studyå¤±è´¥: {e}")
        return False
    
    # 2. æ£€æŸ¥trial 4çš„çŠ¶æ€
    if len(study.trials) < 5:
        print("âŒ Trial 4è¿˜ä¸å­˜åœ¨")
        return False
    
    trial_4 = study.trials[4]
    print(f"\nğŸ“‹ Trial 4å½“å‰çŠ¶æ€:")
    print(f"  çŠ¶æ€: {trial_4.state}")
    print(f"  å€¼: {trial_4.value}")
    print(f"  å‚æ•°: {trial_4.params}")
    
    # ç¡®è®¤è¿™æ˜¯ç¬¬äº”æ¬¡å®éªŒ
    print(f"\nğŸ” ç¡®è®¤è¿™æ˜¯ç¬¬äº”æ¬¡å®éªŒ:")
    print(f"  å‚æ•°: similarity_threshold_init={trial_4.params['similarity_threshold_init']}, "
          f"enhancement_strength_init={trial_4.params['enhancement_strength_init']}")
    
    # æ£€æŸ¥é¢„è®­ç»ƒç›®å½•
    pretrain_dir = "/T20030104/ynj/semma/output/Ultra/JointDataset/2025-11-08-17-48-54"
    if os.path.exists(pretrain_dir):
        print(f"  âœ“ æ‰¾åˆ°é¢„è®­ç»ƒç›®å½•: {pretrain_dir}")
        checkpoint = os.path.join(pretrain_dir, "model_epoch_10.pth")
        if os.path.exists(checkpoint):
            print(f"  âœ“ æ‰¾åˆ°epoch 10 checkpointï¼ˆé¢„è®­ç»ƒå®Œæˆï¼‰")
        else:
            print(f"  âš  æœªæ‰¾åˆ°epoch 10 checkpoint")
    else:
        print(f"  âš  æœªæ‰¾åˆ°é¢„è®­ç»ƒç›®å½•")
    
    if trial_4.state == optuna.trial.TrialState.COMPLETE and trial_4.value != float('inf'):
        print("\nâš  Trial 4å·²ç»å®Œæˆä¸”æœ‰å€¼ï¼Œå°†è¦†ç›–ç°æœ‰ç»“æœ")
    
    # 3. è·å–å‚æ•°å’Œè¯„ä¼°ç»“æœ
    params = None
    eval_results = None
    
    # å¦‚æœæä¾›äº†JSONæ–‡ä»¶ï¼Œä»ä¸­è¯»å–
    if result_json_path and os.path.exists(result_json_path):
        print(f"\nğŸ“‚ ä»JSONæ–‡ä»¶è¯»å–ç»“æœ: {result_json_path}")
        try:
            with open(result_json_path, 'r') as f:
                data = json.load(f)
            params = data.get('params')
            eval_results = data.get('eval_results')
            print(f"  âœ“ æˆåŠŸè¯»å–å‚æ•°å’Œè¯„ä¼°ç»“æœ")
        except Exception as e:
            print(f"  âœ— è¯»å–JSONæ–‡ä»¶å¤±è´¥: {e}")
            result_json_path = None
    
    # å¦‚æœå‚æ•°æœªçŸ¥ï¼Œä»ç°æœ‰trialè·å–
    if params is None:
        params = trial_4.params
        print(f"\nğŸ“ ä»æ•°æ®åº“è¯»å–çš„å‚æ•°: {params}")
    
    # 4. ä»è¯„ä¼°æ—¥å¿—ä¸­æå–ç»“æœï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    if eval_results is None:
        print(f"\nğŸ“Š å°è¯•ä»è¯„ä¼°æ—¥å¿—ä¸­æå–æŒ‡æ ‡ï¼ˆTrial 4çš„è¯„ä¼°ï¼‰...")
        eval_results = find_evaluation_logs_for_trial4()
    
    if not eval_results:
        print("\nâš  æ— æ³•ä»æ—¥å¿—ä¸­è‡ªåŠ¨æå–ç»“æœ")
        print("  è¯·æ‰‹åŠ¨è¾“å…¥è¯„ä¼°ç»“æœ:")
        
        representative_datasets = [
            "FB15k237", "WN18RR", "CoDExSmall",
            "FB15k237Inductive", "WN18RRInductive", "NELLInductive"
        ]
        
        eval_results = {}
        for dataset_name in representative_datasets:
            print(f"\n  {dataset_name}:")
            mrr = input("    MRR: ").strip()
            hits10 = input("    Hits@10: ").strip()
            
            try:
                eval_results[dataset_name] = {
                    'mrr': float(mrr),
                    'hits@10': float(hits10)
                }
            except ValueError:
                print(f"    âš  è·³è¿‡ {dataset_name}ï¼ˆæ ¼å¼é”™è¯¯ï¼‰")
    
    if not eval_results:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœ")
        return False
    
    # 5. è®¡ç®—ç»¼åˆåˆ†æ•°
    score, avg_mrr, avg_hits10 = calculate_score(eval_results)
    print(f"\nğŸ“ˆ è®¡ç®—ç»“æœ:")
    print(f"  è¯„ä¼°æ•°æ®é›†æ•°: {len(eval_results)}")
    print(f"  å¹³å‡MRR: {avg_mrr:.4f}")
    print(f"  å¹³å‡Hits@10: {avg_hits10:.4f}")
    print(f"  ç»¼åˆåˆ†æ•°: {score:.4f}")
    
    # 6. æ›´æ–°Optunaæ•°æ®åº“
    print(f"\nğŸ’¾ æ›´æ–°Optunaæ•°æ®åº“...")
    try:
        conn = sqlite3.connect(study_db)
        cursor = conn.cursor()
        
        # Optunaä½¿ç”¨è´Ÿå€¼å› ä¸ºæˆ‘ä»¬æœ€å°åŒ–ï¼ˆä½†å®é™…æ˜¯æœ€å¤§åŒ–scoreï¼‰
        new_value = -score
        
        # è·å–trial_id
        cursor.execute("SELECT trial_id FROM trials WHERE number = 4")
        trial_id_result = cursor.fetchone()
        if not trial_id_result:
            print("âŒ æ‰¾ä¸åˆ°trial 4çš„ID")
            conn.close()
            return False
        
        trial_id = trial_id_result[0]
        
        # æ›´æ–°trialå€¼
        cursor.execute("""
            UPDATE trial_values 
            SET value = ?, value_type = 'FINITE'
            WHERE trial_id = ? AND objective = 0
        """, (new_value, trial_id))
        
        # å¦‚æœtrial_valuesè¡¨ä¸­æ²¡æœ‰è®°å½•ï¼Œæ’å…¥ä¸€æ¡
        if cursor.rowcount == 0:
            cursor.execute("""
                INSERT INTO trial_values (trial_id, objective, value, value_type)
                VALUES (?, 0, ?, 'FINITE')
            """, (trial_id, new_value))
        
        # æ›´æ–°trialçŠ¶æ€ä¸ºCOMPLETE
        cursor.execute("""
            UPDATE trials 
            SET state = 'COMPLETE' 
            WHERE trial_id = ?
        """, (trial_id,))
        
        # æ›´æ–°å‚æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
        cursor.execute("""
            UPDATE trial_params 
            SET param_value = ?
            WHERE trial_id = ? AND param_name = 'similarity_threshold_init'
        """, (str(params['similarity_threshold_init']), trial_id))
        
        cursor.execute("""
            UPDATE trial_params 
            SET param_value = ?
            WHERE trial_id = ? AND param_name = 'enhancement_strength_init'
        """, (str(params['enhancement_strength_init']), trial_id))
        
        conn.commit()
        conn.close()
        
        print(f"  âœ“ æˆåŠŸæ›´æ–°trial 4:")
        print(f"    å€¼: {new_value:.4f} (å¯¹åº”åˆ†æ•°: {score:.4f})")
        print(f"    çŠ¶æ€: COMPLETE")
        print(f"    å‚æ•°: {params}")
        
    except Exception as e:
        print(f"âŒ æ›´æ–°æ•°æ®åº“å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 7. ä¿å­˜trialç»“æœæ–‡ä»¶
    print(f"\nğŸ“ ä¿å­˜trialç»“æœæ–‡ä»¶...")
    trial_dir = "/T20030104/ynj/semma/optuna_tune/trials/trial_4"
    os.makedirs(trial_dir, exist_ok=True)
    
    result = {
        'trial_number': 4,
        'score': score,
        'params': params,
        'eval_results': {
            **eval_results,
            'avg_mrr': avg_mrr,
            'avg_hits10': avg_hits10,
            'score': score
        },
        'timestamp': datetime.now().isoformat(),
        'manually_added': True,
        'note': 'ç¬¬äº”æ¬¡å®éªŒï¼ˆTrial 4ï¼‰'
    }
    
    result_file = os.path.join(trial_dir, 'result.json')
    with open(result_file, 'w') as f:
        json.dump(result, f, indent=2)
    
    print(f"  âœ“ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    # 8. éªŒè¯æ›´æ–°
    print(f"\nğŸ” éªŒè¯æ›´æ–°...")
    study = optuna.load_study(
        study_name="enhancement_params_tuning",
        storage=f"sqlite:///{study_db}"
    )
    
    if len(study.trials) > 4:
        trial_4_updated = study.trials[4]
        print(f"  Trial 4æ–°çŠ¶æ€:")
        print(f"    çŠ¶æ€: {trial_4_updated.state}")
        print(f"    å€¼: {trial_4_updated.value:.4f} (å¯¹åº”åˆ†æ•°: {-trial_4_updated.value:.4f})")
        print(f"    å‚æ•°: {trial_4_updated.params}")
    
    if study.best_trial:
        print(f"\nğŸ† å½“å‰æœ€ä½³trial:")
        print(f"  Trial ID: {study.best_trial.number}")
        print(f"  æœ€ä½³å€¼: {-study.best_trial.value:.4f}")
        print(f"  å‚æ•°: {study.best_trial.params}")
    
    print("\n" + "="*70)
    print("âœ… Trial 4ä¿®å¤å®Œæˆï¼")
    print("="*70)
    print("\nğŸ’¡ æç¤º:")
    print("  - ç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“ï¼Œä¸ä¼šå½±å“å½“å‰æ­£åœ¨è¿è¡Œçš„ç¨‹åº")
    print("  - åç»­trialsä¼šåŸºäºè¿™ä¸ªç»“æœç»§ç»­ä¼˜åŒ–")
    print("  - å¯ä»¥ä½¿ç”¨ optuna-dashboard æŸ¥çœ‹æ›´æ–°åçš„ç»“æœ")
    
    return True

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¿®å¤Trial 4çš„ç»“æœï¼ˆç¬¬äº”æ¬¡å®éªŒï¼‰')
    parser.add_argument('--result_json', type=str, default=None,
                       help='åŒ…å«trial 4ç»“æœçš„JSONæ–‡ä»¶è·¯å¾„ï¼ˆæ ¼å¼: {"params": {...}, "eval_results": {...}}ï¼‰')
    args = parser.parse_args()
    
    success = fix_trial_4(result_json_path=args.result_json)
    sys.exit(0 if success else 1)

