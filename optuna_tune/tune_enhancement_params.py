#!/usr/bin/env python
"""
EnhancedUltraç›¸ä¼¼åº¦å¢å¼ºå‚æ•°è°ƒä¼˜è„šæœ¬
ä¸“é—¨è°ƒä¼˜ similarity_threshold_init å’Œ enhancement_strength_init ä¸¤ä¸ªå‚æ•°
"""

import optuna
import optuna.visualization as vis
import os
import sys
import subprocess
import json
import yaml
import shutil
from pathlib import Path
import torch
from datetime import datetime
import re
import glob

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from ultra import util

class EnhancementParamsTuner:
    def __init__(self, 
                 pretrain_config="config/transductive/pretrain_semma.yaml",
                 flags_path="flags.yaml",
                 output_dir="./optuna_tune/trials"):
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.pretrain_config = os.path.join(self.project_root, pretrain_config)
        self.flags_path = os.path.join(self.project_root, flags_path)
        self.output_dir = os.path.join(self.project_root, output_dir)
        os.makedirs(self.output_dir, exist_ok=True)
        
        # å¯è§†åŒ–è¾“å‡ºç›®å½•
        self.viz_dir = os.path.join(self.output_dir, "visualizations")
        os.makedirs(self.viz_dir, exist_ok=True)
        
        # å¤‡ä»½åŸå§‹flags.yaml
        self.flags_backup = os.path.join(self.output_dir, "flags_backup.yaml")
        shutil.copy(self.flags_path, self.flags_backup)
        
        # ä»£è¡¨æ€§æ•°æ®é›†åˆ—è¡¨ï¼ˆç”¨äºå¿«é€Ÿè¯„ä¼°ï¼‰
        # åŒ…å«è½¬å¯¼å’Œå½’çº³æ•°æ®é›†ï¼Œè¦†ç›–ä¸åŒç±»å‹
        self.representative_datasets = [
            # è½¬å¯¼æ•°æ®é›†ï¼ˆ3ä¸ªï¼‰
            ("FB15k237", None, "transductive"),
            ("WN18RR", None, "transductive"),
            ("CoDExSmall", None, "transductive"),
            # å½’çº³æ•°æ®é›†ï¼ˆ3ä¸ªï¼‰
            ("FB15k237Inductive", "v1", "inductive"),
            ("WN18RRInductive", "v1", "inductive"),
            ("NELLInductive", "v1", "inductive"),
        ]
        
        self.current_trial = 0
        self.total_trials = 0
        self.start_time = None
        
    def objective(self, trial):
        """Optunaç›®æ ‡å‡½æ•°"""
        self.current_trial = trial.number + 1
        
        # é‡‡æ ·ä¸¤ä¸ªå‚æ•°
        similarity_threshold = trial.suggest_float(
            'similarity_threshold_init', 
            0.5,  # æœ€å°å€¼
            0.95,  # æœ€å¤§å€¼
            step=0.05  # æ­¥é•¿
        )
        
        enhancement_strength = trial.suggest_float(
            'enhancement_strength_init', 
            0.01,  # æœ€å°å€¼
            0.15,  # æœ€å¤§å€¼ï¼ˆä¿æŒè¾ƒå°ï¼Œå› ä¸ºæœ€ç»ˆä¼šæ˜ å°„åˆ°0-0.2ï¼‰
            step=0.01  # æ­¥é•¿
        )
        
        params = {
            'similarity_threshold_init': similarity_threshold,
            'enhancement_strength_init': enhancement_strength
        }
        
        # æ‰“å°è¿›åº¦
        elapsed = (datetime.now() - self.start_time).total_seconds() / 3600 if self.start_time else 0
        eta = (elapsed / max(1, self.current_trial - 1)) * (self.total_trials - self.current_trial) if self.current_trial > 1 else 0
        
        print(f"\n{'='*70}")
        print(f"Trial {trial.number+1}/{self.total_trials}")
        print(f"å·²ç”¨æ—¶é—´: {elapsed:.1f}å°æ—¶ | é¢„è®¡å‰©ä½™: {eta:.1f}å°æ—¶")
        print(f"å‚æ•°: similarity_threshold_init={similarity_threshold:.3f}, enhancement_strength_init={enhancement_strength:.3f}")
        print(f"{'='*70}")
        
        try:
            # 1. æ›´æ–°flags.yaml
            self._update_flags(params)
            
            # 2. è¿è¡Œé¢„è®­ç»ƒï¼ˆ10å°æ—¶ï¼‰
            print(f"\n[æ­¥éª¤1/3] å¼€å§‹é¢„è®­ç»ƒ...")
            checkpoint_path = self._run_pretrain()
            if checkpoint_path is None:
                print(f"[Trial {trial.number}] âœ— é¢„è®­ç»ƒå¤±è´¥")
                return float('inf')
            
            print(f"âœ“ é¢„è®­ç»ƒå®Œæˆï¼Œcheckpoint: {checkpoint_path}")
            
            # 3. å¿«é€Ÿè¯„ä¼°ï¼ˆä»£è¡¨æ€§æ•°æ®é›†ï¼Œçº¦30-40åˆ†é’Ÿï¼‰
            print(f"\n[æ­¥éª¤2/3] åœ¨ä»£è¡¨æ€§æ•°æ®é›†ä¸Šå¿«é€Ÿè¯„ä¼°...")
            eval_results = self._fast_evaluate(checkpoint_path)
            
            if not eval_results:
                print(f"[Trial {trial.number}] âœ— è¯„ä¼°å¤±è´¥: æ²¡æœ‰è·å¾—ä»»ä½•è¯„ä¼°ç»“æœ")
                print(f"  æ£€æŸ¥checkpointè·¯å¾„: {checkpoint_path}")
                print(f"  æ£€æŸ¥è¯„ä¼°æ—¥å¿—ä»¥è·å–æ›´å¤šä¿¡æ¯")
                return float('inf')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æŒ‡æ ‡
            valid_results = {k: v for k, v in eval_results.items() if isinstance(v, dict) and 'mrr' in v and 'hits@10' in v}
            if not valid_results:
                print(f"[Trial {trial.number}] âœ— è¯„ä¼°å¤±è´¥: æ²¡æœ‰æœ‰æ•ˆçš„æŒ‡æ ‡ç»“æœ")
                print(f"  è¯„ä¼°ç»“æœ: {eval_results}")
                return float('inf')
            
            # 4. è®¡ç®—ç»¼åˆåˆ†æ•°
            score = self._calculate_score(eval_results)
            
            print(f"\n[æ­¥éª¤3/3] è¯„ä¼°å®Œæˆ")
            print(f"âœ“ ç»¼åˆåˆ†æ•°: {score:.4f}")
            print(f"  å¹³å‡MRR: {eval_results.get('avg_mrr', 0):.4f}")
            print(f"  å¹³å‡Hits@10: {eval_results.get('avg_hits10', 0):.4f}")
            
            if trial.number > 0 and hasattr(trial.study, 'best_value'):
                best_so_far = -trial.study.best_value
                print(f"  å½“å‰æœ€ä½³: {best_so_far:.4f} (Trial {trial.study.best_trial.number})")
            
            # 5. ä¿å­˜ç»“æœ
            self._save_trial(trial.number, params, score, eval_results)
            
            # 6. æ¯5ä¸ªtrialç”Ÿæˆä¸€æ¬¡å¯è§†åŒ–
            if (trial.number + 1) % 5 == 0:
                self._generate_realtime_plots(trial.study)
            
            # è¿”å›è´Ÿåˆ†æ•°ï¼ˆOptunaæœ€å°åŒ–ï¼Œæˆ‘ä»¬æœ€å¤§åŒ–scoreï¼‰
            return -score
            
        except Exception as e:
            print(f"[Trial {trial.number}] âœ— é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return float('inf')
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup()
    
    def _update_flags(self, params):
        """æ›´æ–°flags.yamlä¸­çš„å‚æ•°"""
        with open(self.flags_path, 'r') as f:
            lines = f.readlines()
        
        updated = False
        for i, line in enumerate(lines):
            if line.startswith('similarity_threshold_init:'):
                lines[i] = f"similarity_threshold_init: {params['similarity_threshold_init']}\n"
                updated = True
            elif line.startswith('enhancement_strength_init:'):
                lines[i] = f"enhancement_strength_init: {params['enhancement_strength_init']}\n"
                updated = True
        
        if updated:
            with open(self.flags_path, 'w') as f:
                f.writelines(lines)
    
    def _run_pretrain(self):
        """è¿è¡Œé¢„è®­ç»ƒ"""
        cmd = [
            "python", "script/pretrain.py",
            "-c", self.pretrain_config,
            "--gpus", "[0]",
            "--seed", "42"
        ]
        
        try:
            print(f"  æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=39600  # 11å°æ—¶è¶…æ—¶ï¼ˆ10å°æ—¶é¢„è®­ç»ƒ+ç¼“å†²ï¼‰
            )
            
            if result.returncode == 0:
                checkpoint = self._parse_checkpoint_path(result.stdout, result.stderr)
                return checkpoint
            else:
                print(f"  é”™è¯¯è¾“å‡º: {result.stderr[-500:]}")
                return None
                
        except subprocess.TimeoutExpired:
            print("  âœ— é¢„è®­ç»ƒè¶…æ—¶")
            return None
        except Exception as e:
            print(f"  âœ— é¢„è®­ç»ƒå¼‚å¸¸: {e}")
            return None
    
    def _parse_checkpoint_path(self, stdout, stderr):
        """ä»è¾“å‡ºä¸­è§£æcheckpointè·¯å¾„"""
        # ä»é…ç½®è¯»å–output_dir
        config = util.load_config(self.pretrain_config, context={'gpus': '[0]'})
        
        output_dir = getattr(config, 'output_dir', './output')
        if not os.path.isabs(output_dir):
            output_dir = os.path.join(self.project_root, output_dir)
        
        # æŸ¥æ‰¾æœ€æ–°çš„checkpoint
        patterns = [
            os.path.join(output_dir, "**", "*.pt"),
            os.path.join(output_dir, "**", "*.pth"),
        ]
        
        latest_checkpoint = None
        latest_time = 0
        
        for pattern in patterns:
            for file in glob.glob(pattern, recursive=True):
                if os.path.getmtime(file) > latest_time:
                    latest_time = os.path.getmtime(file)
                    latest_checkpoint = file
        
        if latest_checkpoint:
            return latest_checkpoint
        
        # ä»è¾“å‡ºä¸­è§£æ
        all_output = stdout + stderr
        for line in all_output.split('\n'):
            if 'checkpoint' in line.lower() or 'saved' in line.lower():
                paths = re.findall(r'[\w/.-]+\.(?:pt|pth)', line)
                if paths:
                    return paths[-1]
        
        return None
    
    def _fast_evaluate(self, checkpoint_path):
        """åœ¨ä»£è¡¨æ€§æ•°æ®é›†ä¸Šå¿«é€Ÿè¯„ä¼°"""
        results = {}
        
        for dataset_name, version, dataset_type in self.representative_datasets:
            print(f"\n  è¯„ä¼°æ•°æ®é›†: {dataset_name}" + (f" (v{version})" if version else ""))
            
            if dataset_type == "transductive":
                config_path = "config/transductive/inference-fb.yaml"
                cmd = [
                    "python", "script/run.py",
                    "-c", config_path,
                    "--dataset", dataset_name,
                    "--ckpt", checkpoint_path,
                    "--gpus", "[0]",
                    "--epochs", "0",
                    "--bpe", "null"
                ]
            else:  # inductive
                config_path = "config/inductive/inference.yaml"
                cmd = [
                    "python", "script/run.py",
                    "-c", config_path,
                    "--dataset", dataset_name,
                    "--version", version,
                    "--ckpt", checkpoint_path,
                    "--gpus", "[0]",
                    "--epochs", "0",
                    "--bpe", "null"
                ]
            
            try:
                result = subprocess.run(
                    cmd,
                    cwd=self.project_root,
                    capture_output=True,
                    text=True,
                    timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶ï¼ˆæ¯ä¸ªæ•°æ®é›†ï¼‰
                )
                
                if result.returncode == 0:
                    metrics = self._parse_metrics(result.stdout)
                    if metrics:
                        results[dataset_name] = metrics
                        print(f"    âœ“ MRR: {metrics.get('mrr', 0):.4f}, Hits@10: {metrics.get('hits@10', 0):.4f}")
                    else:
                        print(f"    âš  æ— æ³•è§£ææŒ‡æ ‡")
                else:
                    print(f"    âœ— è¯„ä¼°å¤±è´¥")
                    
            except subprocess.TimeoutExpired:
                print(f"    âœ— è¯„ä¼°è¶…æ—¶")
            except Exception as e:
                print(f"    âœ— è¯„ä¼°å¼‚å¸¸: {e}")
        
        return results
    
    def _parse_metrics(self, output):
        """ä»è¾“å‡ºä¸­è§£æMRRå’ŒHits@10"""
        metrics = {}
        
        # æŸ¥æ‰¾MRRï¼ˆæ”¯æŒ mrr, mrr-tail ç­‰æ ¼å¼ï¼‰
        mrr_patterns = [
            r'mrr(?:-tail)?[:\s]+(\d+\.\d+)',  # åŒ¹é… mrr: æˆ– mrr-tail:
            r'mrr[:\s]+(\d+\.\d+)',  # å¤‡ç”¨æ¨¡å¼
            r'mrr[:\s]+(\d+\.\d+e[+-]?\d+)',  # ç§‘å­¦è®¡æ•°æ³•
        ]
        
        for pattern in mrr_patterns:
            matches = re.findall(pattern, output, re.IGNORECASE)
            if matches:
                try:
                    # å–æœ€åä¸€ä¸ªåŒ¹é…ï¼ˆé€šå¸¸æ˜¯testé›†çš„ç»“æœï¼‰
                    metrics['mrr'] = float(matches[-1])
                    break
                except:
                    pass
        
        # æŸ¥æ‰¾Hits@10ï¼ˆæ”¯æŒ hits@10, hits@10-tail ç­‰æ ¼å¼ï¼‰
        hits10_patterns = [
            r'hits@10(?:-tail)?[:\s]+(\d+\.\d+)',  # åŒ¹é… hits@10: æˆ– hits@10-tail:
            r'hits@10[:\s]+(\d+\.\d+)',  # å¤‡ç”¨æ¨¡å¼
            r'hits@10[:\s]+(\d+\.\d+e[+-]?\d+)',  # ç§‘å­¦è®¡æ•°æ³•
        ]
        
        for pattern in hits10_patterns:
            matches = re.findall(pattern, output, re.IGNORECASE)
            if matches:
                try:
                    # å–æœ€åä¸€ä¸ªåŒ¹é…ï¼ˆé€šå¸¸æ˜¯testé›†çš„ç»“æœï¼‰
                    metrics['hits@10'] = float(matches[-1])
                    break
                except:
                    pass
        
        return metrics
    
    def _calculate_score(self, eval_results):
        """è®¡ç®—ç»¼åˆåˆ†æ•°"""
        if not eval_results:
            return 0.0
        
        total_mrr = 0.0
        total_hits10 = 0.0
        count = 0
        
        for dataset_name, metrics in eval_results.items():
            if 'mrr' in metrics and 'hits@10' in metrics:
                total_mrr += metrics['mrr']
                total_hits10 += metrics['hits@10']
                count += 1
        
        if count == 0:
            return 0.0
        
        avg_mrr = total_mrr / count
        avg_hits10 = total_hits10 / count
        
        # åŠ æƒå¹³å‡ï¼šMRRæƒé‡0.6ï¼ŒHits@10æƒé‡0.4
        score = 0.6 * avg_mrr + 0.4 * avg_hits10
        
        # ä¿å­˜åˆ°eval_resultsä¸­
        eval_results['avg_mrr'] = avg_mrr
        eval_results['avg_hits10'] = avg_hits10
        eval_results['score'] = score
        
        return score
    
    def _save_trial(self, trial_num, params, score, eval_results):
        """ä¿å­˜è¯•éªŒç»“æœ"""
        trial_dir = os.path.join(self.output_dir, f"trial_{trial_num}")
        os.makedirs(trial_dir, exist_ok=True)
        
        result = {
            'trial_number': trial_num,
            'score': score,
            'params': params,
            'eval_results': eval_results,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(os.path.join(trial_dir, 'result.json'), 'w') as f:
            json.dump(result, f, indent=2)
    
    def _generate_realtime_plots(self, study):
        """å®æ—¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        try:
            print(f"\n[å¯è§†åŒ–] ç”Ÿæˆå®æ—¶å›¾è¡¨...")
            
            # 1. ä¼˜åŒ–å†å²
            fig = vis.plot_optimization_history(study)
            fig.write_html(os.path.join(self.viz_dir, "optimization_history.html"))
            
            # 2. å‚æ•°é‡è¦æ€§
            if len(study.trials) > 3:
                try:
                    fig = vis.plot_param_importances(study)
                    fig.write_html(os.path.join(self.viz_dir, "param_importances.html"))
                except:
                    pass
            
            # 3. å‚æ•°å…³ç³»ï¼ˆç­‰é«˜çº¿å›¾ï¼‰
            if len(study.trials) > 5:
                try:
                    fig = vis.plot_contour(
                        study, 
                        params=['similarity_threshold_init', 'enhancement_strength_init']
                    )
                    fig.write_html(os.path.join(self.viz_dir, "contour_plot.html"))
                except:
                    pass
            
            # 4. å¹³è¡Œåæ ‡å›¾
            if len(study.trials) > 5:
                try:
                    fig = vis.plot_parallel_coordinate(study)
                    fig.write_html(os.path.join(self.viz_dir, "parallel_coordinate.html"))
                except:
                    pass
            
            print(f"  âœ“ å›¾è¡¨å·²ä¿å­˜åˆ°: {self.viz_dir}")
            print(f"    æ‰“å¼€ {os.path.join(self.viz_dir, 'optimization_history.html')} æŸ¥çœ‹è¿›åº¦")
            
        except Exception as e:
            print(f"  âš  ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")
    
    def _cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        # æ¢å¤åŸå§‹flags.yaml
        if os.path.exists(self.flags_backup):
            shutil.copy(self.flags_backup, self.flags_path)
    
    def run_study(self, n_trials=10):
        """è¿è¡Œè°ƒå‚ç ”ç©¶"""
        print("\n" + "="*70)
        print("ğŸš€ EnhancedUltra ç›¸ä¼¼åº¦å¢å¼ºå‚æ•°è°ƒä¼˜")
        print("="*70)
        print(f"è°ƒä¼˜å‚æ•°:")
        print(f"  - similarity_threshold_init: [0.5, 0.95], step=0.05")
        print(f"  - enhancement_strength_init: [0.01, 0.15], step=0.01")
        print(f"\nè¯„ä¼°ç­–ç•¥:")
        print(f"  - é¢„è®­ç»ƒ: å®Œæ•´10ä¸ªepoch (~10å°æ—¶)")
        print(f"  - å¿«é€Ÿè¯„ä¼°: {len(self.representative_datasets)}ä¸ªä»£è¡¨æ€§æ•°æ®é›† (~30-40åˆ†é’Ÿ)")
        print(f"  - æ€»æ—¶é—´ä¼°ç®—: {n_trials} trials Ã— (~10.5å°æ—¶) = {n_trials * 10.5:.1f}å°æ—¶")
        print("="*70)
        
        self.total_trials = n_trials
        self.start_time = datetime.now()
        
        # åˆ›å»ºstudy
        study = optuna.create_study(
            direction='minimize',  # æˆ‘ä»¬è¿”å›è´Ÿåˆ†æ•°ï¼Œæ‰€ä»¥æœ€å°åŒ–
            study_name='enhancement_params_tuning',
            storage=f"sqlite:///{self.output_dir}/study.db",
            load_if_exists=True,
            sampler=optuna.samplers.TPESampler(seed=42),
            pruner=optuna.pruners.MedianPruner(
                n_startup_trials=2,  # è‡³å°‘è¿è¡Œ2ä¸ªtrialæ‰å¼€å§‹å‰ªæ
                n_warmup_steps=1,
                interval_steps=1
            )
        )
        
        # è¿è¡Œä¼˜åŒ–
        study.optimize(
            self.objective,
            n_trials=n_trials,
            show_progress_bar=True
        )
        
        # ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–
        print(f"\n[å¯è§†åŒ–] ç”Ÿæˆæœ€ç»ˆå›¾è¡¨...")
        self._generate_final_plots(study)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_result = {
            'best_params': study.best_params,
            'best_value': -study.best_value,  # è½¬å›æ­£å€¼
            'n_trials': len(study.trials),
            'total_time_hours': (datetime.now() - self.start_time).total_seconds() / 3600
        }
        
        with open(os.path.join(self.output_dir, 'final_results.json'), 'w') as f:
            json.dump(final_result, f, indent=2)
        
        # æ¢å¤åŸå§‹flags.yaml
        self._cleanup()
        
        print("\n" + "="*70)
        print("âœ… è°ƒå‚å®Œæˆï¼")
        print("="*70)
        print(f"æœ€ä½³å‚æ•°:")
        print(f"  similarity_threshold_init: {study.best_params['similarity_threshold_init']:.3f}")
        print(f"  enhancement_strength_init: {study.best_params['enhancement_strength_init']:.3f}")
        print(f"æœ€ä½³åˆ†æ•°: {-study.best_value:.4f}")
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        print(f"å¯è§†åŒ–å›¾è¡¨: {self.viz_dir}")
        print("\nğŸ’¡ æç¤º: ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨Optuna DashboardæŸ¥çœ‹è¯¦ç»†ç»“æœ:")
        print(f"   optuna-dashboard sqlite:///{self.output_dir}/study.db")
        print("="*70)
        
        return study.best_params
    
    def _generate_final_plots(self, study):
        """ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–å›¾è¡¨"""
        try:
            plots = {
                'optimization_history': vis.plot_optimization_history(study),
                'param_importances': None,
                'parallel_coordinate': None,
                'contour': None,
            }
            
            if len(study.trials) > 3:
                try:
                    plots['param_importances'] = vis.plot_param_importances(study)
                except:
                    pass
            
            if len(study.trials) > 5:
                try:
                    plots['parallel_coordinate'] = vis.plot_parallel_coordinate(study)
                    plots['contour'] = vis.plot_contour(
                        study, 
                        params=['similarity_threshold_init', 'enhancement_strength_init']
                    )
                except:
                    pass
            
            # ä¿å­˜æ‰€æœ‰å›¾è¡¨
            for name, fig in plots.items():
                if fig is not None:
                    fig.write_html(os.path.join(self.viz_dir, f"final_{name}.html"))
            
            print(f"  âœ“ æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {self.viz_dir}")
            
        except Exception as e:
            print(f"  âš  ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='EnhancedUltraç›¸ä¼¼åº¦å¢å¼ºå‚æ•°è°ƒä¼˜')
    parser.add_argument('--n_trials', type=int, default=10, help='è¯•éªŒæ¬¡æ•°')
    parser.add_argument('--pretrain_config', type=str, 
                       default='config/transductive/pretrain_semma.yaml',
                       help='é¢„è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    
    tuner = EnhancementParamsTuner(pretrain_config=args.pretrain_config)
    best_params = tuner.run_study(n_trials=args.n_trials)
    
    print(f"\nğŸ¯ æœ€ç»ˆæœ€ä¼˜å‚æ•°:")
    for k, v in best_params.items():
        print(f"   {k}: {v}")

