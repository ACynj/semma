{
  "trial_number": 6,
  "score": 0.6700449666666668,
  "params": {
    "similarity_threshold_init": 0.9,
    "enhancement_strength_init": 0.04
  },
  "eval_results": {
    "FB15k237": {
      "mrr": 0.47192,
      "hits@10": 0.671015
    },
    "WN18RR": {
      "mrr": 0.550954,
      "hits@10": 0.66305
    },
    "CoDExSmall": {
      "mrr": 0.639078,
      "hits@10": 0.878009
    },
    "FB15k237Inductive": {
      "mrr": 0.503534,
      "hits@10": 0.671533
    },
    "WN18RRInductive": {
      "mrr": 0.702543,
      "hits@10": 0.800268
    },
    "NELLInductive": {
      "mrr": 0.764564,
      "hits@10": 0.91791
    },
    "avg_mrr": 0.6054321666666667,
    "avg_hits10": 0.7669641666666668,
    "score": 0.6700449666666668
  },
  "timestamp": "2025-11-10T03:03:02.965007",
  "manually_added": true,
  "note": "\u7b2c\u4e03\u6b21\u5b9e\u9a8c\uff08Trial 6\uff09"
}