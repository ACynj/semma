{
  "trial_number": 1,
  "score": 0.6785586000000001,
  "params": {
    "similarity_threshold_init": 0.8500000000000001,
    "enhancement_strength_init": 0.09
  },
  "eval_results": {
    "FB15k237": {
      "mrr": 0.472028,
      "hits@10": 0.670185
    },
    "WN18RR": {
      "mrr": 0.550377,
      "hits@10": 0.666879
    },
    "CoDExSmall": {
      "mrr": 0.651676,
      "hits@10": 0.889497
    },
    "FB15k237Inductive": {
      "mrr": 0.499596,
      "hits@10": 0.659367
    },
    "WN18RRInductive": {
      "mrr": 0.718805,
      "hits@10": 0.813673
    },
    "NELLInductive": {
      "mrr": 0.801496,
      "hits@10": 0.937811
    },
    "avg_mrr": 0.6156630000000001,
    "avg_hits10": 0.7729020000000001,
    "score": 0.6785586000000001
  },
  "timestamp": "2025-11-08T02:06:03.812346",
  "manually_added": true
}