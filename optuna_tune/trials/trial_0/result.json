{
  "trial_number": 0,
  "score": 0.6706075666666667,
  "params": {
    "similarity_threshold_init": 0.65,
    "enhancement_strength_init": 0.15
  },
  "eval_results": {
    "FB15k237": {
      "mrr": 0.474183,
      "hits@10": 0.671944
    },
    "WN18RR": {
      "mrr": 0.554386,
      "hits@10": 0.665284
    },
    "CoDExSmall": {
      "mrr": 0.636913,
      "hits@10": 0.881838
    },
    "FB15k237Inductive": {
      "mrr": 0.504086,
      "hits@10": 0.661801
    },
    "WN18RRInductive": {
      "mrr": 0.708132,
      "hits@10": 0.817694
    },
    "NELLInductive": {
      "mrr": 0.780579,
      "hits@10": 0.873134
    },
    "avg_mrr": 0.6097131666666666,
    "avg_hits10": 0.7619491666666667,
    "score": 0.6706075666666667
  },
  "timestamp": "2025-11-07T13:37:11.543021",
  "fixed": true
}