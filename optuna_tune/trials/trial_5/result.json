{
  "trial_number": 5,
  "score": 0.6769684333333335,
  "params": {
    "similarity_threshold_init": 0.5,
    "enhancement_strength_init": 0.15
  },
  "eval_results": {
    "FB15k237": {
      "mrr": 0.468747,
      "hits@10": 0.668963
    },
    "WN18RR": {
      "mrr": 0.551705,
      "hits@10": 0.670389
    },
    "CoDExSmall": {
      "mrr": 0.644289,
      "hits@10": 0.876368
    },
    "FB15k237Inductive": {
      "mrr": 0.506302,
      "hits@10": 0.666667
    },
    "WN18RRInductive": {
      "mrr": 0.724651,
      "hits@10": 0.816354
    },
    "NELLInductive": {
      "mrr": 0.796223,
      "hits@10": 0.91791
    },
    "avg_mrr": 0.6153195,
    "avg_hits10": 0.7694418333333334,
    "score": 0.6769684333333335
  },
  "timestamp": "2025-11-09T14:39:12.530214",
  "manually_added": true,
  "note": "\u7b2c\u516d\u6b21\u5b9e\u9a8c\uff08Trial 5\uff09"
}