run: semma # [ultra, semma, EnhancedUltra] Model type to use
LLM: gpt4o # [gpt4o, qwen3-32b, deepseekv3]
rg2_embedding: combined-sum # ["combined", "combined-sum", "no llm", "llm name", "llm description"]
k: 0 # should be positive if you want to add specific num of 5th type edges to each relation
model_embed: jinaai # [sentbert, jinaai]
topx: 0 # top x% of all relation pairs
threshold: 0.8 # threshold for constructing rg2
embedding_combiner: mlp # [mlp, concat, attention]
eval_on_valid: True # [True, False]
use_cos_sim_weights: True # [True, False], if True, we use cosine similarity weights for the 5th type edges
gpus: [0] # [0, 1, 2] # which gpu to use
harder_setting: False # [True, False], if True, we use harder setting where there are new relations 
is-inverse-relation-classify: False # [True, False], if True, use relation type classification for inverse relation embeddings
# KG-ICL Prompt Enhancement Settings
use_kg_icl_prompt: False # [True, False], if True, use KG-ICL style prompt enhancement
use_kg_icl_in_training: Flase # [True, False], if True, also use KG-ICL during training (like original KG-ICL paper)
prompt_num_examples: 2 # number of example triples to use in prompt graph (reduced from 3 to save memory)
prompt_max_hops: 1 # maximum hops for neighbor expansion in prompt graph (reduced from 2 to save memory)
prompt_num_layers: 1 # number of layers in prompt encoder (reduced from 2 to save memory)
