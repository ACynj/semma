run: semma # [ultra, semma, EnhancedUltra] Model type to use
LLM: gpt4o # [gpt4o, qwen3-32b, deepseekv3]
rg2_embedding: combined-sum # ["combined", "combined-sum", "no llm", "llm name", "llm description"]
k: 0 # should be positive if you want to add specific num of 5th type edges to each relation
model_embed: jinaai # [sentbert, jinaai]
topx: 0 # top x% of all relation pairs
threshold: 0.8 # threshold for constructing rg2
embedding_combiner: mlp # [mlp, concat, attention]
eval_on_valid: True # [True, False]
use_cos_sim_weights: True # [True, False], if True, we use cosine similarity weights for the 5th type edges
gpus: [0] # [0, 1, 2] # which gpu to use
harder_setting: False # [True, False], if True, we use harder setting where there are new relations 
is-inverse-relation-classify: False # [True, False], if True, use relation type classification for inverse relation embeddings
# Base paths configuration
base_path: /T20030104/ynj/semma # Base directory path for the project
ckpt_path: /T20030104/ynj/semma/ckpts # Checkpoint directory path
models_path: /T20030104/ynj/semma/models # Models directory path
kg_datasets_path: /T20030104/ynj/semma/kg-datasets # Knowledge graph datasets directory path
openrouter_path: /T20030104/ynj/semma/openrouter # OpenRouter data directory path
# EnhancedUltra Similarity Enhancement Parameters
similarity_threshold_init: 0.85
enhancement_strength_init: 0.09
# Adaptive Enhancement Gate Settings
use_adaptive_gate: False # [True, False], if True, use adaptive gate to learn when to apply enhancement
