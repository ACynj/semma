# 1 INTRODUCTION
A knowledge graph (KG) is graph-structural data with relational facts (Battaglia et al., 2018; Ji et al., 2020; Chen et al., 2020), based on which, one can conduct link prediction to deduce new facts from existing ones. The typical problem is to find the answer entity for the specific query, e.g., to find the answer Los Angeles to the query (LeBron, lives_in, ?). With continuous advances in recent years, the link prediction on KG has been widely applied in recommendation systems (Cao et al., 2019; Wang et al., 2019), online question answering (Huang et al., 2019), and drug discovery (Yu et al., 2021).

The prediction system learns from the local structure of a KG, where existing methods can be generally summarized as two categories: (1) semantic models that implicitly capture the local evidence through learning the low-dimensional embeddings of entities and relations (Bordes et al., 2013; Dettmers et al., 2017; Zhang et al., 2019; Wang et al., 2017); and (2) structural models that explicitly explore the KG’s structure based on relational paths or graphs with recurrent neural networks (RNNs) or graph neural networks (GNNs) (Das et al., 2017; Schlichtkrull et al., 2018; Sadeghian et al., 2019; Vashishth et al., 2019; Teru et al., 2020; Qu et al., 2021; Zhu et al., 2021; Zhang & Yao, 2022).

Although achieving leading performances, these structural models suffer from a severe scalability problem as the entire KG has been potentially or progressively taken for prediction. This inefficient manner hinders their application and optimization on large-scale KGs, e.g., OGB (Hu et al., 2020). Thus, it raises an open question: Is all the information necessary for prediction on knowledge graphs? Intuitively, only partial knowledge stored in the human brain is relevant to a given question, which is extracted by recalling and then utilized in the careful thinking procedure. Similarly, generating candidates and then ranking promising ones are common practices in large-scale recommendation systems with millions even billions of users (Cheng et al., 2016; Covington et al., 2016). These facts motivate us to conduct efficient link prediction with an effective sampling mechanism for KGs.

In this work, we propose the novel one-shot-subgraph link prediction on a knowledge graph. This idea paves a new way to alleviate the scalability problem of existing KG methods from a data-centric perspective: decoupling the prediction procedure into two steps with a corresponding sampler and a predictor. Thereby, the prediction of a specific query is conducted by (i) fast sampling of one query-dependent subgraph with the sampler and (ii) slow prediction on the subgraph with predictor. Nevertheless, it is non-trivial to achieve efficient and effective link prediction on large-scale KGs due to the two major challenges. (1) Sampling speed and quality: The fast sampling of the one-shot sampler should be capable of covering the essential evidence and potential answers to support the query. (2) Joint optimization: The sampler and predictor should be optimized jointly to avoid trivial solutions and to guarantee the expressiveness and adaptivity of the overall model to a specific KG.

To solve these challenges technically, we first implement the one-shot-subgraph link prediction by the non-parametric and computation-efficient Personalized PageRank (PPR), which is capable of effectively identifying the potential answers without requiring learning. With the efficient subgraphbased prediction, we further propose to search the data-adaptive configurations in both data and model spaces. We show it unnecessary to utilize the whole KG in inference; meanwhile, only a relatively small proportion of information (e.g., 10% of entities) is sufficient. Our main contributions are:
- We conceptually formalize the new manner of one-shot-subgraph link prediction on KGs (Sec. 3) and technically instantiate it with efficient heuristic samplers and powerful KG predictors (Sec. 4.1).
- We solve a non-trivial and bi-level optimization problem of searching the optimal configurations in both data and model spaces (Sec. 4.2) and theoretically analyze the extrapolation power (Sec. 4.3).
- We conduct extensive experiments on five large-scale datasets and achieve an average of 94.4% improvement in efficiency of prediction and 6.9% promotion in effectiveness of prediction (Sec. 5).


# 2 PRELIMINARIES
Notations. A knowledge graph is denoted as \(G=(V, R, E)\), where the entity set \(V\), the relation set \(R\), and factual edge set \(E=\{(x, r, v): x, v \in V, r \in R\}\). Here, a fact is formed as a triplet and denoted as \((x, r, v)\). Besides, a sampled subgraph of \(G\) is denoted as \(G_{s}=(V_{s}, R_{s}, E_{s})\), satisfying that \(V_{s} \subseteq V\), \(R_{s} \subseteq R\), \(E_{s} \subseteq E\). The atomic problem of link prediction is denoted as a query \((u, q, ?)\), i.e., given the query entity \(u\) and query relation \(q\), to find the answer entity \(a\), making \((u, q, v)\) valid.

Semantic models encode entities and relations to low-dimensional entity embeddings \(H_{V} \in \mathbb{R}^{|V| \times D_{v}}\) and relation embeddings \(H_{R} \in \mathbb{R}^{|R| \times D_{r}}\), where \(D_{v}, D_{r}\) are dimensions. A scoring function \(f_{\theta}\), e.g., TransE (Bordes et al., 2013) or QuatE (Zhang et al., 2019), is necessary here to quantify the plausibility of a query triplet \((u, q, v)\) with the learned embeddings \((h_{u}, h_{q}, h_{v})\) as \(f_{\theta}:(\mathbb{R}^{D_{v}}, \mathbb{R}^{D_{r}}, \mathbb{R}^{D_{v}}) \mapsto \mathbb{R}\).

Efficient semantic models aim to reduce the size of entity embeddings. NodePiece (Galkin et al., 2022) proposes an anchor-based approach that obtains fixed-size embeddings as \(G \stackrel{f_{\theta}}{\longmapsto} \hat{H}_{V} \in \mathbb{R}^{N \times D_{v}}\) and inference as \((\hat{H}, G) \stackrel{f_{\theta,(u, q)}}{\to} \hat{Y}\), where \(\hat{Y}\) are the scores of candidate answers, and \(N \ll |V|\). Designed to reduce the embedding size, NodePiece cannot reduce the graph size for structural models.

Structural models are based on relational paths or graphs for link prediction. Wherein the path-based models, e.g., MINERVA (Das et al., 2017), DRUM (Sadeghian et al., 2019), and RNNLogic (Qu et al., 2021), aim to learn probabilistic and logical rules and well capture the sequential patterns in KGs. The graph-based models such as R-GCN (Schlichtkrull et al., 2018) and CompGCN (Vashishth et al., 2019) propagate the low-level entity embeddings among the neighboring entities to obtain high-level embeddings. Recent methods NBFNet (Zhu et al., 2021) and RED-GNN (Zhang & Yao, 2022) progressively propagate from \(u\) to its neighborhood in a breadth-first-searching (BFS) manner.

Sampling-based structural models adopt graph sampling approaches to decrease the computation complexity, which can be categorized into two-fold as follows. First, subgraph-wise methods such as GraIL (Teru et al., 2020) and CoMPILE (Mai et al., 2021) extract enclosing subgraphs between query entity \(u\) and each candidate answer \(v\). Second, layer-wise sampling methods extract a subgraph for message propagation in each layer of a model. Wherein designed for node-level tasks on homogeneous graphs, GraphSAGE (Hamilton et al., 2017) and FastGCN (Chen et al., 2018) randomly sample neighbors around the query entity. While the KG sampling methods, e.g., DPMPN (Xu et al., 2019), AdaProp (Zhang et al., 2023c), and AStarNet (Zhu et al., 2023), extract a learnable subgraph in \(\ell\)-th layer by the GNN model in \(\ell\)-th layer, coupling the procedures of sampling and prediction.


# 3 One-shot-subgraph LINK PREDICTION ON KNOWLEDGE GRAPHS
To achieve efficient link prediction, we conceptually design the one-shot-subgraph manner that avoids directly predicting with the entire KG. We formalize this new manner in the following Def. 1.

**Definition 1 (One-shot-subgraph Link Prediction on Knowledge Graphs).** Instead of directly predicting on the original graph \(G\), the prediction procedure is decoupled to two-fold: (1) one-shot sampling of a query-dependent subgraph and (2) prediction on this subgraph. The prediction pipeline becomes 
\[
\mathcal{G} \stackrel{g_{\phi},(u, q)}{\to} \mathcal{G}_{s} \stackrel{f_{\theta}}{\to} \hat{Y}, \quad(1)
\]
where the sampler \(g_{\phi}\) generates only one subgraph \(G_{s}\) (satisfies \(|V_{s}| \ll |V|, |E_{s}| \ll |E|\)) conditioned on the given query \((u, q, ?)\). Based on subgraph \(G_{s}\), the predictor \(f_{\theta}\) outputs the final predictions \(\hat{Y}\).

**Comparison with existing manners of prediction.** In brief, semantic models follow the manner of encoding the entire \(G\) to the embeddings \(H=(H_{V}, H_{R})\) and prediction (inference) without \(G\), i.e., 
\[
H \stackrel{f_{\theta},(u, q)}{\to} \hat{Y}, s.t. \mathcal{G} \stackrel{f_{\theta}}{\to} H,
\]
which is parameter-expensive, especially when encountering a large-scale graph with a large entity set. On the other hand, structural models adopt the way of learning and prediction with \(G\), i.e., 
\[
\mathcal{G} \stackrel{f_{\theta},(u, q)}{\to} \hat{Y},
\]
that directly or progressively conduct prediction with the entire graph \(G\). Namely, all the entities and edges can be potentially taken in the prediction of one query, which is computation-expensive. By contrast, our proposed one-shot prediction manner (Def. 1) enjoys the advantages 1 & 2 as follows.

**Advantage 1 (Low complexity of computation and parameter).** The one-shot-subgraph model is (1) computation-efficient: the extracted subgraph is much smaller than the original graph, i.e., \(|V_{s}| \ll |V|\) and \(|E_{s}| \ll |E|\); and (2) parameter-efficient: it avoids learning the expensive entities’ embeddings.

**Advantage 2 (Flexible propagation scope).** The scope here refers to the range of message propagation starting from the query entity \(u\). Normally, an \(L\)-layer structural method will propagate to the full \(L\)-hop neighbors of \(u\). By contrast, the adopted one-shot sampling enables the bound of propagation scope within the extracted subgraph, where the scope is decoupled from the model’s depth \(L\).

**Comparison with existing sampling methods.** Although promising to the scalability issue, existing sampling methods for structural models are not efficient or effective enough for learning and prediction on large-scale KGs. To be specific, the random layer-wise sampling methods cannot guarantee the coverage of answer entities, i.e., \(\mathbb{I}(v \in V_{u})\). By contrast, the learnable layer-wise sampling methods extract the query-dependent subgraph \(G_{s}^{(\ell)}\) in \(\ell\)-th layer via the GNN model \(f_{\theta}^{(\ell)}\) in \(\ell\)-th layer as 
\[
\mathcal{G} \stackrel{f_{\theta}^{(1)},(u, q)}{\to} \mathcal{G}_{s}^{(1)} \stackrel{f_{\theta}^{(2)},(u, q)}{\to} \mathcal{G}_{s}^{(2)} \mapsto \cdots \mapsto \mathcal{G}_{s}^{(L-1)} \stackrel{f_{\theta}^{(L)},(u, q)}{\longmapsto} \hat{Y},
\]
coupling the sampling and prediction procedures that (1) are bundled with specific architectures and (2) with extra computation cost in the layer-wise sampling operation. Besides, the subgraph-wise sampling methods extract the enclosing subgraphs between query entity \(u\) and each candidate answer \(v \in V\), and then independently reason on each of these subgraphs to obtain the final prediction \(\hat{Y}\) as 
\[
\left\{\hat{Y}_{v}: \mathcal{G} \stackrel{(u, v)}{\to} \mathcal{G}_{s}^{(u, v)} \stackrel{f_{\theta},(u, q, v)}{\to} \hat{Y}_{v}\right\}_{v \in \mathcal{V}} \mapsto \hat{Y}.
\]

Note these approaches are extremely expensive on large-scale graphs, as each candidate \((u, v)\) corresponds to a subgraph to be scored. By contrast, one-shot sampling manner enjoys the advantage 3.

**Advantage 3 (High efficiency in subgraph sampling).** Our proposed prediction manner requires only one subgraph for answering one query, which is expected to cover all the potential answers and supporting facts. Notably, this query-specific subgraph is extracted in a one-shot and decoupled manner that does not involve the predictor, reducing the computation cost in subgraph sampling.


# 4 INSTANTIATING THE ONE-SHOT-SUBGRAPH LINK PREDICTION
Note that it is non-trivial to achieve Def. 1, wherein (i) the implementation of sampler, (ii) the architecture of predictor, and (iii) the method to optimize these two modules need to be figured out. Here, the major challenge lies in the sampler, which is required to be efficient, query-dependent, and local-structure-preserving. In this section, we elaborate on the detailed implementation (Sec. 4.1), set up a bi-level problem for optimization (Sec. 4.2), and investigate the extrapolation power (Sec. 4.3).

## 4.1 REALIZATION: THREE-STEP PREDICTION WITH PERSONALIZED PAGERANK
Overview. As illustrated in Fig. 1, the three key steps of our method are (1) generating the sampling distribution \(\mathbb{P}_{G}\) by sampler \(g_{\phi}\), (2) sampling a subgraph from the distribution as \(G_{s} \sim \mathbb{P}_{G}\) with top-K entities and edges, and (3) predicting on the subgraph \(G_{s}\) and acquiring the final prediction \(\hat{Y}\) by predictor \(f_{\theta}\). The three-step procedure is summarized in Algorithm 1 and elaborated on as follows.

Step-1. Generate sampling distribution. Previous studies show that \(a\) is generally near to \(u\) (Zhu et al., 2021; Zhang & Yao, 2022), and the relational paths connecting \(u\) and \(v\) that support the query also lie close to \(u\) (Das et al., 2017; Sadeghian et al., 2019). To efficiently capture the local evidence of \(u\), we choose the heuristic Personalized PageRank (PPR) (Page et al., 1999; Jeh & Widom, 2003) as the sampling indicator. Note that PPR is not only efficient for its non-parametric nature but also query-dependent and local-structure-preserving for its single-source scoring that starts from \(u\).

Specifically, PPR starts propagation from \(u\) to evaluate the importance of each neighbor of \(u\) and generates the PageRank scores as the sampling probability that encodes the local neighborhood of the query entity \(u\). Besides, it can also preserve the locality and connectivity of subgraphs by leveraging the information from a large neighborhood. Given a query entity \(u\), we obtain the probability \(p \in \mathbb{R}^{|V|}\) 
\[
Non-parametric indicator : p^{(k+1)} \leftarrow \alpha \cdot s+(1-\alpha) \cdot D^{-1} A \cdot p^{(k)}, \quad (2)
\]
by iteratively updating the scores up to \(K=100\) steps to approximate the converged scores efficiently. Here, the initial score \(p^{(0)}=s=\mathbb{1}(u) \in \{0,1\}^{|V|}\) indicates the query entity \(u\) to be explored. The two-dimensional degree matrix \(D \in \mathbb{R}^{|V| \times |V|}\) and adjacency matrix \(A \in \{0,1\}^{|V| \times |V|}\) together work as the transition matrix, wherein \(A_{i j}=1\) means an edge \((i, r, j) \in E\) and \(D_{i j}=degree(v_{i})\) if \(i=j\) else \(D_{i j}=0\). The damping coefficient \(\alpha\) (=0.85 by default) controls the differentiation degree.

Step-2. Extract a subgraph. Based on the PPR scores \(p\) (Eqn. 2), the subgraph \(G_{s}=(V_{s}, E_{s}, R_{s})\) (where \(R_{s}=R\)) is extracted with the most important entities and edges. Denoting the sampling ratios of entities and edges as \(r_{V}^{q}\), \(r_{E}^{q} \in (0,1]\) that depend on the query relation \(q\), we sample \(|V_{s}|=r_{V}^{q} \times |V|\) entities and \(|E_{s}|=r_{E}^{q} \times |E|\) edges from the full graph \(G\). With the TopK \((D, P, K)\) operation that picks up top-K elements from candidate \(D\) w.r.t. probability \(P\), the entities \(V_{s}\) and edges \(E_{s}\) are given as 
\[
\begin{aligned} 
& Entity Sampling: \mathcal{V}_{s} \leftarrow TopK\left(\mathcal{V}, p, K=r_{\mathcal{V}}^{q} \times|\mathcal{V}|\right), \\ 
& Edge Sampling: \mathcal{E}_{s} \leftarrow TopK\left(\mathcal{E},\left\{p_{x} \cdot p_{o}: x, o \in \mathcal{V}_{s},(x, r, o) \in \mathcal{E}\right\}, K=r_{\mathcal{E}}^{q} \times|\mathcal{E}|\right) . 
\end{aligned}
\]

Step-3. Reason on the subgraph. From the model’s perspective, we build the configuration space of the predictor and further utilize the advantages of existing structural models introduced in Sec. 2. Three query-dependent message functions MESS(·) are considered, including DRUM, NBFNet, and RED-GNN, which are elaborated in Appendix. B. Note the effective message is propagated from \(u\) to the sampled entities \(o \in V_{s}\). Generally, the layer-wise updating of representations is formulated as
\[
\begin{aligned}
& \text{Indicating: } h_{o}^{(0)} \leftarrow \mathbb{1}(o = u), \\
& \text{Propagation: } h_{o}^{(\ell+1)} \leftarrow \text{DROPOUT}\left(\text{ACT}\left(\text{AGG}\left\{\text{MESS}(h_{x}^{(\ell)}, h_{r}^{(\ell)}, h_{o}^{(\ell)}\right): (x, r, o) \in E_{s}\right\}\right)),
\end{aligned} \quad (4)
\]
where \(\mathbb{1}(o=u)\) is the indicator function that only labels the query entity \(u\) in a query-dependent manner. After a \(L\)-layer propagation, the predictor outputs the final score \(\hat{y}_{o}\) of each entity \(o \in V_{s}\) based on their representations \(h_{o}^{(L)}\) as \(\hat{y}_{o}=\text{Readout}(h_{o}^{(L)}, h_{u}^{(L)}) \in \mathbb{R}\). The loss function \(L_{cls}\) adopted in the training phase is the commonly-used binary cross-entropy loss on all the sampled entities. Namely, 
\[
L_{cls}=-\sum_{o \in V_{s}} y_{o} \log (\hat{y}_{o})+(1-y_{o}) \log (1-\hat{y}_{o}),
\]
where \(y_{o}=1\) if \(o=v\) else \(y_{o}=0\).

**Algorithm 1 One-shot-subgraph Link Prediction on Knowledge Graphs**
1: Require: KG \(G = (V, R, E)\), degree matrix \(D \in \mathbb{R}^{|V| \times |V|}\), adjacency matrix \(A \in \{0, 1\}^{|V| \times |V|}\), damping coefficient \(\alpha\), maximum PPR iterations \(K\), query \((u, q, ?)\), sampler \(g_{\phi}\), predictor \(f_{\theta}\).
2: # Step-1. Generate sampling distribution
3: initialize \(s \leftarrow \mathbb{1}(u)\), \(p^{(0)} \leftarrow \mathbb{1}(u)\).
4: for \(k = 1 \dots K\) do
5:    \(p^{(k+1)} \leftarrow \alpha \cdot s + (1 - \alpha) \cdot D^{-1}A \cdot p^{(k)}\).
6: end for
7: # Step-2. Extract a subgraph \(G_s\)
8: \(V_s \leftarrow TopK(V, p, K = r_V^q \times |V|)\).
9: \(E_s \leftarrow TopK(E, \{p_u \cdot p_v : u, v \in V_s, (u, r, v) \in E\}, K = r_E^q \times |E|)\).
10: # Step-3. Reason on the subgraph
11: initialize representations \(h_{o}^{(0)} \leftarrow \mathbb{1}(o = u)\).
12: for \(\ell = 1 \dots L\) do
13:    \(h_{o}^{(\ell)} \leftarrow \text{DROPOUT}\left(\text{ACT}\left(\text{AGG}\left\{\text{MESS}(h_{x}^{(\ell-1)}, h_{r}^{(\ell-1)}, h_{o}^{(\ell-1)}\right): (x, r, o) \in E_s\right\}\right)\right)\).
14: end for
15: return Prediction \(\hat{y}_o = \text{Readout}(h_{o}^{(L)}, h_{u}^{(L)})\) for each entity \(o \in V_s\).

## 4.2 OPTIMIZATION: EFFICIENT SEARCHING FOR DATA-ADAPTIVE CONFIGURATIONS
Search space. Note that hyperparameters \((r_{V}^{q}, r_{E}^{q})\) and \(L\) play important roles in Algorithm 1. Analytically, a larger subgraph with larger \(r_{V}^{q}\), \(r_{E}^{q}\) does not indicate a better performance, as more irrelevant information is also covered. Besides, a deeper model with a larger \(L\) can capture more complex patterns but is more likely to suffer from the over-smoothing problem (Oono & Suzuki, 2019). Overall, the \((r_{V}^{q}, r_{E}^{q})\) are for sampler’s hyper-parameters \(\phi_{hyper}\). In addition to \(L\), predictor’s hyper-parameters \(\theta_{hyper}\) contain several intra-layer or inter-layer designs, as illustrated in Fig. 2(a).

Search problem. Next, we propose the bi-level optimization problem to adaptively search for the optimal configuration \((\phi_{hyper }^{*}, \theta_{hyper }^{*})\) of design choices on a specific KG, namely, 
\[
\begin{aligned} 
& \phi_{hyper }^{*}=\arg \max _{\phi_{hyper }} \mathcal{M}\left(f_{\left(\theta_{hyper }^{*}, \theta_{learn }^{*}\right)}, g_{\phi_{hyper }}, \mathcal{E}^{val }\right), \\ 
& s.t. \theta_{hyper }^{*}=\arg \max _{\theta_{hyper }} \mathcal{M}\left(f_{\left(\theta_{hyper }, \theta_{learn }^{*}\right)}, g_{\bar{\phi}_{hyper }}, \mathcal{E}^{val }\right),
\end{aligned}
\]
where the performance measurement \(\mathcal{M}\) can be Mean Reciprocal Ranking (MRR) or Hits@k. Note the non-parametric sampler \(g_{\phi}\) only contains hyper-parameters \(\phi_{hyper}\). As for predictor \(f_{\theta}\), its \(\theta=(\theta_{hyper }, \theta_{learn })\) also includes learnable \(\theta_{learn }\) that \(\theta_{learn }^{*}=\arg \min _{\theta_{learn }} \tilde{L}_{cls }(f_{(\theta_{hyper }, \theta_{learn })}, g_{\bar{\phi}_{hyper }}, E^{train })\).

Search algorithm. Directly searching on both data and model spaces is expensive due to the large space size and data scale. Hence, we split the search into two sub-processes as Fig. 2(b), i.e.,
1. First, we freeze the sampler \(g_{\bar{\phi}}\) (with constant \(\phi_{hyper}\)) to search for the optimal predictor \(f_{\theta}^{*}\) with (1) the hyper-parameters optimization for \(\theta_{hyper }^{*}\) and (2) the stochastic gradient descent for \(\theta_{learn }^{*}\).
2. Then, we freeze the predictor \(f_{\theta}^{*}\) and search for the optimal sampler \(g_{\phi}^{*}\), simplifying to pure hyper-parameters optimization for \(\phi_{hyper }^{*}\) in a zero-gradient manner with low computation complexity.

Specifically, we follow the sequential model-based Bayesian Optimization (BO) (Bergstra et al., 2013; Hutter et al., 2011) to obtain \(\phi_{hyper }^{*}\) and \(\theta_{hyper }^{*}\). Random forest (RF) (Breiman, 2001) is chosen as the surrogate model because it has a stronger power for approximating the complex and discrete curvature (Grinsztajn et al., 2022), compared with other common surrogates, e.g., Gaussian Process (GP) (Williams & Rasmussen, 1995) or Multi-layer Perceptron (MLP) (Gardner & Dorling, 1998).

Acceleration in searching. We adopt a data split trick that balances observations and predictions. It saves time as the training does not necessarily traverse all the training facts. Specifically, partial training facts are sampled as training queries while the others are treated as observations, i.e., we randomly separate the training facts into two parts as \(E^{train }=E^{obs } \cup E^{query }\), where the overall prediction system \(f_{\theta} \circ g_{\phi}\) takes \(E^{obs }\) as input and then predicts \(E^{query }\) (see Fig. 2(c)). Here, the split ratio \(r^{split }\) is to balance the sizes of these two parts as \(r^{split }=|E^{obs }| /|E^{query }|\). Thus, the training becomes \(\theta^{*}=\arg \min _{\theta} \sum _{(u, q, v) \in E^{query }} L_{cls}(f_{\theta}(G_{s}), v)\) with the split query edges \(E^{query }\), where \(G_{s}=g_{\phi}(E^{obs}, u, q)\) with the split observation edges \(E^{obs }\). More technical details can be found in the Appendix. B.

## 4.3 THEORY: THE EXTRAPOLATION POWER OF ONE-SHOT-SUBGRAPH LINK PREDICTION
Further, we investigate the extrapolation power of link prediction across graph scales, i.e., training and inference on different scales of graphs. For example, training on small subgraphs \(G_{s}^{train }\) and testing on large subgraphs \(G_{s}^{test }\), where the ratio of entities \(|V_{s}^{test }| /|V_{s}^{train }| \gg 1\). This scenario is practical for recommendation systems on social networks that can encounter much larger graphs in the testing phase. Intuitively, training on smaller \(G_{s}^{train }\) can save time for its faster convergence on subgraphs (Shi et al., 2023), while predicting on larger \(G_{s}^{test }\) might gain promotion for more support of facts in \(G_{s}^{test }\).

Nonetheless, the Theorem 1 below proves that link prediction can become unreliable as the test graph grows. That is, if we use a small subgraph for training and a large subgraph for testing, the predictor will struggle to give different predictions within and across sampling distributions by \(g\), even when these probabilities are arbitrarily different in the underlying graph model. Our empirical results in Fig. 4 support this theoretical finding. Hence, it is necessary to strike a balance of subgraphs’ scale.

**Theorem 1.** Let \(G_{s}^{train } \sim \mathbb{P}_{G}\) and \(G_{s}^{test } \sim \mathbb{P}_{G}\) be the training and testing graphs that are sampled from distribution \(\mathbb{P}_{G}\). Consider any two test entities \(v \in V_{s}^{test }\), for which we can make a prediction decision of fact \((u, q, v)\) with the predictor \(f_{\theta}\), i.e., \(\hat{y}_{v}=f_{\theta}(G_{s}^{test })_{v} \neq \tau\). Let \(G_{s}^{test }\) be large enough to satisfy \(\sqrt{|V_{s}^{test }|} / \sqrt{\log (2|V_{s}^{test }| / p)} \geq 4 \sqrt{2} / d_{min }\), where \(d_{min }\) is the constant of graphon degree (Diaconis & Janson, 2007). Then, for an arbitrary threshold \(\tau \in [0,1]\), the testing subgraph \(G_{s}^{test }\) satisfies that 
\[
\frac{\sqrt{\left|\mathcal{V}_{s}^{test }\right|}}{\sqrt{\log \left(2\left|\mathcal{V}_{s}^{test }\right| / p\right)}} \geq \frac{2\left(C_{1}+C_{2}\| g\| _{\infty}\right)}{\left|f_{\theta}\left(\mathcal{G}_{s}^{test }\right)_{v}-\tau\right| / L\left(M^{train }\right)}.
\]
where the underlying generative function of graph signal \(g \in L^{\infty}\) is with the essential supreme norm as in (Maskey et al., 2022; Zhou et al., 2022). The \(C_{1}\), \(C_{2}\) are constants and depend on \(M^{train }\) where \(\min(\text{supp}(|V_{s}^{test }|)) \gg M^{train } = \max(\text{supp}(|V_{s}^{train }|))\). It means any test graph can be much larger than the largest possible training graph, and \(\text{supp}\) indicates the support of a distribution. Then, if \(u\) and \(v\) are isomorphic in topology and with the same representations, we have a probability at least \(1-\sum_{\ell=1}^{L} 2(|h^{(\ell)}|+1) p\) with hidden size \(|h^{(\ell)}|\) that the same predictions can be obtained whether \(u, v\) are generated by the same or distinct \(g\). The detailed proof can be found in Appendix. A.

=======================================上述为论文原文，接下来是我的idea===============================================
semma模型的问题：
1、SEMMA 的核心痛点之一是关系文本语义差异大：其 5.3 节 ablation 实验指出，关系文本可能是 “高描述性自然语言（如 country of citizenship）” 或 “无意义数据库路径（如 /film/film/genre）”，固定阈值（如原方案 0.8）无法适配这种多样性 —— 对语义密集关系（如亲属关系）会过滤掉有效间接关联，对语义独特关系（如生物领域的 hasDNA）会保留冗余噪声。
导致的后果：
A、对关系语义密集的数据集，丢失了有用的间接连接
B、对语义独特关系，保留了没用的冗余噪声

解决方案：
而上述论文的PPR具备个性化动态筛选能力：以查询依赖为核心，通过迭代传播关系节点的重要性得分，而非全局固定规则。A、对语义密集关系，PPR会通过传播效应自动保留更多语义邻居。B、对语义独特关系，PPR会因传播路径少而减少邻居数量，避免冗余噪声。

2、SEMMA 的核心目标是提升全归纳链路预测泛化性，尤其在 “测试集关系完全 unseen” 场景，但原阈值方案仅能筛选 “余弦相似度高于阈值的直接关联关系”，丢失大量 “间接语义关联”（如 r1 与 r2 相似、r2 与 r3 相似，但 r1 与 r3 相似度低于阈值），这对泛化性提升构成瓶颈。

解决方案：LIM 的 PPR 恰好擅长捕捉间接关联：根据 LIM Sec.3-4，PPR 通过 “从查询起点迭代传播得分”，能发现实体间的隐性结构关联（如查询实体 u 的 2-hop 邻居通过 1-hop 邻居获得高得分）。

3、SEMMA 的文本模块依赖LLM 生成的关系文本嵌入，但 LLM 生成的描述可能存在偏差（如关系 “isA” 被错误描述为 “belongsTo”），导致余弦相似度计算不准 —— 原阈值方案会直接过滤 “误差导致的低相似度边”（如真实相似但嵌入误差导致相似度 0.78<0.8），丢失有效信息。

解决方案：
LIM 的 PPR 具备噪声鲁棒性：根据 LIM Sec.4.1，PPR 通过 “多轮迭代传播”，可利用其他有效关联修正单个节点的误差（如某实体得分偏低，但通过高得分邻居传播后得分被修正）。

具体实现：
根据上述描述，将固定阈值修改成使用PPR捕捉到的关系对来代替固定阈值。
要求： 不要修改后续逻辑，只要将固定阈值替换成PPR，并且通过dynamic_threshold来控制开关，若为False则使用原固定阈值的方案。

虚拟环境：semma_vip